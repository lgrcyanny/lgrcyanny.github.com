<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>HBase Architecture Analysis Part2(Process Architecture) | CyannyLive</title>

  
  <meta name="author" content="Cyanny Liang">
  

  
  <meta name="description" content="Do not go gentle into that good night">
  

  
  
  <meta name="keywords" content="Hadoop,Big Data,HBase">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="HBase Architecture Analysis Part2(Process Architecture)"/>

  <meta property="og:site_name" content="CyannyLive"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="CyannyLive" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">CyannyLive</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>HBase Architecture Analysis Part2(Process Architecture)</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2014/03/13/hbase-architecture-analysis-part2-process-architecture/" rel="bookmark">
        <time class="entry-date published" datetime="2014-03-13T14:00:11.000Z">
          2014-03-13
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h2 id="4-Process-Architecture"><a href="#4-Process-Architecture" class="headerlink" title="4. Process Architecture"></a>4. Process Architecture</h2><h3 id="4-1-HBase-Write-Path"><a href="#4-1-HBase-Write-Path" class="headerlink" title="4.1 HBase Write Path"></a>4.1 HBase Write Path</h3><p>The client doesn’t write data directly into HFile on HDFS. Firstly it writes data to WAL(Write Ahead Log), and Secondly, writes to MemStore shared by a HStore in memory.<br><a id="more"></a><br><a href="http://tinypic.com?ref=20i77l0" target="_blank" rel="external"><img src="http://i58.tinypic.com/20i77l0.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure4.1 HBase Write Path</p>
<p><strong>MemStore</strong> is a write buffer(64MB by default). When the data in MemStore accumulates its threshold, data will be flush to a new HFile on HDFS persistently. Each Column Family can have many HFiles, but each HFile only belongs to one Column Family.<br><strong>WAL</strong> is for data reliability, WAL is persistent on HDFS and each Region Server has only on WAL. When the Region Server is down before MemStore flush, HBase can replay WAL to restore data on a new Region Server.<br>A data write completes successfully only after the data is written to WAL and MemStore.</p>
<h3 id="4-2-HBase-Read-Path"><a href="#4-2-HBase-Read-Path" class="headerlink" title="4.2 HBase Read Path"></a>4.2 HBase Read Path</h3><p>As shown in Figure 4.2, it’s the read path of HBase.<br>1. Client will query the MemStore in memory, if it has the target row.<br>2. When MemStore query failed, client will hit the BlockCache.<br>3. After the MemStore and BlockCache query failed, HBase will load HFiles into memory which may contain the target row info.<br>The MemStore and BlockCache is the mechanism for real time data access for distributed large data.<br><strong>BlockCache is a LRU(Lease Recently Used) priority cache.</strong> Each RegionServer has a single BlockCache. It keeps frequently accessed data from HFile in memory to reduce disk data reads. The “Block”(64KB by default) is the smallest index unit of data or the smallest unit of data that can be read from disk by one pass.<br>For random data access, small block size is preferred, but block index consumes more memory. And for sequential data access, large block size is better, fewer index save more memory.</p>
<p><a href="http://tinypic.com?ref=290x9qt" target="_blank" rel="external"><img src="http://i61.tinypic.com/290x9qt.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure 4.2 HBase Read Path</p>
<h3 id="4-3-HBase-Housekeeping-HFile-Compaction"><a href="#4-3-HBase-Housekeeping-HFile-Compaction" class="headerlink" title="4.3.HBase Housekeeping: HFile Compaction"></a>4.3.HBase Housekeeping: HFile Compaction</h3><p>The data of each column family is flush into multiple HFiles. Too many HFiles means many disk data reads and lower the read performance. Therefore, HBase do HFile compaction periodically.<br><a href="http://tinypic.com?ref=2nk85xj" target="_blank" rel="external"><img src="http://i62.tinypic.com/2nk85xj.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure4.3 HFile Compaction</p>
<p><strong>➢Minor Compaction</strong><br>It happens on multiple HFiles in one HStore. Minor compaction will pick up a couple of adjacent small HFiles and rewrite them into a larger one.<br>The process will keep the deleted or expired cells. The HFile selection standard is configurable. Since minor compaction will affect HBase performace, there is an upper limit on the number of HFiles involved (10 by default).<br><strong>➢Major Compaction</strong><br>Major Compaction compact all HFiles in a HStore(Column Family) into one HFile. It is the only chance to delete records permanently. Major Compaction will usually have to be triggered manually for large clusters.<br>Major Compaction is not region merge, it happens to HStore which will not result in region merge.</p>
<h3 id="4-4-HBase-Delete"><a href="#4-4-HBase-Delete" class="headerlink" title="4.4 HBase Delete"></a>4.4 HBase Delete</h3><p>When HBase client send delete request, the record will be marked “tombstone”, it is a “predicate deletion”, which is supported by LSM-tree. Since HFile is immutable, deletion isn’t available for HFile on HDFS. Therefore, HBase adopts major compaction(Section 4.3) to clean up deleted or expired records.</p>
<h3 id="4-5-Region-Assignment"><a href="#4-5-Region-Assignment" class="headerlink" title="4.5 Region Assignment"></a>4.5 Region Assignment</h3><p>It is a main task of HMaster:<br>1. HMaster invoke the AssignmentManager to do region assignment.<br>2. AssignmentManager checks the existing region assignments in .META.<br>3. If region assignment is valid, then keep the region.<br>4. If region assignment is invalid, then the LoadBalancerFactory will create a DefaultLoadBalancer<br>5. DefaultLoadBalancer will assign the new region randomly to a RegionServer<br>6. Update the assignment to .META.<br>7. The RegionServer open the new region</p>
<h3 id="4-6-Region-Split"><a href="#4-6-Region-Split" class="headerlink" title="4.6 Region Split"></a>4.6 Region Split</h3><p>Region split is the work of RegionServer, not participated by HMaster. When a region in a RegionServer accumulates over size threshold, RegionServer will split the region into half.<br>1. RegionServer offline the region to be split.<br>2. RegionServer split the region into two half<br>3. Update new daughter regions info to .META.<br>4. Open the new daughter regions.<br>5. Report the split info to HMaster.</p>
<h3 id="4-7-Region-Merge"><a href="#4-7-Region-Merge" class="headerlink" title="4.7 Region Merge"></a>4.7 Region Merge</h3><p>A RegionServer can’t have too many regions, because too many regions bring large cost of memory and lower the performance of RegionServer. Meanwhile, HMaster can’t handle load balance with too many regions.<br>Therefore, when the number of regions is over a threshold, region merge is trigged. Region Merge is joined by RegionServer and HMaster.<br>1. Client send RPC region merge request to HMaster<br>2. HMaster moves regions together to the same RegionServer where the more heavily loaded region resided.<br>3. HMaster send request to the RegionServer to run region merge.<br>4. The RegionServer offline the regions to be merged.<br>5. The RegionServer Merge the regions on the local file system.<br>6. Delete the meta info of the merging regions on .META. table, add new meta info to .META. table.<br>7. Open the new merged region<br>8. Report the merge to HMaster</p>
<h3 id="4-8-Auto-Failover"><a href="#4-8-Auto-Failover" class="headerlink" title="4.8 Auto Failover"></a>4.8 Auto Failover</h3><h4 id="4-8-1-RegionServer-Failover"><a href="#4-8-1-RegionServer-Failover" class="headerlink" title="4.8.1 RegionServer Failover"></a>4.8.1 RegionServer Failover</h4><p>When a region server fails, HMaster will do failover automatically.<br>1. RegionServer is down<br>2. HMaster will detect the unavailable RegionServer when there is no heartbeat report.<br>3. HMaster start region reassignment, detect that the region assignment is invalid, then re-assign the regions like the region assignment sequence.</p>
<h4 id="4-8-2-Master-Failover"><a href="#4-8-2-Master-Failover" class="headerlink" title="4.8.2 Master Failover"></a>4.8.2 Master Failover</h4><p>In centralized architecture style, the SPOF is a problem, you may ask How does HBase will do when HMaster failed? There two safeguards for SPOF:<br><strong>1. Multi-Master environment</strong><br>HBase can run in multi-master cluster. There is only one active master, when the master is down, the remaining master will take over the master role.<br>It looks like Hadoop HDFS new feature high availability, a standby NameNode will take over the NameNode role when the active one failed.<br><strong>2. Catalog tables are on RegionServers</strong><br>When client send read/write request to HBase, it talks directly to RegionServer, not HMaster. Hence, the cluster can be steady for a time without HMaster. But it requires that the HMaster failover as soon as possible, HMaster is the vital of the cluster.</p>
<h4 id="4-9-Data-Replication"><a href="#4-9-Data-Replication" class="headerlink" title="4.9 Data Replication"></a>4.9 Data Replication</h4><p>For data reliability, HBase replicates data blocks across the cluster. This is achieved by Hadoop HDFS.<br>By default, there 3 replicas:<br>1. The First replica is written to local node.<br>2. The second replica is written to a random node on a different rack.<br>3. The third replica is on a random node on the same rack<br>HBase stores HFiles and WAL on HDFS. When RegionServer is down, new region assignment can be done by read replicas of HFile and replay the replicas the WAL. In short, HBase has high reliability.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/hadoop/">Hadoop</a>, <a href="/categories/hadoop/hbase/">HBase</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/hadoop/">Hadoop</a><a href="/tags/big-data/">Big Data</a><a href="/tags/hbase/">HBase</a>
    </span>
    

    </div>

    
  </div>
</article>

  
	<section id="comments" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'lgrcyanny';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>





    </main>

    <footer class="site-footer">
  <p class="site-info">
    Copyright
    </br>
    
    &copy; 2018 Cyanny Liang
    
  </p>
</footer>
    
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-40624708-1', 'auto');
    ga('send', 'pageview');

</script>

  </div>
</div>
</body>
</html>