<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine Learning Linear Regression | CyannyLive | AI and Big Data</title>

  
  <meta name="author" content="Cyanny Liang">
  

  
  <meta name="description" content="Wisdom comes from inside">
  

  
  
  <meta name="keywords" content="Machine Learning">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Machine Learning Linear Regression"/>

  <meta property="og:site_name" content="CyannyLive"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="CyannyLive" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
<meta name="generator" content="Hexo 5.3.0"></head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">CyannyLive</a>
    </h1>
    <p class="site-description">AI and Big Data</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
        <li><a href="/about/">About</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Machine Learning Linear Regression</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2016/04/04/machine-learning-linear-regression/" rel="bookmark">
        <time class="entry-date published" datetime="2016-04-04T07:55:31.000Z">
          2016-04-04
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>I have been learning the coursera Machine Learning Course by Andrew Ng for two weeks now. Machine Learning is fun and different. For the coursera assignment1 of linear regression, I want to share something.</p>
<a id="more"></a>

<h2 id="Using-matlab"><a href="#Using-matlab" class="headerlink" title="Using matlab"></a>Using matlab</h2><p>I think matlab is better than octave, please use coursera account. <a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/supplement/rANSM/installing-matlab">Install matlab</a></p>
<h2 id="Octave-Install"><a href="#Octave-Install" class="headerlink" title="Octave Install"></a>Octave Install</h2><p>The course use Octave/Matlab for programming practice. I learned octave basics in two days. I don’t have too much time, can just doing these homework in weekends. For Octavel installed on mac, I encounter some problems and solved it. Now octave is 4.2.0, I think ocatve is better now.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install octave</span><br></pre></td></tr></table></figure>
<p>if you encounter some problem, you can solve it as follows:</p>
<ul>
<li>brew update &amp;&amp; brew upgrade</li>
<li>brew tap –repair</li>
<li>brew install octave</li>
<li>install xserver(seems no need to install)<ul>
<li><a target="_blank" rel="noopener" href="http://www.xquartz.org/">http://www.xquartz.org/</a></li>
</ul>
</li>
<li>font can’t find when plot<ul>
<li>export FONTCONFIG_PATH=/opt/X11/lib/X11/fontconfig</li>
</ul>
</li>
<li>can’t plot unknown or ambiguous terminal type; type just ‘set terminal’ for a list<ul>
<li>brew uninstall gnuplot</li>
<li>download and install aquaterm: <a target="_blank" rel="noopener" href="https://sourceforge.net/projects/aquaterm/?source=typ_redirect">https://sourceforge.net/projects/aquaterm/?source=typ_redirect</a></li>
<li>brew install gnuplot –with-aquaterm –with-qt4</li>
</ul>
</li>
<li>add start config to /usr/local/share/octave/site/m/startup/octaverc<ul>
<li>PS1(‘&gt;&gt; ‘)</li>
</ul>
</li>
</ul>
<h2 id="Gradient-Descent-Algorithm"><a href="#Gradient-Descent-Algorithm" class="headerlink" title="Gradient Descent Algorithm"></a>Gradient Descent Algorithm</h2><p>Implementing gradient desenct algorithm in vectorization style was more efficient than iteration algorithm. Here is my implementation:<br>No for loop looks elegant.</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, num_iters)</span></span></span><br><span class="line"><span class="comment">%GRADIENTDESCENT Performs gradient descent to learn theta</span></span><br><span class="line"><span class="comment">%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by</span></span><br><span class="line"><span class="comment">%   taking num_iters gradient steps with learning rate alpha</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize some useful values</span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line">J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</span><br><span class="line"></span><br><span class="line">    <span class="comment">% ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment">% Instructions: Perform a single gradient step on the parameter vector</span></span><br><span class="line">    <span class="comment">%               theta.</span></span><br><span class="line">    <span class="comment">%</span></span><br><span class="line">    <span class="comment">% Hint: While debugging, it can be useful to print out the values</span></span><br><span class="line">    <span class="comment">%       of the cost function (computeCost) and gradient here.</span></span><br><span class="line">    <span class="comment">%</span></span><br><span class="line">    predications = X * theta;</span><br><span class="line">    errors = predications - y; <span class="comment">% m by 1 vector</span></span><br><span class="line">    <span class="comment">% sum_delta = (alpha / m) * sum(errors .* X, 1); % sum by column, which is 1 by n + 1 matrix</span></span><br><span class="line">    <span class="comment">% transpose X, no need sum(errors .* X, 1) here</span></span><br><span class="line">    sum_delta = (alpha / m) .* (X&#x27; * errors);</span><br><span class="line">    theta = theta - sum_delta;</span><br><span class="line"></span><br><span class="line">    <span class="comment">% ============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">% Save the cost J in every iteration</span></span><br><span class="line">    J_history(iter) = computeCost(X, y, theta);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>Another implementation by my wwzyhao</strong><br>[by wwzyhao]</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, num_iters)</span></span></span><br><span class="line"><span class="comment">%GRADIENTDESCENT Performs gradient descent to learn theta</span></span><br><span class="line"><span class="comment">%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by</span></span><br><span class="line"><span class="comment">%   taking num_iters gradient steps with learning rate alpha</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize some useful values</span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line">J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</span><br><span class="line"></span><br><span class="line">    delta = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X, <span class="number">2</span>), <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:m</span><br><span class="line">        x = (X(<span class="built_in">j</span>,:))&#x27;;</span><br><span class="line">        delta = delta + (<span class="number">1</span> / m) * (theta&#x27; * x - y(<span class="built_in">j</span>)) * x;</span><br><span class="line">    <span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line">    theta = theta - alpha * delta;</span><br><span class="line"></span><br><span class="line">    <span class="comment">% ============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">% Save the cost J in every iteration</span></span><br><span class="line">    J_history(iter) = computeCost(X, y, theta);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>Not better than me! haha~</p>
<h2 id="My-assignments-on-github"><a href="#My-assignments-on-github" class="headerlink" title="My assignments on github"></a>My assignments on github</h2><p><a target="_blank" rel="noopener" href="https://github.com/lgrcyanny/MachineLearningCoursera/tree/master/assignments/ex1/ex1">Assignments1</a><br>For submition errors, please refer to<a target="_blank" rel="noopener" href="https://learner.coursera.help/hc/en-us/community/posts/204693179-linear-regression-submit-error">Jacob Middag</a></p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/machine-learning/">Machine Learning</a>
    </span>
    

    </div>

    
  </div>
</article>

  
	<section id="comments" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'lgrcyanny';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>





    </main>

    <footer class="site-footer">
  <p class="site-info">
    Copyright
    </br>
    
    &copy; 2021 Cyanny Liang
    
  </p>
</footer>
    
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-40624708-1', 'auto');
    ga('send', 'pageview');

</script>

  </div>
</div>
</body>
</html>