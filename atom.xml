<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>CyannyLive</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.cyanny.com/"/>
  <updated>2017-04-10T12:46:39.000Z</updated>
  <id>http://www.cyanny.com/</id>
  
  <author>
    <name>Cyanny Liang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Set Up Apache Storm On Mac In 10min</title>
    <link href="http://www.cyanny.com/2017/04/10/set-up-storm-on-mac-in-10min/"/>
    <id>http://www.cyanny.com/2017/04/10/set-up-storm-on-mac-in-10min/</id>
    <published>2017-04-10T12:19:23.000Z</published>
    <updated>2017-04-10T12:46:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>Storm is a great real time streaming system. Recently, my project is about spark streaming. I want to learn storm either to know more about streaming system. Okay, let’s fire up.<br>Today I tried to install storm cluster on my local mac.<br>It was easy to install. It will cost you about 10min.</p>
<a id="more"></a>
<h2 id="1-install-zookeeper"><a href="#1-install-zookeeper" class="headerlink" title="1. install zookeeper"></a>1. install zookeeper</h2><ul>
<li>download <a href="http://www.apache.org/dyn/closer.cgi/zookeeper/" target="_blank" rel="external">zookeeper-3.4.9</a></li>
<li>configure conf/zoo.cfg as follows:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"># The number of milliseconds of each tick</div><div class="line">tickTime=2000</div><div class="line"># The number of ticks that the initial</div><div class="line"># synchronization phase can take</div><div class="line">initLimit=10</div><div class="line"># The number of ticks that can pass between</div><div class="line"># sending a request and getting an acknowledgement</div><div class="line">syncLimit=5</div><div class="line"># the directory where the snapshot is stored.</div><div class="line"># do not use /tmp for storage, /tmp here is just</div><div class="line"># replace it as your local dir</div><div class="line">dataDir=/Users/lgrcyanny/Codelab/zookeeper/zookeeper-3.4.9/zkdata</div><div class="line"># the port at which the clients will connect</div><div class="line">clientPort=2181</div></pre></td></tr></table></figure>
<h2 id="2-install-storm"><a href="#2-install-storm" class="headerlink" title="2. install storm"></a>2. install storm</h2><ul>
<li>download <a href="http://storm.apache.org/downloads.html" target="_blank" rel="external">latest storm 1.1.0</a></li>
<li>configure conf/storm.yaml as follows:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">storm.zookeeper.servers:</div><div class="line">    - &quot;localhost&quot;</div><div class="line">#storm.zookeeper.port:2181</div><div class="line"></div><div class="line">storm.local.dir: &quot;/Users/lgrcyanny/Codelab/storm/apache-storm-1.1.0/storm-local&quot;</div><div class="line"></div><div class="line">#</div><div class="line"># nimbus.seeds: [&quot;host1&quot;, &quot;host2&quot;, &quot;host3&quot;]</div><div class="line">#</div><div class="line">nimbus.seeds: [&quot;localhost&quot;]</div><div class="line"></div><div class="line">supervisor.slots.ports:</div><div class="line">    - 6700</div><div class="line">    - 6701</div><div class="line">    - 6702</div><div class="line">    - 6703</div></pre></td></tr></table></figure>
<p>to understand these config, please refer to: <a href="http://storm.apache.org/releases/1.1.0/Setting-up-a-Storm-cluster.html" target="_blank" rel="external">Setting-up-a-Storm-cluster.html</a></p>
<h2 id="3-start-stom"><a href="#3-start-stom" class="headerlink" title="3. start stom"></a>3. start stom</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># start nimbus</div><div class="line">./bin/storm nimbus</div><div class="line"># start supervisor for workers</div><div class="line">./bin/storm supervisor</div><div class="line"># start ui</div><div class="line">./bin/storm ui</div></pre></td></tr></table></figure>
<p>open <a href="http://localhost:8080" target="_blank" rel="external">http://localhost:8080</a>, you will see storm started:<br><img src="http://wx1.sinaimg.cn/thumb300/761b7938ly1fehv2miunpj21kw0ps0we.jpg" alt="start storm"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Storm is a great real time streaming system. Recently, my project is about spark streaming. I want to learn storm either to know more about streaming system. Okay, let’s fire up.&lt;br&gt;Today I tried to install storm cluster on my local mac.&lt;br&gt;It was easy to install. It will cost you about 10min.&lt;/p&gt;
    
    </summary>
    
    
      <category term="apache storm" scheme="http://www.cyanny.com/tags/apache-storm/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Logistic Regression</title>
    <link href="http://www.cyanny.com/2017/03/25/machine-learning-logistic-regression/"/>
    <id>http://www.cyanny.com/2017/03/25/machine-learning-logistic-regression/</id>
    <published>2017-03-25T13:48:51.000Z</published>
    <updated>2017-03-25T07:32:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>Logistic Regression is for classification problem, and the predication value is fixed descrete values, such as 1 for positive or 0 for negative. The essence of logistic regression is:</p>
<ul>
<li>hypothesis function is sigmoid function</li>
<li>cost function: J(theta)</li>
<li>gradient descent and algorithms</li>
<li>advantanced optimization with regularization to solve overfitting problem.<a id="more"></a>
<h2 id="Basics-about-logistic-regression"><a href="#Basics-about-logistic-regression" class="headerlink" title="Basics about logistic regression"></a>Basics about logistic regression</h2>hypothesis function = 1 / (1 + exp(-htheta(x))),<br>where htheta(x) = theta’ <em> x(theta’ is transpose theta)<br><img src="http://ww2.sinaimg.cn/mw690/761b7938jw1f2rxxio8x0j20v80nit9x.jpg" alt="Sigmoid Function or Logistic Function"><br>htheta(x) mean <em>*Probalitiy that y=1, given x parameterized by theta P(y=1 | x; theta)</em></em>,<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> htheta(x) &gt;= <span class="number">0.5</span>, then y = <span class="number">1</span></div><div class="line"><span class="keyword">if</span> htheta(x) &lt; <span class="number">0.5</span>, then y = <span class="number">0</span></div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Descision-Boundary"><a href="#Descision-Boundary" class="headerlink" title="Descision Boundary"></a>Descision Boundary</h2><p><img src="http://ww3.sinaimg.cn/mw690/761b7938jw1f2rxxhyf4ij20v00ngtbs.jpg" alt="descision boundary"><br>Our goal is the calculate theta, can classify our traing data with descision boundary.<br>In the example, the traning data can be classified into 2 categories by a straight line.<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (theta'x) &gt;= <span class="number">0</span>, then htheta(x) &gt;= <span class="number">0.5</span>, then y = <span class="number">1</span></div><div class="line"><span class="keyword">if</span> (theta'x) &lt; <span class="number">0</span>, then htheta(x) &lt; <span class="number">0.5</span>, then y = <span class="number">0</span></div></pre></td></tr></table></figure></p>
<h2 id="Cost-function-implementation"><a href="#Cost-function-implementation" class="headerlink" title="Cost function implementation"></a>Cost function implementation</h2><p>For the assignment of week3, predicate the adimission by university with 2 exams grade data.<br>I optimize the implementation with vectoriaztion</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J, grad]</span> = <span class="title">costFunction</span><span class="params">(theta, X, y)</span></span></div><div class="line"><span class="comment">%COSTFUNCTION Compute cost and gradient for logistic regression</span></div><div class="line"><span class="comment">%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the</span></div><div class="line"><span class="comment">%   parameter for logistic regression and the gradient of the cost</span></div><div class="line"><span class="comment">%   w.r.t. to the parameters.</span></div><div class="line"></div><div class="line"><span class="comment">% Initialize some useful values</span></div><div class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></div><div class="line"></div><div class="line"><span class="comment">% You need to return the following variables correctly</span></div><div class="line">J = <span class="number">0</span>;</div><div class="line">grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(theta));</div><div class="line"></div><div class="line"><span class="comment">% ====================== YOUR CODE HERE ======================</span></div><div class="line"><span class="comment">% Instructions: Compute the cost of a particular choice of theta.</span></div><div class="line"><span class="comment">%               You should set J to the cost.</span></div><div class="line"><span class="comment">%               Compute the partial derivatives and set grad to the partial</span></div><div class="line"><span class="comment">%               derivatives of the cost w.r.t. each parameter in theta</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">% Note: grad should have the same dimensions as theta</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">% Predications: h_theta(x)</span></div><div class="line">predications = sigmoid(X * theta);</div><div class="line">cost_items = y .* <span class="built_in">log</span>(predications) + (<span class="number">1</span> - y) .* <span class="built_in">log</span>(<span class="number">1</span> - predications);</div><div class="line">J = (<span class="number">-1</span> / m) * sum(cost_items);</div><div class="line"></div><div class="line">grad = (<span class="number">1</span> / m) * (X' * (hypothesis - y));</div><div class="line"></div><div class="line"><span class="comment">% =============================================================</span></div><div class="line"></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<h2 id="Cost-function-with-regularization"><a href="#Cost-function-with-regularization" class="headerlink" title="Cost function with regularization"></a>Cost function with regularization</h2><p>Regularzation is for overfitting problem.</p>
<ul>
<li>underfit: not fit the training data, with high bias between predications and actual value</li>
<li>Just Right: great fit</li>
<li>Overfitting:  often with too many features, not so much traning data, fit traing data well, but with hight variance, predict new data not very well</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J, grad]</span> = <span class="title">costFunctionReg</span><span class="params">(theta, X, y, lambda)</span></span></div><div class="line"><span class="comment">%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization</span></div><div class="line"><span class="comment">%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using</span></div><div class="line"><span class="comment">%   theta as the parameter for regularized logistic regression and the</span></div><div class="line"><span class="comment">%   gradient of the cost w.r.t. to the parameters.</span></div><div class="line"></div><div class="line"><span class="comment">% Initialize some useful values</span></div><div class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></div><div class="line"></div><div class="line"><span class="comment">% You need to return the following variables correctly</span></div><div class="line">J = <span class="number">0</span>;</div><div class="line">grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(theta));</div><div class="line"></div><div class="line"><span class="comment">% ====================== YOUR CODE HERE ======================</span></div><div class="line"><span class="comment">% Instructions: Compute the cost of a particular choice of theta.</span></div><div class="line"><span class="comment">%               You should set J to the cost.</span></div><div class="line"><span class="comment">%               Compute the partial derivatives and set grad to the partial</span></div><div class="line"><span class="comment">%               derivatives of the cost w.r.t. each parameter in theta</span></div><div class="line">hypothesis = sigmoid(X * theta);</div><div class="line">cost_items = (y .* <span class="built_in">log</span>(hypothesis)) + (<span class="number">1</span> - y) .* <span class="built_in">log</span>(<span class="number">1</span> - hypothesis);</div><div class="line"><span class="comment">% don't penalize theta0</span></div><div class="line">reg_theta = [<span class="number">0</span>; theta(<span class="number">2</span>:length(theta))];</div><div class="line">J = (<span class="number">-1</span> / m) * sum(cost_items) + (lambda / (<span class="number">2</span> * m)) * sum(reg_theta .^ <span class="number">2</span>);</div><div class="line"><span class="comment">%grad = (1 / m) * sum((predications - y) .* X)' + (lambda / m) * penalize_theta;</span></div><div class="line">grad = (<span class="number">1</span> / m) * (X' * (hypothesis - y)) + (lambda / m) * reg_theta;</div><div class="line"></div><div class="line"><span class="comment">% =============================================================</span></div><div class="line"></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>the lambda for regularization can’t be too large:</p>
<ul>
<li>large lamba will got very small theta value, and underfit.</li>
<li>small lambda will got large theta velue, and overfit.</li>
<li>the lambda for the exerise is 1</li>
</ul>
<h2 id="Github-assignments"><a href="#Github-assignments" class="headerlink" title="Github assignments"></a>Github assignments</h2><p><a href="https://github.com/lgrcyanny/MachineLearningCoursera/tree/master/assignments/ex2-logistic-regression" target="_blank" rel="external">Week 3 Assignments</a></p>
<h2 id="Write-on-the-last"><a href="#Write-on-the-last" class="headerlink" title="Write on the last"></a>Write on the last</h2><p>After one year, I learn the logistic regression again. Last week, Andrew NG left Baidu. Maybe, these great people thought Baidu is not worth to fight for. Now I still decidated on a Spark project and focus on Spark Streaming. As team leader, I am bearing a great burden and is stressful. It’s a great chance to train my leadership. I am also wondering next opportunity. Learning Machine Learning is right and worth to do. Anyway, even though mist is on the path, just go forward and fight~</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Logistic Regression is for classification problem, and the predication value is fixed descrete values, such as 1 for positive or 0 for negative. The essence of logistic regression is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hypothesis function is sigmoid function&lt;/li&gt;
&lt;li&gt;cost function: J(theta)&lt;/li&gt;
&lt;li&gt;gradient descent and algorithms&lt;/li&gt;
&lt;li&gt;advantanced optimization with regularization to solve overfitting problem.
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://www.cyanny.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>I look down on binary search algorithm</title>
    <link href="http://www.cyanny.com/2017/02/21/binary-search-algorithm/"/>
    <id>http://www.cyanny.com/2017/02/21/binary-search-algorithm/</id>
    <published>2017-02-21T01:29:49.000Z</published>
    <updated>2017-03-05T02:29:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>One day, I wanted to use binary search in one of my feature in my project. My friend said the algorithm was not easy to implement bug free. I did’t believe that. I spent 10min to write it.</p>
<a id="more"></a>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span></span>(list: <span class="type">Array</span>[<span class="type">Int</span>], start: <span class="type">Int</span>, end: <span class="type">Int</span>, x: <span class="type">Int</span>): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</div><div class="line">   <span class="keyword">if</span> (start &lt;= end) &#123;</div><div class="line">     <span class="keyword">val</span> middle = (end - start) / <span class="number">2</span></div><div class="line">    <span class="keyword">if</span> (list(middle) == x) &#123;</div><div class="line">       <span class="type">Some</span>(middle)</div><div class="line">     &#125; <span class="keyword">else</span> <span class="keyword">if</span> (list(middle) &gt; x) &#123;</div><div class="line">       search(list, middle + <span class="number">1</span>, end, x)</div><div class="line">     &#125; <span class="keyword">else</span> &#123;</div><div class="line">       search(list, start, middle - <span class="number">1</span>, x)</div><div class="line">     &#125;</div><div class="line">   &#125; <span class="keyword">else</span> &#123;</div><div class="line">     <span class="type">None</span></div><div class="line">   &#125;</div><div class="line"> &#125;</div><div class="line"></div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">   <span class="keyword">val</span> list = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</div><div class="line">   println(search(list, <span class="number">0</span>, list.size - <span class="number">1</span>, <span class="number">5</span>))</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<p>Ooh, definitly my code has bug, yes I admitted that it was not very easy to implement binary search bug free.<br>I revised it.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span></span>(list: <span class="type">Array</span>[<span class="type">Int</span>], start: <span class="type">Int</span>, end: <span class="type">Int</span>, x: <span class="type">Int</span>): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</div><div class="line">    <span class="keyword">if</span> (start &lt;= end) &#123;</div><div class="line">      <span class="keyword">val</span> middle = (end - start) / <span class="number">2</span> + start <span class="comment">// bug 1, without plus start</span></div><div class="line">      <span class="keyword">if</span> (list(middle) == x) &#123;</div><div class="line">        <span class="type">Some</span>(middle)</div><div class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (list(middle) &gt; x) &#123;   <span class="comment">// bug2, when middle bigger than x, not search middle+1,end</span></div><div class="line">        search(list, start, middle - <span class="number">1</span>, x)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        search(list, middle + <span class="number">1</span>, end, x)</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="type">None</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> list = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</div><div class="line">    println(search(list, <span class="number">0</span>, list.size - <span class="number">1</span>, <span class="number">5</span>))</div><div class="line">    println(search(list, <span class="number">0</span>, list.size - <span class="number">1</span>, <span class="number">11</span>))</div><div class="line">    println(search(list, <span class="number">0</span>, list.size - <span class="number">1</span>, <span class="number">8</span>))</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">output:</div><div class="line">Some(4)</div><div class="line">None</div><div class="line">Some(7)</div></pre></td></tr></table></figure>
<p>It was an interesting problem. I should tain my programming skills more.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;One day, I wanted to use binary search in one of my feature in my project. My friend said the algorithm was not easy to implement bug free. I did’t believe that. I spent 10min to write it.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Algorithm" scheme="http://www.cyanny.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>My Booklist and Reservations for 2017</title>
    <link href="http://www.cyanny.com/2017/01/22/booklist-for-2017/"/>
    <id>http://www.cyanny.com/2017/01/22/booklist-for-2017/</id>
    <published>2017-01-22T02:18:09.000Z</published>
    <updated>2017-03-05T02:29:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>一直没有写关于2016的回顾，有很多方面吧。2016年发生很多事儿的一年，对于技术上的发展也有了新的思考，搞技术不再是死磕某种工具、算法或bug，其实本质上是为了解决问题或者做更好的产品。虽然我做的不是具体的产品而是底层的工具和平台，但这些工具的出口也是依赖”pillar application”, 多想想也是好处的。</p>
<p>2016工作忙，读的书没有很多，但想想扎克伯克比我们还忙一年能挑战23本书确实很牛，其实自己的时间管理是不太到位的，大部分周末都懒散睡觉或者出去逛街了，回归2016年，读的书们：<a id="more"></a></p>
<ul>
<li><strong>“Programming in scala, 2nd”</strong></li>
</ul>
<p>第一次啃一本大部头英文书883页，我都没有勇气打印出来，看的电子版，目前2017年初，走到第703页，成功在望。看完这个书，对scala的理解和应用，已经超过我的C++和Java技能了</p>
<ul>
<li><strong>Spark源码解析, 许鹏著</strong></li>
</ul>
<p>如果你问我学spark该看哪本书，我不会给你推荐这个的。不是说它不好，而是应该好好看官方doc. 不过这个书对我理解spark还是带来了很大帮助，但是spark1.0系列的，只能对着spark1.6和spark2.0的源码看，然后看看当年的大神们怎么设计的。许鹏的书里贴了一些关键代码，虽然贴代码占页数有点不厚道，但书叫源码解析所以也就忍了</p>
<ul>
<li><strong>Python Tutorial</strong></li>
</ul>
<p>我想学python很久了，这个Tutorial我看了好多遍，其实真正到实际应用中，我才真正学会了python</p>
<ul>
<li><strong>深入理解Java虚拟机</strong></li>
</ul>
<p>JVM的虚拟机调优部分看了，还有很多没看完，用到再查</p>
<ul>
<li><strong>spark的各种论文，PPT等</strong></li>
</ul>
<p>论文比较分散，有spark sql, spark rdd, spark streaming等，各种PPT也比较分散</p>
<ul>
<li><strong>人间词话</strong></li>
</ul>
<p>有时候看看诗，心里很开心。比如年终遇到一些事儿的时候喜欢一句诗：”天长水阔知何处”，心里那个纠结</p>
<ul>
<li><strong>Dunes: 沙丘，英文版</strong></li>
</ul>
<p>科幻大神的书，挑战了英文文学，虽然磕磕绊绊看完了，但现在我可以自信的看完大部头的英文原版文学书啦。之前看过英文的哈利波特，安德游戏，觉得比较幼稚没看下去。沙丘是比较对我胃口的一本</p>
<ul>
<li><strong>倚天屠龙记</strong></li>
</ul>
<p>我童年在教科书和教辅中度过，这么好的书这么大了才看，真心停不下来</p>
<ul>
<li><strong>百万富翁，Mark Twain, 英文版</strong></li>
</ul>
<p>喜欢反转的剧情，喜欢Mark Twain的文笔</p>
<ul>
<li><strong>各种博客，news</strong></li>
</ul>
<p>我手机里有将近30个news app，我是多爱看news，喜欢的包括Business Insider, Hack News, QZ.com, 36氪，钛媒体，推酷等，一般好的news分析我放到pocket里。当然还喜欢听喜马拉雅的段子。再觉得其实看news没有看书好，多看书好</p>
<p>2017年啦，上面还有programming in scala, 人间词话还没看完，新的书又来啦，虽然写打算我不一定看，但列一下我的打算吧：</p>
<ul>
<li>机器学习Coursera课程</li>
</ul>
<p>机器学习，AI这么火，我2017年要把这个课程学完</p>
<ul>
<li>Creativity, Inc. Overcoming the Unseen Forces That Stand in the Way of True Inspiration</li>
<li>从0到1</li>
<li>make users awesome</li>
<li>platform revolution</li>
<li>奇点系列：<a href="https://book.douban.com/series/27014" target="_blank" rel="external">https://book.douban.com/series/27014</a></li>
<li>各种科幻和悬疑发现中</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一直没有写关于2016的回顾，有很多方面吧。2016年发生很多事儿的一年，对于技术上的发展也有了新的思考，搞技术不再是死磕某种工具、算法或bug，其实本质上是为了解决问题或者做更好的产品。虽然我做的不是具体的产品而是底层的工具和平台，但这些工具的出口也是依赖”pillar application”, 多想想也是好处的。&lt;/p&gt;
&lt;p&gt;2016工作忙，读的书没有很多，但想想扎克伯克比我们还忙一年能挑战23本书确实很牛，其实自己的时间管理是不太到位的，大部分周末都懒散睡觉或者出去逛街了，回归2016年，读的书们：
    
    </summary>
    
    
      <category term="Learning" scheme="http://www.cyanny.com/tags/learning/"/>
    
  </entry>
  
  <entry>
    <title>Scala Collections</title>
    <link href="http://www.cyanny.com/2016/11/07/scala-collections/"/>
    <id>http://www.cyanny.com/2016/11/07/scala-collections/</id>
    <published>2016-11-07T02:09:10.000Z</published>
    <updated>2017-03-05T02:28:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>In scala there are many fancy collections with great utilities. Here are some key notes for scala collections which did a great help to me.<br><a id="more"></a></p>
<h1 id="Collections-Hierarchy"><a href="#Collections-Hierarchy" class="headerlink" title="Collections Hierarchy"></a>Collections Hierarchy</h1><p><img src="http://ww1.sinaimg.cn/mw1024/761b7938jw1f9jbrdeugkj20ku0q041j.jpg" alt="collection hierarchy"></p>
<h1 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h1><p>Collections have two kinds:</p>
<pre><code>- mutable collections
- immutable collections
</code></pre><h2 id="Immutable-Collections"><a href="#Immutable-Collections" class="headerlink" title="Immutable Collections"></a>Immutable Collections</h2><ul>
<li><p>Lists are finite immutable sequences. They provide constant-time access to their first element as well as the rest of the list</p>
</li>
<li><p>A stream is like a list except that its elements are computed lazily. Because of this, a stream can be infinitely long. for example:</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> s = <span class="number">1</span> #:: <span class="number">2</span> #:: <span class="number">3</span> #:: <span class="type">Stream</span>.empty</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span></span>(m: <span class="type">Int</span>, n: <span class="type">Int</span>): <span class="type">Stream</span>[<span class="type">Int</span>] = m #:: fib(n, m + n)</div></pre></td></tr></table></figure>
<ul>
<li><p>Vectors are a new collection type in Scala 2.8 that give efficient access to elements beyond the head.</p>
<ul>
<li>Access to any elements of a vector take only “effectively constant time,” as defined below.</li>
<li>shallow trees</li>
<li>when only one level, store 32 elements in an array</li>
<li>if lager than 32, grow to 2 levels, each node in level 2 has 32 elements, and level 1 store 32 pointers, now level 2 has 2^10 elements</li>
<li>level 3 has 2^ 15 elements</li>
<li>to access an element, the complexity is log32(N)</li>
<li>Vector have very decent random access performance</li>
<li>The default implementation to immutable IndexedSeq</li>
</ul>
</li>
<li><p>Stack</p>
<ul>
<li>first-in-last-out</li>
<li>you can use ArrayBuffer to implement a Stack<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> emtpyStack = <span class="type">Stack</span>.empty</div><div class="line"><span class="keyword">val</span> hasOne = emptyStack.push(<span class="number">1</span>)</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Immutable queues</p>
</li>
<li><p>Ranges</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> range = <span class="number">1</span> to <span class="number">10</span></div></pre></td></tr></table></figure>
</li>
<li><p>Hash Tries</p>
<ul>
<li>Hash tries4 are a standard way to implement immutable sets and maps efficiently.</li>
</ul>
</li>
<li><p>RedBlackTrees</p>
<ul>
<li>Red-black trees are a form of balanced binary trees where some nodes are designated “red” and others “black.”</li>
<li>TreeSet</li>
<li>TreeMap</li>
<li>default implementation for SortedSet</li>
</ul>
</li>
<li><p>Immutable bit sets<br><img src="http://ww2.sinaimg.cn/mw690/761b7938jw1f9jc4ydow0j20r208276p.jpg" alt="bit set example"></p>
<ul>
<li>Operations on bit sets are very fast. Testing for inclusion takes constant time. Adding an item to the set takes time proportional to the number of Longs in the bit set’s array, which is typically a small number.</li>
</ul>
</li>
<li><p>ListMap</p>
<ul>
<li>The only possible difference is if the map is for some reason constructed in such a way that the first elements in the list are selected much more often than the other elements.</li>
</ul>
</li>
</ul>
<h2 id="Mutable-Collections"><a href="#Mutable-Collections" class="headerlink" title="Mutable Collections"></a>Mutable Collections</h2><ul>
<li><p>Array buffer: operations simply access and modify the underlying array.</p>
</li>
<li><p>List Buffer: A list buffer is like an array buffer except that it uses a linked list internally instead of an array.</p>
<ul>
<li>you plan to convert the buffer to a list once it is built up, use a list buffer instead of an array buffer.</li>
</ul>
</li>
<li><p>StringBuilder:  a string builder is useful for building strings</p>
</li>
<li><p>LinkedList: Linked lists are mutable sequences that consist of nodes that are linked with next pointer</p>
<ul>
<li>use  LinkedList.empty.isEmpty for empty list</li>
<li>linked lists are best operated on sequen- tially. In addition, linked lists make it easy to insert an element or linked list into another linked list.</li>
</ul>
</li>
<li><p>Double Linked List: The main benefit of that additional link is that it makes element removal very fast</p>
</li>
<li><p>Mutable List:</p>
<ul>
<li>A MutableList consists of a single linked list together with a pointer that refers to the terminal empty node of that list.</li>
<li>The default implementation for LinearSeq</li>
</ul>
</li>
<li><p>Queue</p>
<ul>
<li>the dequeue method will just remove the head element from the queue and return it</li>
</ul>
</li>
<li><p>Array Sequences</p>
<ul>
<li>A class for polymorphic arrays of elements that’s represented internally by an array of objects</li>
<li>Array sequences are mutable sequences of fixed size that store their elements internally in an Array[AnyRef]</li>
</ul>
</li>
<li><p>Stack</p>
<ul>
<li>It works exactly the same as the immutable version except that modifications happen in place</li>
</ul>
</li>
<li><p>ArrayStack</p>
<ul>
<li>ArrayStack is an alternative implementation of a mutable stack, which is backed by an Array that gets resized as needed</li>
<li>It provides fast indexing and is generally slightly more efficient for most operations than a normal mutable stack.</li>
</ul>
</li>
<li><p>HashTable</p>
<ul>
<li>A hash table stores its elements in an underlying array, placing each item at a position in the array determined by the hash code of that item.</li>
<li>As a result, the default mutable map and set types in Scala are based on hash tables.</li>
<li>HashMap, HashSet implements with hash tables in array</li>
<li>Iteration over a hash table is not guaranteed to occur in any particular order.<ul>
<li>To get a guaranteed iteration order, use a linked hash map or set instead of a regular one.</li>
<li>Iteration over such a collection is always in the same order that the elements were initially added.</li>
</ul>
</li>
</ul>
</li>
<li><p>Weak Hash Maps</p>
<ul>
<li>A weak hash map is a special kind of hash map in which the garbage collector does not follow links from the map to the keys stored in it</li>
<li>This means that a key and its associated value will disappear from the map if there is no other reference to that key</li>
<li>Weak hash maps are useful for tasks such as caching, where you want to re-use an expensive function’s result if the function is called again on the same key</li>
<li>Weak hash maps in Scala are implemented as a wrapper of an underlying Java implementation, java.util.WeakHashMap.</li>
</ul>
</li>
<li><p>Concurrent Maps</p>
<ul>
<li>A concurrent map can be accessed by several threads at once.</li>
<li>Currently, its only implementation is Java’s java.util.concurrent.ConcurrentMap</li>
</ul>
</li>
<li><p>BitSet</p>
<ul>
<li>Mutable bit sets are slightly more efficient at updating than immutable ones, because they don’t have to copy around Longs that haven’t changed.</li>
</ul>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In scala there are many fancy collections with great utilities. Here are some key notes for scala collections which did a great help to me.&lt;br&gt;
    
    </summary>
    
    
      <category term="Scala" scheme="http://www.cyanny.com/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>春江花月夜</title>
    <link href="http://www.cyanny.com/2016/09/27/%E6%98%A5%E6%B1%9F%E8%8A%B1%E6%9C%88%E5%A4%9C/"/>
    <id>http://www.cyanny.com/2016/09/27/春江花月夜/</id>
    <published>2016-09-27T01:35:59.000Z</published>
    <updated>2017-03-05T02:27:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>有人说张若虚的这首诗很值得背诵下来<br><a id="more"></a></p>
<h2 id="春江花月夜"><a href="#春江花月夜" class="headerlink" title="春江花月夜"></a>春江花月夜</h2><hr>
<blockquote>
<p>春江潮水连海平，海上明月共潮生。<br>滟滟随波千万里，何处春江无月明！<br>江流宛转绕芳甸，月照花林皆似霰;<br>空里流霜不觉飞，汀上白沙看不见。<br>江天一色无纤尘，皎皎空中孤月轮。<br>江畔何人初见月？江月何年初照人？<br>人生代代无穷已，江月年年只相似。<br>不知江月待何人，但见长江送流水。<br>白云一片去悠悠，青枫浦上不胜愁。<br>谁家今夜扁舟子？何处相思明月楼？<br>可怜楼上月徘徊，应照离人妆镜台。<br>玉户帘中卷不去，捣衣砧上拂还来。<br>此时相望不相闻，愿逐月华流照君。<br>鸿雁长飞光不度，鱼龙潜跃水成文。<br>昨夜闲潭梦落花，可怜春半不还家。<br>江水流春去欲尽，江潭落月复西斜。<br>斜月沉沉藏海雾，碣石潇湘无限路。<br>不知乘月几人归，落月摇情满江树。</p>
<hr>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有人说张若虚的这首诗很值得背诵下来&lt;br&gt;
    
    </summary>
    
    
      <category term="诗话" scheme="http://www.cyanny.com/tags/%E8%AF%97%E8%AF%9D/"/>
    
  </entry>
  
  <entry>
    <title>Eight Queens Problem In Scala</title>
    <link href="http://www.cyanny.com/2016/09/27/eight-queens-problem-in-scala/"/>
    <id>http://www.cyanny.com/2016/09/27/eight-queens-problem-in-scala/</id>
    <published>2016-09-27T01:15:28.000Z</published>
    <updated>2017-03-05T02:29:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>I have dedicated in <strong>Programming in Scala</strong> for about 4 months. My work is busy, but I can’t give up reading more books.<br>Scala is a fabulous language, both object oriented and functional.<br>Eight qeens problem can be expressed in scala easily and concise.<br><a id="more"></a></p>
<h2 id="Eight-Queens-Problem"><a href="#Eight-Queens-Problem" class="headerlink" title="Eight Queens Problem"></a>Eight Queens Problem</h2><p>Given a standard chess-board, place eight queens such that no queen is in check from any other (a queen can check another piece if they are on the same column, row, or diagonal)</p>
<h2 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h2><p>The problem in scala is recursively.</p>
<ol>
<li><p>First each solution is a List[(Row, Column)]</p>
<pre><code>- Each element is a coordinated, the queen position in each row
- The coordicate for row k comes first, followed by row `k-1`, `k-2`, ... 0
</code></pre></li>
<li><p>Use a Set[List[Row, Column]], represent all solutions</p>
</li>
<li>To place next <code>k+1</code> qeen, we iterate all solutions, if match the condition, yield another list</li>
</ol>
<h2 id="Lets-run-the-code"><a href="#Lets-run-the-code" class="headerlink" title="Lets run the code"></a>Lets run the code</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * the coordinates of the queen in row k comes first in each List[(ROW, Column)], followed</div><div class="line">    * by k -1, k - 2, ..., 0 and so on</div><div class="line">    *</div><div class="line">    * @param n</div><div class="line">    * @return</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">queens</span></span>(n: <span class="type">Int</span>): <span class="type">Set</span>[<span class="type">List</span>[(<span class="type">Row</span>, <span class="type">Column</span>)]] = &#123;</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">placeQueen</span></span>(k: <span class="type">Int</span>): <span class="type">Set</span>[<span class="type">List</span>[(<span class="type">Row</span>, <span class="type">Column</span>)]] = &#123;</div><div class="line">      <span class="keyword">if</span> (k &lt; <span class="number">0</span>) &#123;</div><div class="line">        <span class="type">Set</span>(<span class="type">List</span>())</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">for</span> &#123;</div><div class="line">          queens &lt;- placeQueen(k - <span class="number">1</span>)</div><div class="line">          column &lt;- <span class="number">0</span> until n</div><div class="line">          queen = (k, column)</div><div class="line">          <span class="keyword">if</span> isSafe(queen, queens)</div><div class="line">        &#125; <span class="keyword">yield</span> queen :: queens</div><div class="line">      &#125;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSafe</span></span>(queen: (<span class="type">Row</span>, <span class="type">Column</span>), queens: <span class="type">List</span>[(<span class="type">Row</span>, <span class="type">Column</span>)]): <span class="type">Boolean</span> = &#123;</div><div class="line">      queens.forall &#123; placedQueen =&gt;</div><div class="line">        placedQueen._1 != queen._1 &amp;&amp;</div><div class="line">          placedQueen._2 != queen._2 &amp;&amp;</div><div class="line">          (<span class="type">Math</span>.abs(placedQueen._1 - queen._1) !=</div><div class="line">            <span class="type">Math</span>.abs(placedQueen._2 - queen._2))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    placeQueen(n - <span class="number">1</span>)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h2 id="Github-Link"><a href="#Github-Link" class="headerlink" title="Github Link"></a>Github Link</h2><p><a href="https://github.com/lgrcyanny/ScalaPractice/blob/master/ProgrammingInScala/src/main/scala/com/chapter23/EightQueens.scala" target="_blank" rel="external">Eight Queens</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/lgrcyanny/ScalaPractice.git</div><div class="line"></div><div class="line">mvn clean package</div><div class="line"></div><div class="line">cd ProgrammingInScala</div><div class="line"></div><div class="line">scala -cp target/programming-in-scala-1.0-SNAPSHOT.jar com.chapter23.EightQueens</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I have dedicated in &lt;strong&gt;Programming in Scala&lt;/strong&gt; for about 4 months. My work is busy, but I can’t give up reading more books.&lt;br&gt;Scala is a fabulous language, both object oriented and functional.&lt;br&gt;Eight qeens problem can be expressed in scala easily and concise.&lt;br&gt;
    
    </summary>
    
    
      <category term="Scala" scheme="http://www.cyanny.com/tags/scala/"/>
    
      <category term="Algorithm" scheme="http://www.cyanny.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Neural Networks</title>
    <link href="http://www.cyanny.com/2016/04/17/machine-learning-neural-networks/"/>
    <id>http://www.cyanny.com/2016/04/17/machine-learning-neural-networks/</id>
    <published>2016-04-17T13:29:20.000Z</published>
    <updated>2017-03-05T02:27:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>This week is about the mysterious Neural Networks. The courses in this week just explain the basics about Neural Networks.</p>
<h2 id="What-is-Neural-Networks"><a href="#What-is-Neural-Networks" class="headerlink" title="What is Neural Networks"></a>What is Neural Networks</h2><p>It’s a technique to train our data based on how human brains works. A simple Neural Network has:</p>
<ul>
<li>input layer</li>
<li>hidden layer</li>
<li>output layer</li>
</ul>
<p>We use Neural NetWorks to make classification and regression.<br>We use sigmoid function the map data from input layer to hidden layer then the output layer, the function is called activation function.<br><a id="more"></a><br><img src="http://ww3.sinaimg.cn/mw690/761b7938jw1f3019b35a2j21380kcn1d.jpg" alt="Neural Network"></p>
<p>In Neural Network, we add bias unit, x0, a1 to do calculate.<br>With Neural Network, we build more complex hypothesis function.<br><img src="http://ww4.sinaimg.cn/mw690/761b7938jw1f301cv572fj214o0m2teg.jpg" alt="Neural Network"></p>
<p>To play with neural network, you can try google’s open source <a href="http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.28657&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification" target="_blank" rel="external">tensorflow</a></p>
<h2 id="Handwritten-Digital-Classification"><a href="#Handwritten-Digital-Classification" class="headerlink" title="Handwritten Digital Classification"></a>Handwritten Digital Classification</h2><p>This week’s assignment is to do multi classification on handwritten recognize.</p>
<p><strong>Do multi classification with one-vs-all logistic regression</strong><br>For handwritens in 10 lables: 0~9, we do 10 regression regression to calculate 10 group of theta. And then make 10 predications base on these 10 group of theta, choose the lable with max hypothesis value（probaility value）</p>
<p><strong>1. Cost function</strong><br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J, grad]</span> = <span class="title">lrCostFunction</span><span class="params">(theta, X, y, lambda)</span></span></div><div class="line">reg_theta = [<span class="number">0</span>;theta(<span class="number">2</span>:end)];</div><div class="line">predictions = sigmoid(X * theta);</div><div class="line">J = (<span class="number">-1</span> / m) * sum((y .* <span class="built_in">log</span>(predictions) + (<span class="number">1</span> - y) .* <span class="built_in">log</span>(<span class="number">1</span> - predictions))) + (lambda / (<span class="number">2</span> * m)) * sum(reg_theta .^ <span class="number">2</span>);</div><div class="line">grad = (<span class="number">1</span> / m) * (X' * (predictions - y)) + (lambda / m) * reg_theta;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure></p>
<p><strong>2. OneVsAll</strong><br>make 10 classifications<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[all_theta]</span> = <span class="title">oneVsAll</span><span class="params">(X, y, num_labels, lambda)</span></span></div><div class="line"><span class="comment">% Some useful variables</span></div><div class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>);</div><div class="line">n = <span class="built_in">size</span>(X, <span class="number">2</span>);</div><div class="line"></div><div class="line"><span class="comment">% You need to return the following variables correctly</span></div><div class="line">all_theta = <span class="built_in">zeros</span>(num_labels, n + <span class="number">1</span>);</div><div class="line"></div><div class="line"><span class="comment">% Add ones to the X data matrix</span></div><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line"></div><div class="line"><span class="comment">% Note: For this assignment, we recommend using fmincg to optimize the cost</span></div><div class="line"><span class="comment">%       function. It is okay to use a for-loop (for c = 1:num_labels) to</span></div><div class="line"><span class="comment">%       loop over the different classes.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">%       fmincg works similarly to fminunc, but is more efficient when we</span></div><div class="line"><span class="comment">%       are dealing with large number of parameters.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:num_labels</div><div class="line">    initial_theta = <span class="built_in">zeros</span>(n + <span class="number">1</span>, <span class="number">1</span>);</div><div class="line">    options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="number">50</span>);</div><div class="line">    <span class="comment">% theta is a column vector</span></div><div class="line">    <span class="comment">% Run fmincg to obtain the optimal theta</span></div><div class="line">    [theta] = fmincg(@(t)(lrCostFunction(t, X, (y == <span class="built_in">i</span>), lambda)), ...</div><div class="line">                    initial_theta, options);</div><div class="line">    all_theta(<span class="built_in">i</span>, :) = theta';</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure></p>
<p><strong>3. PredictOneVsAll</strong><br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span> = <span class="title">predictOneVsAll</span><span class="params">(all_theta, X)</span></span></div><div class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>);</div><div class="line">num_labels = <span class="built_in">size</span>(all_theta, <span class="number">1</span>);</div><div class="line"></div><div class="line"><span class="comment">% You need to return the following variables correctly</span></div><div class="line">p = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X, <span class="number">1</span>), <span class="number">1</span>);</div><div class="line"></div><div class="line"><span class="comment">% Add ones to the X data matrix</span></div><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line">predictions = X * all_theta';</div><div class="line"><span class="comment">% calculate max of each row</span></div><div class="line">[max_predictions, p] = max(predictions, [], <span class="number">2</span>);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure></p>
<p>The logistic has great accurracy, about 95% in this case, but neural network will have higher accuracy, about 97%.</p>
<h2 id="Neural-Forward-Propagation-algorithm"><a href="#Neural-Forward-Propagation-algorithm" class="headerlink" title="Neural Forward Propagation algorithm"></a>Neural Forward Propagation algorithm</h2><p>In the assignment, it build hypothesis function with 3 layers neural network.<br>The predications implementation</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span> = <span class="title">predict</span><span class="params">(Theta1, Theta2, X)</span></span></div><div class="line"><span class="comment">%PREDICT Predict the label of an input given a trained neural network</span></div><div class="line"><span class="comment">%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the</span></div><div class="line"><span class="comment">%   trained weights of a neural network (Theta1, Theta2)</span></div><div class="line"></div><div class="line"><span class="comment">% Useful values</span></div><div class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>);</div><div class="line">num_labels = <span class="built_in">size</span>(Theta2, <span class="number">1</span>);</div><div class="line"></div><div class="line"><span class="comment">% You need to return the following variables correctly </span></div><div class="line">p = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X, <span class="number">1</span>), <span class="number">1</span>);</div><div class="line"></div><div class="line"><span class="comment">% ====================== YOUR CODE HERE ======================</span></div><div class="line"><span class="comment">% Instructions: Complete the following code to make predictions using</span></div><div class="line"><span class="comment">%               your learned neural network. You should set p to a </span></div><div class="line"><span class="comment">%               vector containing labels between 1 to num_labels.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">% Hint: The max function might come in useful. In particular, the max</span></div><div class="line"><span class="comment">%       function can also return the index of the max element, for more</span></div><div class="line"><span class="comment">%       information see 'help max'. If your examples are in rows, then, you</span></div><div class="line"><span class="comment">%       can use max(A, [], 2) to obtain the max for each row.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">% Theta1 is 25 * 401, X is 5000 * 401</span></div><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line"><span class="comment">% z2 is 5000 * 25</span></div><div class="line">z2 = X * Theta1';</div><div class="line">a2 = sigmoid(z2);</div><div class="line"><span class="comment">% a2 with bias unit is 5000 * 26</span></div><div class="line">a2 = [ones(m, <span class="number">1</span>) a2];</div><div class="line"></div><div class="line"><span class="comment">% Theta2 is 10 * 26</span></div><div class="line"><span class="comment">% z3 is 5000 * 10</span></div><div class="line">z3 = a2 * Theta2';</div><div class="line">a3 = sigmoid(z3);</div><div class="line">[max_valid, p] = max(a3, [], <span class="number">2</span>);</div><div class="line"></div><div class="line"><span class="comment">% =========================================================================</span></div><div class="line"></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>My question is:</p>
<ul>
<li>how to train the Theta1, Theta2</li>
<li>how to decide how many units in hidden layer<br>In the later course, I think NG will explain it. Next week, I will learn backpropagation algorithm.</li>
</ul>
<h2 id="My-assignment"><a href="#My-assignment" class="headerlink" title="My assignment"></a>My assignment</h2><p><a href="https://github.com/lgrcyanny/MachineLearningCoursera/tree/master/assignments" target="_blank" rel="external">Week Assignments</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This week is about the mysterious Neural Networks. The courses in this week just explain the basics about Neural Networks.&lt;/p&gt;
&lt;h2 id=&quot;What-is-Neural-Networks&quot;&gt;&lt;a href=&quot;#What-is-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;What is Neural Networks&quot;&gt;&lt;/a&gt;What is Neural Networks&lt;/h2&gt;&lt;p&gt;It’s a technique to train our data based on how human brains works. A simple Neural Network has:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input layer&lt;/li&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;output layer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use Neural NetWorks to make classification and regression.&lt;br&gt;We use sigmoid function the map data from input layer to hidden layer then the output layer, the function is called activation function.&lt;br&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://www.cyanny.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Linear Regression</title>
    <link href="http://www.cyanny.com/2016/04/04/machine-learning-linear-regression/"/>
    <id>http://www.cyanny.com/2016/04/04/machine-learning-linear-regression/</id>
    <published>2016-04-04T07:55:31.000Z</published>
    <updated>2017-03-05T02:27:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>I have been learning the coursera Machine Learning Course by Andrew Ng for two weeks now. Machine Learning is fun and different. For the coursera assignment1 of linear regression, I want to share something.<br><a id="more"></a></p>
<h2 id="Using-matlab"><a href="#Using-matlab" class="headerlink" title="Using matlab"></a>Using matlab</h2><p>I think matlab is better than octave, please use coursera account. <a href="https://www.coursera.org/learn/machine-learning/supplement/rANSM/installing-matlab" target="_blank" rel="external">Install matlab</a></p>
<h2 id="Octave-Install"><a href="#Octave-Install" class="headerlink" title="Octave Install"></a>Octave Install</h2><p>The course use Octave/Matlab for programming practice. I learned octave basics in two days. I don’t have too much time, can just doing these homework in weekends. For Octavel installed on mac, I encounter some problems and solved it. Now octave is 4.2.0, I think ocatve is better now.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew install octave</div></pre></td></tr></table></figure>
<p>if you encounter some problem, you can solve it as follows:</p>
<ul>
<li>brew update &amp;&amp; brew upgrade</li>
<li>brew tap –repair</li>
<li>brew install octave</li>
<li>install xserver(seems no need to install)<ul>
<li><a href="http://www.xquartz.org/" target="_blank" rel="external">http://www.xquartz.org/</a></li>
</ul>
</li>
<li>font can’t find when plot<ul>
<li>export FONTCONFIG_PATH=/opt/X11/lib/X11/fontconfig</li>
</ul>
</li>
<li>can’t plot unknown or ambiguous terminal type; type just ‘set terminal’ for a list<ul>
<li>brew uninstall gnuplot</li>
<li>download and install aquaterm: <a href="https://sourceforge.net/projects/aquaterm/?source=typ_redirect" target="_blank" rel="external">https://sourceforge.net/projects/aquaterm/?source=typ_redirect</a></li>
<li>brew install gnuplot –with-aquaterm –with-qt4</li>
</ul>
</li>
<li>add start config to /usr/local/share/octave/site/m/startup/octaverc<ul>
<li>PS1(‘&gt;&gt; ‘)</li>
</ul>
</li>
</ul>
<h2 id="Gradient-Descent-Algorithm"><a href="#Gradient-Descent-Algorithm" class="headerlink" title="Gradient Descent Algorithm"></a>Gradient Descent Algorithm</h2><p>Implementing gradient desenct algorithm in vectorization style was more efficient than iteration algorithm. Here is my implementation:<br>No for loop looks elegant.</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, num_iters)</span></span></div><div class="line"><span class="comment">%GRADIENTDESCENT Performs gradient descent to learn theta</span></div><div class="line"><span class="comment">%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by</span></div><div class="line"><span class="comment">%   taking num_iters gradient steps with learning rate alpha</span></div><div class="line"></div><div class="line"><span class="comment">% Initialize some useful values</span></div><div class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></div><div class="line">J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</div><div class="line"></div><div class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</div><div class="line"></div><div class="line">    <span class="comment">% ====================== YOUR CODE HERE ======================</span></div><div class="line">    <span class="comment">% Instructions: Perform a single gradient step on the parameter vector</span></div><div class="line">    <span class="comment">%               theta.</span></div><div class="line">    <span class="comment">%</span></div><div class="line">    <span class="comment">% Hint: While debugging, it can be useful to print out the values</span></div><div class="line">    <span class="comment">%       of the cost function (computeCost) and gradient here.</span></div><div class="line">    <span class="comment">%</span></div><div class="line">    predications = X * theta;</div><div class="line">    errors = predications - y; <span class="comment">% m by 1 vector</span></div><div class="line">    <span class="comment">% sum_delta = (alpha / m) * sum(errors .* X, 1); % sum by column, which is 1 by n + 1 matrix</span></div><div class="line">    <span class="comment">% transpose X, no need sum(errors .* X, 1) here</span></div><div class="line">    sum_delta = (alpha / m) .* (X' * errors);</div><div class="line">    theta = theta - sum_delta;</div><div class="line"></div><div class="line">    <span class="comment">% ============================================================</span></div><div class="line"></div><div class="line">    <span class="comment">% Save the cost J in every iteration</span></div><div class="line">    J_history(iter) = computeCost(X, y, theta);</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p><strong>Another implementation by my wwzyhao</strong><br>[by wwzyhao]<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, num_iters)</span></span></div><div class="line"><span class="comment">%GRADIENTDESCENT Performs gradient descent to learn theta</span></div><div class="line"><span class="comment">%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by</span></div><div class="line"><span class="comment">%   taking num_iters gradient steps with learning rate alpha</span></div><div class="line"></div><div class="line"><span class="comment">% Initialize some useful values</span></div><div class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></div><div class="line">J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</div><div class="line"></div><div class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</div><div class="line"></div><div class="line">    delta = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X, <span class="number">2</span>), <span class="number">1</span>);</div><div class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:m</div><div class="line">        x = (X(<span class="built_in">j</span>,:))';</div><div class="line">        delta = delta + (<span class="number">1</span> / m) * (theta' * x - y(<span class="built_in">j</span>)) * x;</div><div class="line">    <span class="keyword">end</span>;</div><div class="line"></div><div class="line">    theta = theta - alpha * delta;</div><div class="line"></div><div class="line">    <span class="comment">% ============================================================</span></div><div class="line"></div><div class="line">    <span class="comment">% Save the cost J in every iteration</span></div><div class="line">    J_history(iter) = computeCost(X, y, theta);</div><div class="line"></div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure></p>
<p>Not better than me! haha~</p>
<h2 id="My-assignments-on-github"><a href="#My-assignments-on-github" class="headerlink" title="My assignments on github"></a>My assignments on github</h2><p><a href="https://github.com/lgrcyanny/MachineLearningCoursera/tree/master/assignments/ex1/ex1" target="_blank" rel="external">Assignments1</a><br>For submition errors, please refer to<a href="https://learner.coursera.help/hc/en-us/community/posts/204693179-linear-regression-submit-error" target="_blank" rel="external">Jacob Middag</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I have been learning the coursera Machine Learning Course by Andrew Ng for two weeks now. Machine Learning is fun and different. For the coursera assignment1 of linear regression, I want to share something.&lt;br&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://www.cyanny.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Programming In Scala Overview Key Note</title>
    <link href="http://www.cyanny.com/2016/03/06/programming-in-scala-overview-key-note/"/>
    <id>http://www.cyanny.com/2016/03/06/programming-in-scala-overview-key-note/</id>
    <published>2016-03-06T10:01:42.000Z</published>
    <updated>2017-03-05T02:27:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>过去半年里，都在忙着spark相关的项目，主要的编程语言是scala，前些年主要用的是c++，刚转到scala上时，有点不适应函数式编程语言的思想，现在已经半年多过去了，觉得scala真的是awesome，简洁有力的表达，整合了functional programming和object programming，并且设计良好。<br>去年上了Martin Odersky的编程课<i>Functional Programming In Scala</i>, 觉得挺感兴趣，学习了很多，想在scala语言上看得更多，更深入些，我选择了Scala的权威著作<a href="http://www.artima.com/shop/programming_in_scala_2ed" target="_blank" rel="external">Programming In Scala</a>, 虽然这书有快900页，但我想挑战下，与其每天泡在各种新闻资讯里，还不如看看书泡在书里。每天都看看书，觉得挺好，有些笔记和心得就写在博客里，毕竟工作忙，写博客的时间很少，但我想还是应该多总结和留下写想法。之后阅读Spark的源码，我也会坚持写下些东西，看过了和写下来总还是不一样嘛。<br><a id="more"></a></p>
<h1 id="Scala-Overview"><a href="#Scala-Overview" class="headerlink" title="Scala Overview"></a>Scala Overview</h1><h2 id="1-Scala-is-a-Scalable-Language"><a href="#1-Scala-is-a-Scalable-Language" class="headerlink" title="1. Scala is a Scalable Language"></a>1. Scala is a Scalable Language</h2><ul>
<li>Scala is a blend of object-oriented and functional programing language</li>
<li>Grow new types, such as BigInt</li>
<li>Grow new control structures, such as actor based api</li>
</ul>
<h2 id="2-What-makes-scala-scalable"><a href="#2-What-makes-scala-scalable" class="headerlink" title="2. What makes scala scalable"></a>2. What makes scala scalable</h2><p>fusion object-oriented and functional programming</p>
<ul>
<li><strong>Object-Oriented</strong><ul>
<li>it combines data with operations under a formalized interface. So objects have a lot to do with language scalability: the same techniques apply to the construction of small as well as large program</li>
<li>Scala is an object-oriented language in pure form: every value is an object and every operation is a method call.</li>
<li>An example is Scala’s traits. Traits are like interfaces in Java, but they can also have method implementations and even fields</li>
</ul>
</li>
<li><strong>Functional</strong><ul>
<li>Functional programming fundation was raid in lonzo Church’s lambda calculus, in the 1930. The first functional programming language is Lisp, created in the late 1950s, other functional programming languages are Scheme, SML, Erlang, Haskell, OCaml, and F#.</li>
<li>Functional programming two main ideas:<ul>
<li>Firstly, <strong>functional are first class values</strong><ul>
<li>You can pass functions as arguments to other functions, return them as results from functions, or store them in variables. You can also define a function inside another function, just as you can define an integer value inside a function</li>
<li>Functions that are first-class values provide a convenient means for abstracting over operations and creating new control structures.</li>
</ul>
</li>
<li>Secondly, <strong>operations of a program should map input values to output values rather than change data in place.</strong><ul>
<li>methods should not have any side effects<ul>
<li><strong>Referentially transparent</strong>, which means that for any given input the method call could be replaced by its result without affecting the program’s semantics</li>
</ul>
</li>
<li>encourage immutable data structures and referentially transparent methods</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="3-Why-chooose-scala"><a href="#3-Why-chooose-scala" class="headerlink" title="3. Why chooose scala?"></a>3. Why chooose scala?</h2><ul>
<li><strong>Compatibility</strong><ul>
<li>compile to JVM bytecode, run on jvm</li>
<li>compatible with java types, reuse java types</li>
<li>implicity conversions : support string.toInt,</li>
<li>java can call scala code</li>
</ul>
</li>
<li><strong>Brevity, scala is concise</strong><ul>
<li>reduction on list</li>
<li>Avoid biolerplate in java, such as avoid class getter and setter, default constructor</li>
<li>Type inference</li>
<li>tools in library, can be used as trait</li>
</ul>
</li>
<li><p><strong>High-level abstractions</strong></p>
<ul>
<li><p>Scala helps you manage complexity by letting you raise the level of abstraction in the interfaces you design and use.<br>For example, The Scala code treats the same strings as higher-level sequences of characters that can be queried with predicates</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> tr = str.exists(_.isUpper)</div></pre></td></tr></table></figure>
</li>
<li><p>Functional literals are lightweight</p>
</li>
</ul>
</li>
<li><strong>Advanced static type system</strong><ul>
<li>it allows you to parameterize types with generics, to combine types using intersections, and to hide details of types using abstract types.</li>
<li>Although some argues that static typed language is verbose and not flexible, In scala  <strong>verbosity</strong> is avoided through type inference and <strong>flexibility</strong> is gained through pattern matching and several new ways to write and compose types.</li>
<li>Advantages of static typing system:<ul>
<li>Verifiable properties: Static type systems can prove the absence of certain run-time errors. Reduce the number of unit tests.</li>
<li>Safe refactoring : make changes to a codebase with a high degree of confidence</li>
<li>Documentation: Static types are program documentation that is checked by the compiler for correctness.<ul>
<li>Scala has a very sophisticated type inference system that lets you omit almost all type information that’s usually considered annoying</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="4-Scala-roots"><a href="#4-Scala-roots" class="headerlink" title="4.Scala roots"></a>4.Scala roots</h2><p>Although only a few features of Scala are genuinely new; most have been already applied in some form in other languages. Its design models many languages, such as SmallTalk, Ruby, Algol, Simula, OCaml, Haskell etc.<br>Scala’s innovations come primarily from how its constructs are put together<br>Scala is also not the first language to integrate functional and object-oriented programming, although it probably goes furthest in this direction</p>
<p>Given Scala’s innovations:</p>
<ul>
<li>its abstract types provide a more object-oriented alternative to generic types,</li>
<li>its traits allow for flexible component assembly,</li>
<li>its extractors provide a representation-independent way to do pattern matching.</li>
</ul>
<h2 id="5-Starting-Programming"><a href="#5-Starting-Programming" class="headerlink" title="5. Starting Programming"></a>5. Starting Programming</h2><p><a href="https://github.com/lgrcyanny/ScalaPractice/tree/master/ProgrammingInScala" target="_blank" rel="external">Programming In Scala</a><br>读书的时候写写代码，都是书里的例子，用maven构建的scala progject。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;过去半年里，都在忙着spark相关的项目，主要的编程语言是scala，前些年主要用的是c++，刚转到scala上时，有点不适应函数式编程语言的思想，现在已经半年多过去了，觉得scala真的是awesome，简洁有力的表达，整合了functional programming和object programming，并且设计良好。&lt;br&gt;去年上了Martin Odersky的编程课&lt;i&gt;Functional Programming In Scala&lt;/i&gt;, 觉得挺感兴趣，学习了很多，想在scala语言上看得更多，更深入些，我选择了Scala的权威著作&lt;a href=&quot;http://www.artima.com/shop/programming_in_scala_2ed&quot;&gt;Programming In Scala&lt;/a&gt;, 虽然这书有快900页，但我想挑战下，与其每天泡在各种新闻资讯里，还不如看看书泡在书里。每天都看看书，觉得挺好，有些笔记和心得就写在博客里，毕竟工作忙，写博客的时间很少，但我想还是应该多总结和留下写想法。之后阅读Spark的源码，我也会坚持写下些东西，看过了和写下来总还是不一样嘛。&lt;br&gt;
    
    </summary>
    
    
      <category term="Scala" scheme="http://www.cyanny.com/tags/scala/"/>
    
      <category term="Language" scheme="http://www.cyanny.com/tags/language/"/>
    
  </entry>
  
  <entry>
    <title>Learning Akka</title>
    <link href="http://www.cyanny.com/2015/10/07/learning-akka/"/>
    <id>http://www.cyanny.com/2015/10/07/learning-akka/</id>
    <published>2015-10-07T13:54:48.000Z</published>
    <updated>2017-03-05T02:29:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近学习了scala，主要是跟着<strong>Functional Programming in Scala</strong>的课程学习的，scala主要的用处还是在spark上，关于spark也看了一些源码，处于继续探索中。</p>
<p>并发时的消息通信是处理spark这样的分布式系统的关键。spark主要依赖scala社区的akka进行消息通讯。在spark1.2的代码中可以看到很多actor的影子，然而spark1.4中，都是封装为EndPoint，但Actor的思想和模式还在。</p>
<p>基于Actor的消息通讯模型是akka的核心。Akka为处理并发、容错和可扩展性的分布式问题提供了一套基于Actor模型的库。并发的消息通讯中，抽象是关键的方面。Actor模型是1973年由Carl Hewitt在论文”Actor Model of Computation- Scalable Robust Information Systems”中提出, 后被应用于爱立信公司研发的Elang语言，爱立信公司应用Actor模型开发了高并发和可靠的通信系统。</p>
<a id="more"></a>
<p>###1. Akka的重要的4个方面<br><em>Actor</em></p>
<ul>
<li>为处理并发，并行问题提供了简单和high-level的抽象</li>
<li>event-driven的异步非阻塞通讯</li>
<li>轻量的actor进程，几百万个actor只占heap 1G左右</li>
</ul>
<p><em>容错性</em></p>
<ul>
<li>具有”let-it-crash”语意的监督者层级结构</li>
<li>不同的监督者层级可以分布在不同的JVM中，提供真正容错的系统</li>
<li>系统可以自我恢复，never stop</li>
</ul>
<p><em>寻址透明</em></p>
<ul>
<li>Actor在Akka内部采用统一的寻址方式，屏蔽底层细节</li>
<li>Actor是纯消息传递，且为异步消息传递</li>
</ul>
<p><em>可持久化</em></p>
<ul>
<li>Actor收到的消息会被存储在邮箱中，收到的消息可以被持久化，actor可以被迁移到其他节点，重新回复并重启。</li>
</ul>
<p>总之，Akka的设计围绕着以下三个核心展开：</p>
<ul>
<li>Scale-up 并发</li>
<li>Scal-out(Remoting)，可扩展性</li>
<li>容错性</li>
</ul>
<p>###2. 永远不变的HelloWorld<br><a href="https://github.com/lgrcyanny/LearningAkka" target="_blank" rel="external">Hello World</a><br><em>Greeter</em><br>Actor必须实现receive方法, 通过hello world例子可以看到actor的入门很简单。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> akka.actor.<span class="type">Actor</span></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">Greeter</span> </span>&#123;</div><div class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">Greet</span></span></div><div class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">Done</span></span></div><div class="line">&#125;</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Greeter</span> <span class="keyword">extends</span> <span class="title">Actor</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receive</span> </span>= &#123;</div><div class="line">    <span class="keyword">case</span> msg<span class="meta">@Greeter</span>.<span class="type">Greet</span> =&gt;</div><div class="line">      println(<span class="string">s"Hello Akka, receive <span class="subst">$msg</span>"</span>)</div><div class="line">      sender() ! <span class="type">Greeter</span>.<span class="type">Done</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><em>HelloWorld Actor</em><br>负责启动Greeter Actor<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HelloWorld</span> <span class="keyword">extends</span> <span class="title">Actor</span> </span>&#123;</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">preStart</span> </span>= &#123;</div><div class="line">    <span class="keyword">val</span> greeter = context.actorOf(<span class="type">Props</span>[<span class="type">Greeter</span>], <span class="string">"greeter"</span>)</div><div class="line">    greeter ! <span class="type">Greeter</span>.<span class="type">Greet</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span> </span>= &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Greeter</span>.<span class="type">Done</span> =&gt;</div><div class="line">      context.stop(self)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><em>Main启动</em></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">Main</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</div><div class="line">    akka.<span class="type">Main</span>.main(<span class="type">Array</span>(classOf[<span class="type">HelloWorld</span>].getName))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Akka的主要学习还是看官方的文档，akka的特定用法看api很不错。<br>以上就是akka的入门的一些学习吧，写的仓促，如有不对的地方还请指正。最近很忙，也没能好好写博客。<br>akka的深入篇，会持续更新。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近学习了scala，主要是跟着&lt;strong&gt;Functional Programming in Scala&lt;/strong&gt;的课程学习的，scala主要的用处还是在spark上，关于spark也看了一些源码，处于继续探索中。&lt;/p&gt;
&lt;p&gt;并发时的消息通信是处理spark这样的分布式系统的关键。spark主要依赖scala社区的akka进行消息通讯。在spark1.2的代码中可以看到很多actor的影子，然而spark1.4中，都是封装为EndPoint，但Actor的思想和模式还在。&lt;/p&gt;
&lt;p&gt;基于Actor的消息通讯模型是akka的核心。Akka为处理并发、容错和可扩展性的分布式问题提供了一套基于Actor模型的库。并发的消息通讯中，抽象是关键的方面。Actor模型是1973年由Carl Hewitt在论文”Actor Model of Computation- Scalable Robust Information Systems”中提出, 后被应用于爱立信公司研发的Elang语言，爱立信公司应用Actor模型开发了高并发和可靠的通信系统。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Akka" scheme="http://www.cyanny.com/tags/akka/"/>
    
      <category term="Scala" scheme="http://www.cyanny.com/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>位排序和快排</title>
    <link href="http://www.cyanny.com/2015/05/04/bitsort-ant-qsort/"/>
    <id>http://www.cyanny.com/2015/05/04/bitsort-ant-qsort/</id>
    <published>2015-05-04T13:57:53.000Z</published>
    <updated>2017-03-05T02:30:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>这已经不是一个新话题了，但是从开始实现一个C++位排序和快排，我还是花费了2个多小时，这里就记下自己的一点点体会啦。</p>
<p>首先，题目来自《编程珠玑》第一章，主要是做位排序，同时和快排做比较：</p>
<p>1. 实现位逻辑运算，实现位向量，并用该位向量实现1,000,000个数字的排序，数字最大是10,000,000</p>
<p>2. 实现1000,000个数字的快排序</p>
<p>3. 实现生成小于n且没有重复的k个整数，这里n=10,000,000, k = 1,000,000</p>
<a id="more"></a>
<h3 id="1-位排序"><a href="#1-位排序" class="headerlink" title="1. 位排序"></a>1. 位排序</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> &amp;lt;iostream&amp;gt;</span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> &amp;lt;string&amp;gt;</span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> &amp;lt;fstream&amp;gt;</span></div><div class="line"></div><div class="line"><span class="comment">// 这里以32位的int来实现位向量，初始化的开销可能会比较大</span></div><div class="line"><span class="keyword">class</span> BitVector &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    BitVector(<span class="keyword">int</span> size) &#123;</div><div class="line">        <span class="keyword">int</span> n = (<span class="number">1</span> + (size / BITS_PER_WORD));</div><div class="line">        buffer = <span class="keyword">new</span> <span class="keyword">int</span>[n];</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &amp;lt; n; ++i) &#123;</div><div class="line">            buffer[i] = <span class="number">0</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    ~BitVector() &#123;</div><div class="line">        <span class="keyword">delete</span>[] buffer;</div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</div><div class="line">        buffer[(i &amp;gt;&amp;gt; SHIFT)] |= <span class="number">1</span> &amp;lt;&amp;lt; (i &amp;amp; MASK);</div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">clr</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</div><div class="line">        buffer[(i &amp;gt;&amp;gt; SHIFT)] &amp;amp;= ~(<span class="number">1</span> &amp;lt;&amp;lt; (i &amp;amp; MASK));</div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">test</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> buffer[(i &amp;gt;&amp;gt; SHIFT)] &amp;amp; (<span class="number">1</span> &amp;lt;&amp;lt; (i &amp;amp; MASK));</div><div class="line">    &#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line">    <span class="keyword">int</span>* buffer;</div><div class="line">    <span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> BITS_PER_WORD = <span class="number">32</span>;</div><div class="line">    <span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> MASK = <span class="number">0x1f</span>;</div><div class="line">    <span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> SHIFT = <span class="number">5</span>;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> BitVector::BITS_PER_WORD;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> BitVector::MASK;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> BitVector::SHIFT;</div><div class="line"></div><div class="line"><span class="keyword">class</span> Solution &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="comment">// 输入是要排序的文件名，同时需要给出输出文件名称</span></div><div class="line">    <span class="comment">// range是数字的范围</span></div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">sort</span><span class="params">(</span></span></div><div class="line">            <span class="keyword">int</span> range,</div><div class="line">            <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp;amp; input_file_name,</div><div class="line">            <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp;amp; output_file_name) &#123;</div><div class="line">        <span class="function">BitVector <span class="title">bits_vector</span><span class="params">(range)</span></span>;</div><div class="line">        <span class="built_in">std</span>::<span class="function">ifstream <span class="title">fin</span><span class="params">(input_file_name)</span></span>;</div><div class="line">        <span class="keyword">int</span> i = <span class="number">0</span>;</div><div class="line">        <span class="keyword">while</span> (fin &amp;gt;&amp;gt; i) &#123;</div><div class="line">            bits_vector.<span class="built_in">set</span>(i);</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">std</span>::<span class="function">ofstream <span class="title">fout</span><span class="params">(output_file_name)</span></span>;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &amp;lt; range; ++i) &#123;</div><div class="line">            <span class="keyword">if</span> (bits_vector.test(i)) &#123;</div><div class="line">                fout &amp;lt;&amp;lt; i &amp;lt;&amp;lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> <span class="keyword">const</span> *argv[])</span> </span>&#123;</div><div class="line">    Solution s;</div><div class="line">    <span class="keyword">int</span> range = <span class="number">10000000</span>;</div><div class="line">    s.sort(range, &amp;quot;numbers_input.txt&amp;quot;, &amp;quot;numbers_output.txt&amp;quot;);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="2-Quick-Sort"><a href="#2-Quick-Sort" class="headerlink" title="2. Quick Sort"></a>2. Quick Sort</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> Solution &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">sort</span><span class="params">(</span></span></div><div class="line">            <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp;amp; input_file_name,</div><div class="line">            <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp;amp; output_file_name) &#123;</div><div class="line">        <span class="built_in">std</span>::<span class="built_in">vector</span>&amp;lt;<span class="keyword">int</span>&amp;gt; arr;</div><div class="line">        <span class="keyword">int</span> number = <span class="number">0</span>;</div><div class="line">        <span class="built_in">std</span>::<span class="function">ifstream <span class="title">fin</span><span class="params">(input_file_name)</span></span>;</div><div class="line">        <span class="keyword">while</span>(fin &amp;gt;&amp;gt; number) &#123;</div><div class="line">            arr.push_back(number);</div><div class="line">        &#125;</div><div class="line">        qsort(&amp;amp;arr, <span class="number">0</span>, arr.size() - <span class="number">1</span>);</div><div class="line">        <span class="built_in">std</span>::<span class="function">ofstream <span class="title">fout</span><span class="params">(output_file_name)</span></span>;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &amp;lt; arr.size(); ++i) &#123;</div><div class="line">            fout &amp;lt;&amp;lt; arr[i] &amp;lt;&amp;lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">qsort</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&amp;lt;<span class="keyword">int</span>&amp;gt;* arr, <span class="keyword">int</span> p, <span class="keyword">int</span> r)</span> </span>&#123;</div><div class="line">        <span class="keyword">if</span> (p &amp;gt;= r) &#123;</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">int</span> q = partition(arr, p, r);</div><div class="line">        qsort(arr, p, q - <span class="number">1</span>);</div><div class="line">        qsort(arr, q + <span class="number">1</span>, r);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&amp;lt;<span class="keyword">int</span>&amp;gt;* arr, <span class="keyword">int</span> p, <span class="keyword">int</span> r)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> i = p;</div><div class="line">        <span class="keyword">int</span> j = i - <span class="number">1</span>;</div><div class="line">        <span class="keyword">for</span> (; i &amp;lt; r; ++i) &#123;</div><div class="line">            <span class="keyword">if</span> (arr-&amp;gt;at(i) &amp;lt; arr-&amp;gt;at(r)) &#123;</div><div class="line">                ++j;</div><div class="line">                <span class="built_in">std</span>::swap(arr-&amp;gt;at(i) , arr-&amp;gt;at(j));</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        ++j;</div><div class="line">        <span class="built_in">std</span>::swap(arr-&amp;gt;at(j) , arr-&amp;gt;at(r));</div><div class="line">        <span class="keyword">return</span> j;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<h3 id="3-生成范围是-0-n-的k个随机数"><a href="#3-生成范围是-0-n-的k个随机数" class="headerlink" title="3. 生成范围是[0,n)的k个随机数"></a>3. 生成范围是[0,n)的k个随机数</h3><p>该算法首先生成顺序的n个数字，再遍历该数组，将当前索引i和 random(i, n)生成的索引对应的数字交换，保证生成不重复的随机序列，生成1000000个范围为10000000的数字5.3s可以完成</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">swap</span><span class="params">(list, i, j)</span>:</span></div><div class="line">    tmp = list[i]</div><div class="line">    list[i] = list[j]</div><div class="line">    list[j] = tmp</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">()</span>:</span></div><div class="line">    number_limit = int(sys.argv[<span class="number">1</span>])</div><div class="line">    n = int(sys.argv[<span class="number">2</span>])</div><div class="line">    list = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(number_limit):</div><div class="line">        list.append(i)</div><div class="line">    f = open(<span class="string">'numbers_input.txt'</span>, <span class="string">'w'</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">        random_index = random.randint(i, number_limit - <span class="number">1</span>)</div><div class="line">        swap(list, i, random_index)</div><div class="line">        f.write(str(list[i]) + <span class="string">'\n'</span>)</div><div class="line">    f.close()</div><div class="line">generate()</div></pre></td></tr></table></figure>
<h3 id="4-性能比较"><a href="#4-性能比较" class="headerlink" title="4.性能比较"></a>4.性能比较</h3><p>运行的环境是Mac OSX 10.10，内存DDR3 4G，1.8GHz Inter Core i5<br>1. 位排序总运行时间是: 3.98s, 去掉读写文件的时间，0.47s完成排序<br>2. 快排序总运行时间: 4.71s, 去掉读写文件时间，0.63s完成排序<br>总之，快排的优势在于不限制排序数字范围，位排序限定范围，性能比快排高效。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这已经不是一个新话题了，但是从开始实现一个C++位排序和快排，我还是花费了2个多小时，这里就记下自己的一点点体会啦。&lt;/p&gt;
&lt;p&gt;首先，题目来自《编程珠玑》第一章，主要是做位排序，同时和快排做比较：&lt;/p&gt;
&lt;p&gt;1. 实现位逻辑运算，实现位向量，并用该位向量实现1,000,000个数字的排序，数字最大是10,000,000&lt;/p&gt;
&lt;p&gt;2. 实现1000,000个数字的快排序&lt;/p&gt;
&lt;p&gt;3. 实现生成小于n且没有重复的k个整数，这里n=10,000,000, k = 1,000,000&lt;/p&gt;
    
    </summary>
    
    
      <category term="Algorithm" scheme="http://www.cyanny.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>查找两个排序数组的中位数</title>
    <link href="http://www.cyanny.com/2015/04/03/find-median-for-two-sorted-array/"/>
    <id>http://www.cyanny.com/2015/04/03/find-median-for-two-sorted-array/</id>
    <published>2015-04-03T14:07:01.000Z</published>
    <updated>2016-09-27T01:03:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>题目是：求两个排序数组的中位数。<br>设：两个排序的数组a[m], b[n]，求a和b数组的中位数。<br>算法是：<br>mid_a是数组a的中位数index，同理mid_b是数组b的中位数索引<br>1. 如果a[mid_a] == b[mid_b] 中位数为a[mid_a]</p>
<p>2. 如果a[mid_a] &lt; b[mid_b], 递归查找a(mid_a + 1, m - 1), b(0, mid_b)，因为a[mid_a]比较小，不可能作为下一次查询的中位数</p>
<p>3. 如果a[mid_a] &gt; b[mid_b], 递归查找a(0, mid_a), b(mid_b + 1, n - 1)，因为b[mid_b]比较小，不可能作为下一次查询的中位数</p>
<p>4. 当只少于4个元素需要查找时递归停止，merge这少于4的元素，求出中位数。这里需要考虑奇偶数的情况，只剩下2个或3个元素，不如只考虑4个简单。</p>
<p>注意：这里求上中位数，当n为奇数时，中位数是唯一的，出现位置为n/2；当n为偶数时候，存在两个中位数，数组index从0开始，位置分别为n/2 - 1（上中位数）和n/2（下中位数）。</p>
<a id="more"></a>
<p>[cpp]</p>
<p>#include &lt;iostream&gt;<br>using namespace std;</p>
<p>class Solution {<br>public:<br>  int find_median(int a[], int p, int q, int b[], int r, int s) {<br>    int size_a = q - p + 1;<br>    int size_b = s - r + 1;<br>    int total_size = size_a + size_b;<br>    if (total_size &lt;= 4) {<br>      // calcuate median for less than 4 elements<br>      int i = p;<br>      int j = r;<br>      int num = (total_size + 1) / 2;<br>      for (int k = 1; k &lt; num; k++) {<br>        // A fake merge without copying, just move index<br>        if (i &lt;= q &amp;&amp; j &lt;= s) {<br>          if (a[i] &lt;= b[j]) {<br>            ++i;<br>          } else {<br>            ++j;<br>          }<br>        } else if (i &gt; q) {<br>          ++j;<br>        } else if (j &gt; s) {<br>          ++i;<br>        }<br>      }<br>      int median = 0;<br>      if (i &lt;= q &amp;&amp; j &lt;= s) {<br>        median = a[i] &lt; b[j] ? a[i] : b[j];<br>      } else if (i &gt; q){<br>        median = b[j];<br>      } else if (j &gt; s) {<br>        median = a[i];<br>      }<br>      return median;<br>    }<br>    int mid_a = get_median_index(a, p, q);<br>    int mid_b = get_median_index(b, r, s);<br>    if (a[mid_a] == b[mid_b]) {<br>      return a[mid_a];<br>    } else if (a[mid_a] &lt; b[mid_b]) {<br>      // mid_a is less, no need to be seached later, no possible to be median<br>      return find_median(a, mid_a + 1, q, b, r, mid_b);<br>    } else {<br>      // mid_b is less, no need to be seached later, no possible to be median<br>      return find_median(a, p, mid_a, b, mid_b + 1, s);<br>    }<br>  }</p>
<p>private:<br>  int get_median_index(int a[], int p, int q) {<br>    int n = q - p + 1;<br>    int mid = n / 2;<br>    if (n % 2 == 0) {<br>      return p + mid - 1;<br>    } else {<br>      return p + mid;<br>    }<br>  }<br>};</p>
<p>int main(int argc, char const *argv[])<br>{<br>  int a[] = {5, 6, 7, 8};<br>  int m = sizeof(a) / sizeof(int);<br>  int b[] = {1, 2, 3, 4};<br>  int n = sizeof(b) / sizeof(int);<br>  Solution s;<br>  int median = s.find_median(a, 0, m - 1, b, 0, n - 1);<br>  cout &lt;&lt; median &lt;&lt; endl;<br>  return 0;<br>}<br>[/cpp]</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;题目是：求两个排序数组的中位数。&lt;br&gt;设：两个排序的数组a[m], b[n]，求a和b数组的中位数。&lt;br&gt;算法是：&lt;br&gt;mid_a是数组a的中位数index，同理mid_b是数组b的中位数索引&lt;br&gt;1. 如果a[mid_a] == b[mid_b] 中位数为a[mid_a]&lt;/p&gt;
&lt;p&gt;2. 如果a[mid_a] &amp;lt; b[mid_b], 递归查找a(mid_a + 1, m - 1), b(0, mid_b)，因为a[mid_a]比较小，不可能作为下一次查询的中位数&lt;/p&gt;
&lt;p&gt;3. 如果a[mid_a] &amp;gt; b[mid_b], 递归查找a(0, mid_a), b(mid_b + 1, n - 1)，因为b[mid_b]比较小，不可能作为下一次查询的中位数&lt;/p&gt;
&lt;p&gt;4. 当只少于4个元素需要查找时递归停止，merge这少于4的元素，求出中位数。这里需要考虑奇偶数的情况，只剩下2个或3个元素，不如只考虑4个简单。&lt;/p&gt;
&lt;p&gt;注意：这里求上中位数，当n为奇数时，中位数是唯一的，出现位置为n/2；当n为偶数时候，存在两个中位数，数组index从0开始，位置分别为n/2 - 1（上中位数）和n/2（下中位数）。&lt;/p&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="http://www.cyanny.com/categories/algorithm/"/>
    
    
      <category term="algorithm" scheme="http://www.cyanny.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Hive架构笔记</title>
    <link href="http://www.cyanny.com/2014/08/16/hive-architecture/"/>
    <id>http://www.cyanny.com/2014/08/16/hive-architecture/</id>
    <published>2014-08-16T15:32:27.000Z</published>
    <updated>2016-09-27T01:03:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hive是基于Hadoop的大数据查询引擎, 存储于HDFS上的数据,要求开发者采用MR才能进行计算, 但MR是low-level的分布式计算,对于复杂的分析job, 多重MR是很复杂的, Hive致力于让用户采用传统的RDBMS的SQL查询处理分布式的大数据.<br><a id="more"></a></p>
<h3 id="1-架构"><a href="#1-架构" class="headerlink" title="1. 架构"></a>1. 架构</h3><p><a href="http://tinypic.com?ref=2qs2w6e" target="_blank" rel="external"><img src="http://i60.tinypic.com/2qs2w6e.jpg" alt="Image and video hosting by TinyPic"></a></p>
<p>(1) external interface: 提供用户接口，WebUI, CLI, JDBC, ODBC<br>(2) Thrift Server：为JDBC, ODBC 提供服务<br>(3) MetaStore: 存储元数据信息<br>(4) Driver：控制HQL的生命周期， 包括</p>
<ul>
<li>Compiler: 将HQL编译为Operator DAG的执行计划</li>
<li>Optimizer: 对Operator DAG进行基于规则的优化</li>
<li>Executor: 执行Optimizer最终生成的DAG MR<br>(5) Hadoop：MR， HDFS： 执行physical plan（DAG）</li>
</ul>
<h3 id="2-Hive与传统数据库的区别"><a href="#2-Hive与传统数据库的区别" class="headerlink" title="2. Hive与传统数据库的区别"></a>2. Hive与传统数据库的区别</h3><p>虽然Hive提供的HQL和RDBMS很类似,并且也致力于向标准SQL靠近, 但还是有一些区别:</p>
<p>(1) Hive Data Model and Typed System</p>
<ul>
<li>Hive支持传统数据类型,如int , float, double, string, 也支持struct, array, map</li>
<li>Hive 提供对struct, array, map的field访问的语法,例如”a[1]”, “a.b.c”</li>
<li><p>Hive提供SerDe Interface, 可以支持用户自定义的文件格式的读写<br>(2) HQL</p>
</li>
<li><p>Hive的子查询只能在from子句中, from可以在select前面</p>
</li>
<li>Hive join只支持等值join</li>
<li>Hive支持multi insert</li>
<li>HQL中可以自定义Map和Reduce脚本</li>
<li>Hive引入了Partition的概念, 例如数据总是按day, hour产生时,可以分配event_day, event_hour的partition, 每一个查询针对每一个partition操作,就不会加载所有的数据,加快查询,提高性能</li>
</ul>
<h3 id="3-Hive-MetaStore-and-Data-Model"><a href="#3-Hive-MetaStore-and-Data-Model" class="headerlink" title="3. Hive MetaStore and Data Model"></a>3. Hive MetaStore and Data Model</h3><p>Hive MetaStore提供元数据信息的抽象<br>(1) DataBase<br>类似传统的数据库概念，是表的namespace，默认的table放入’default’ database</p>
<p>(2) Table<br>数据表<br>重要信息包括：schema, location, SerDe, input_format, out_format等</p>
<p>(3) Partition<br>每个表的数据可以有分区, 每个partition key需要在create table时指定,例如: event_day, event_hour等<br>每个Partition可以有自己的schema和location.</p>
<p>(4) Bucket<br>Table和Partition的Storage Descriptor可以按指定的column bucket(分桶), 默认以hash分桶, 分桶可以优化map join, table sample操作.<br>Bucket是Table或Partition所在directory下的file</p>
<p>Hive Metastore信息可以存储在本地Derby数据库,或MySQL, Hive采用DataNucleus进行ORM</p>
<h3 id="4-Driver"><a href="#4-Driver" class="headerlink" title="4. Driver"></a>4. Driver</h3><p>目标：将HQL解析为DAG DAG的物理执行计划<br>步骤：<br>(1) Compiler的Parser将HQL解析为AST(Abstract Syntax Tree), Hive中采用Antlr进行词法解析和语法解析</p>
<p>(2)Semantic Analyzer, 语义分析,检查Table是否存在, 类型, 表达式等是否合法<br>语义分析将AST解析为中间表示(Intermediate Representation), 即Query Block(QB) tree,<br>QB tree 是一个tree的结构, 目的是方便将AST转化为Operator DAG.</p>
<p>(3) Logical Plan Generator将QB tree转化为logical plan, 即Operator DAG</p>
<p>(4) Optimizer优化Operator DAG, 进行基于规则的算子变换(transformation), 每一个变换会实现5个接口, Node, GraphWalker, Dispatcher, Rule 和Processor, Opritimizer提供的优化包括:</p>
<ul>
<li>列裁剪</li>
<li>partition裁剪</li>
<li>谓词下推, 更早地filter数据, 从源头减少数据量</li>
<li>合并多个共享join key的join算子</li>
<li>Join Reordering: 在reduce时, 让small table常驻内存</li>
<li><p>基于用户的Hint的优化, 包括:</p>
<ul>
<li>Repartitioning of data to handle skews in group by processing: 对于有倾斜性的数据, reducer分配到的数据不均匀,影响性能, 做聚集时, 例如sum, 可以将group by拆分为两个map reduce阶段, 第一个阶段hash randomly分配数据,并部分聚集, 减少第二个阶段的聚集量.</li>
<li>map side join(将小的表存储在内存中)</li>
<li>Hash based partial aggregations in the mappers, 将需要聚集的行存在内存hash table中</li>
<li>Physical Plan Generator 将Operator DAG转化为Physical Plan, 包括多个阶段的MR和HDFS tasks</li>
</ul>
</li>
<li><p>Execution Engine 按照物理计划的依赖关系执行task</p>
</li>
</ul>
<h3 id="5-相关的"><a href="#5-相关的" class="headerlink" title="5. 相关的"></a>5. 相关的</h3><ul>
<li>Hive基于成本的优化</li>
<li>Scope, 微软的SQL Like language</li>
<li>Pig, allows users to write declarative scripts to process data</li>
<li>Dremel, Google’s Query Engine</li>
<li>Apache Drill</li>
<li>Spark</li>
<li>MR</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive是基于Hadoop的大数据查询引擎, 存储于HDFS上的数据,要求开发者采用MR才能进行计算, 但MR是low-level的分布式计算,对于复杂的分析job, 多重MR是很复杂的, Hive致力于让用户采用传统的RDBMS的SQL查询处理分布式的大数据.&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.cyanny.com/categories/hadoop/"/>
    
      <category term="Hive" scheme="http://www.cyanny.com/categories/hadoop/hive/"/>
    
    
      <category term="Hadoop" scheme="http://www.cyanny.com/tags/hadoop/"/>
    
      <category term="Hive" scheme="http://www.cyanny.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Dijkstra Algorithm</title>
    <link href="http://www.cyanny.com/2014/04/12/dijkstra-algorithm-in-java/"/>
    <id>http://www.cyanny.com/2014/04/12/dijkstra-algorithm-in-java/</id>
    <published>2014-04-12T03:55:04.000Z</published>
    <updated>2017-03-05T02:31:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>计算单源最短路径的算法有Bellman-Ford算法，主要是计算带负权有向图的单源最短路径。第二种算法：对于DAG图（Directed Acyclic Graph, 有向无环图）可以通过拓扑排序后再计算单源最短路径。第三，可以使用BFS(广度优先搜索)计算单源最短路径。第四，就是著名的Dijkstra算法，要求有向图的所有边的权值非负，当然图可以不是DAG图。</p>
<a id="more"></a>
<p>Dijkstra的实现思想主要是采用最小优先级队列，队列排序的标准是source节点到当前节点的距离，每次从堆顶取出一个节点，遍历该节点的邻接表，进行松弛(Relax)操作，松弛操作的伪代码如下：<br>[java]<br>relax(u, v) {<br>  if (v.distance &gt; u.distance + weight[u, v]) {<br>    v.distance = u.distance + weight[u, v];<br>    v.parent = u;<br>  }<br>}<br>[/java]</p>
<p>Dijkstra算法的伪代码如下：<br>[java]<br>dijkstraShortestPath(graph, s) {<br>  for each v in graph {<br>    v.parent = null;<br>    v.distance = MaxValue;<br>  }<br>  MinPriorityQueue queue;<br>  s.distance = 0;<br>  queue.add(s);<br>  while (queue != empty) {<br>    u = queue.extractMin();<br>    for each v in adjacentlist of u {<br>      if (v.distance &gt; u.distance + weight[u, v]) {<br>        v.distance = u.distance + weight[u, v];<br>        v.parent = u;<br>        queue.add(v);<br>      }<br>    }<br>  }<br>}<br>[/java]</p>
<p>Dijkstra算法实现时的一个难点是如何实现最小优先级队列以及图如何表示，我在实现中采用Java提供的PriorityQueue，同时Graph的表示采用邻接表。而邻接表最初的想法是一个ArrayList<arraylist<edge>&gt;, 每个Edge对象 = Vertex + weight; 但这个实现的限制是，在ArrayList中每个list的index就十分重要，当从queue中取得一个节点，需要按index找到他的邻接list。在<a href="https://github.com/mburst/dijkstras-algorithm/blob/master/Dijkstras.java" target="_blank" rel="external">GitHub上</a>看到一个实现，是用LinkedHashMap表示邻接表，但凭什么给每个节点按string的key进行标识呢？不是很认同。<br>最后采取的办法是，将每个节点的邻接list放入每个节点对象中，主要是受<a href="http://www.algolist.com/code/java/Dijkstra%27s_algorithm" target="_blank" rel="external">这个博客</a>的启发。<br>最后实现代码如下，也不是很长。</arraylist<edge></p>
<h3 id="1-定义Vertex"><a href="#1-定义Vertex" class="headerlink" title="1. 定义Vertex"></a>1. 定义Vertex</h3><p>[java]<br>public class Vertex implements Comparable&lt;Vertex&gt;{<br>    public String id = null;<br>    public Vertex parent = null;<br>    public Edge[] adjacencyList = null;<br>    public int distance = Integer.MAX_VALUE; // The min distance from source to the current vertex</p>
<pre><code>public Vertex() {

}

public Vertex(String id) {
    this.id = id;
}

/**
 * Override this method for min priority queue
 */
@Override
public int compareTo(Vertex o) {
    return this.distance &amp;lt; o.distance ? -1 : (this.distance == o.distance ? 0 : 1);
}

@Override
public String toString() {
    return &amp;quot;Vertex [id=&amp;quot; + id + &amp;quot;, distance=&amp;quot; + distance + &amp;quot;]&amp;quot;;
}
</code></pre><p>}<br>[/java]</p>
<h3 id="2-定义Edge对象"><a href="#2-定义Edge对象" class="headerlink" title="2. 定义Edge对象"></a>2. 定义Edge对象</h3><p>[java]<br>public class Edge {<br>    public Vertex vertex = null;<br>    public int weight = 0;</p>
<pre><code>public Edge() {

}

public Edge(Vertex vertex, int weight) {
    this.vertex = vertex;
    this.weight = weight;
}
</code></pre><p>}<br>[/java]</p>
<h3 id="3-Dijkstra算法实现"><a href="#3-Dijkstra算法实现" class="headerlink" title="3. Dijkstra算法实现"></a>3. Dijkstra算法实现</h3><p>测试的图如下：</p>
<p><a href="http://tinypic.com?ref=9ggih1.jpg" target="_blank" rel="external"><img src="http://i59.tinypic.com/9ggih1.jpg" alt="Image and video hosting by TinyPic"></a><br>[java]<br>public class DijkstraGraph {<br>    public Vertex[] graph = null;</p>
<pre><code>public DijkstraGraph(Vertex[] graph) {
    this.graph = graph;
}

public void computShortestPath(Vertex source) {
    PriorityQueue&amp;lt;Vertex&amp;gt; queue = new PriorityQueue&amp;lt;Vertex&amp;gt;();
    source.distance = 0;
    queue.add(source);
    while (!queue.isEmpty()) {
        Vertex u = queue.remove();
        for (int i = 0; i &amp;lt; u.adjacencyList.length; i++) {
            Edge v = u.adjacencyList[i]; // The neighbor of vertex u
            if (v.vertex.distance &amp;gt; u.distance + v.weight) {
                v.vertex.distance = u.distance + v.weight;
                v.vertex.parent = u;
                queue.add(v.vertex);
            }
        }
    }
}

public void printShortestPath() {
    for (int i = 0; i &amp;lt; graph.length; i++) {
        Vertex target = graph[i];
        Stack&amp;lt;Vertex&amp;gt; path = new Stack&amp;lt;Vertex&amp;gt;();
        while (target != null) {
            path.push(target);
            target = target.parent;
        }
        printPath(path);
    }
}

private void printPath(Stack&amp;lt;Vertex&amp;gt; path) {
    System.out.println(&amp;quot;=============The path is=========&amp;quot;);
    while (!path.isEmpty()) {
        System.out.println(path.pop());
    }
}

public static void main(String[] args) {
    Vertex v0 = new Vertex(&amp;quot;s&amp;quot;);
    Vertex v1 = new Vertex(&amp;quot;t&amp;quot;);
    Vertex v2 = new Vertex(&amp;quot;x&amp;quot;);
    Vertex v3 = new Vertex(&amp;quot;z&amp;quot;);
    Vertex v4 = new Vertex(&amp;quot;y&amp;quot;);

    v0.adjacencyList = new Edge[] { new Edge(v1, 10), new Edge(v4, 5) };
    v1.adjacencyList = new Edge[] { new Edge(v2, 1), new Edge(v4, 2) };
    v2.adjacencyList = new Edge[] { new Edge(v3, 4) };
    v3.adjacencyList = new Edge[] { new Edge(v0, 7), new Edge(v2, 6) };
    v4.adjacencyList = new Edge[] { new Edge(v1, 3), new Edge(v2, 9),
            new Edge(v3, 2) };

    Vertex[] graph = { v0, v1, v2, v3 };

    DijkstraGraph dijkstraGraph = new DijkstraGraph(graph);
    dijkstraGraph.computShortestPath(v0);
    dijkstraGraph.printShortestPath(); // Each path from v0

}
</code></pre><p>}<br>[/java]</p>
<p>源码在<a href="https://github.com/lgrcyanny/Algorithm/tree/master/src/com/algorithm/graph/dijkstra" target="_blank" rel="external">Cyanny GitHub</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;计算单源最短路径的算法有Bellman-Ford算法，主要是计算带负权有向图的单源最短路径。第二种算法：对于DAG图（Directed Acyclic Graph, 有向无环图）可以通过拓扑排序后再计算单源最短路径。第三，可以使用BFS(广度优先搜索)计算单源最短路径。第四，就是著名的Dijkstra算法，要求有向图的所有边的权值非负，当然图可以不是DAG图。&lt;/p&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="http://www.cyanny.com/categories/algorithm/"/>
    
    
      <category term="algorithm" scheme="http://www.cyanny.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>HBase MapReduce排序Secondary Sort</title>
    <link href="http://www.cyanny.com/2014/03/20/hbase-mapreduce-e6-8e-92-e5-ba-8f-secondary-sort/"/>
    <id>http://www.cyanny.com/2014/03/20/hbase-mapreduce-e6-8e-92-e5-ba-8f-secondary-sort/</id>
    <published>2014-03-20T10:51:08.000Z</published>
    <updated>2016-09-27T01:03:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce是Hadoop中处理大数据的方法，是一个处理大数据的简单算法、编程泛型。虽然思想简单，但其实真正用起来还是有很多问题，不是所有的问题都可以像WordCount那样典型和直观, 有很多需要trick的地方。MapReduce的中心思想是分而治之，数据要松耦合，可以划分为小数据集并行处理，如果数据本身在计算上存在很强的依赖关系，就不要赶鸭子上架，用MapReduce了。<br>MapReduce编程中，最重要的是要抓住Map和Reduce的input和output，好的input和output可以降低实现的复杂度。最近，写了很多关于MapReduce的job，有倒排索引，统计，排序等。其中，对排序花费了一番功夫，MapReduce做WordCount很好理解,<br>Map input:[offset, text],  output: [word, 1],</p>
<p>Reduce input: [word, 1], output: [word, totalcount],还可以设置Combiner进行优化。</p>
<a id="more"></a>
<p>但排序不同了，大量的文件记录，分配给map，然后reduce出来的文件分布在各个机器上，怎么保证有序呢？排序是算法中常考常用的，MapReduce做排序还需要理解一下MapReduce过程中，非常magic的过程Shuffle and Sort.[more…]</p>
<h3 id="Shuffle-and-Sort过程解析"><a href="#Shuffle-and-Sort过程解析" class="headerlink" title="Shuffle and Sort过程解析"></a>Shuffle and Sort过程解析</h3><p><a href="http://tinypic.com?ref=1416uzt" target="_blank" rel="external"><img src="http://i59.tinypic.com/1416uzt.jpg" alt="Image and video hosting by TinyPic"></a><br>如上图，Shuffle的过程包括了Map端和Reduce端。</p>
<h4 id="Map端"><a href="#Map端" class="headerlink" title="Map端"></a>Map端</h4><p><a href="http://tinypic.com?ref=j0ftw0" target="_blank" rel="external"><img src="http://i59.tinypic.com/j0ftw0.jpg" alt="Image and video hosting by TinyPic"></a></p>
<ol>
<li>Input Split分配给Map</li>
<li>Map进行计算，输出[key, value]形式的output</li>
<li>Map的输出结果缓存在内存里</li>
<li>内存中进行Partition，默认是HashPartitioner(采用取模hash (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks)， 目的是将map的结果分给不同的reducer，有几个Partition，就有几个reducer，partition的数目可以在job启动时通过参数 “-Dmapreduc.job.reduces”设置(Hadoop 2.2.0), HashPartitioner可以让map的结果均匀的分给不同机器上的reducer，保证负载均衡。</li>
<li>内存中在Partition结束后，会对每个Partition中的结果按照key进行排序。</li>
<li>排序结束后，相同的key在一起了，如果设置过combiner，就合并数据，减少写入磁盘的记录数</li>
<li>当内存中buffer(default 100M)达到阈值(default 80%)，就会把记录spill(即溢写)到磁盘中，优化Map时可以调大buffer的阈值，缓存更多的数据。</li>
<li>当磁盘中的spill文件数目在3(min.num.spills.for.combine)个（包括）以上, map的新的output会再次运行combiner，而如果磁盘中spill file文件就1~2个，就没有必要调用combiner，因为combiner大多数情况和reducer是一样的逻辑，可以在reduer端再计算。</li>
<li>Map结束时会把spill出来的多个文件合并成一个，merge过程最多10（默认）个文件同时merge成一个文件，多余的文件分多次merge，merge过程是merge sort的算法。</li>
<li>Map端shuffle完毕，数据都有序的存放在磁盘里，等待reducer来拿取</li>
</ol>
<h4 id="Reducer端"><a href="#Reducer端" class="headerlink" title="Reducer端"></a>Reducer端</h4><p>shuffle and sort的过程不仅仅在map端，别忘了reducer端还没拿数据呢，reduce job当然不能开启。</p>
<ol>
<li>Copy phase: reducer的后台进程(default 5个)到被Application Master (Hadoop 2.2), 或者之前的JobTracker 指定的机器上将map的output拷贝到本地，先拷贝到内存，内存满了就拷贝的磁盘。</li>
<li>Sort phase(Merge phase): Reduer采用merge sort，将来自各个map的data进行merge， merge成有序的更大的文件。</li>
<li>如果设置过Combiner，merge过程可能会调用Combiner，调不调用要看在磁盘中产生的文件数目是否超过了设定的阈值。(这一点我还没有确认，但Combiner在Reducer端是可能调用。)</li>
<li>Reduce phase: reduce job开始，输入是shuffle sort过程merge产生的文件。</li>
</ol>
<h3 id="MapReduce排序-Secondary-Sort-案例"><a href="#MapReduce排序-Secondary-Sort-案例" class="headerlink" title="MapReduce排序(Secondary Sort)案例"></a>MapReduce排序(Secondary Sort)案例</h3><p>理解了Shuffle的过程后，我们可以着手开始做排序了。<br>1. 需求<br>现在我在HBase里面，存储了10万条文献的评论统计记录，每个文献的评论数目是2~20，每个文献评论统计记录在HBase的存储示例如下：<br>即表 “pb_stat_comments_count”的记录<br>[rowkey, column family, column qualifer, value] =<br>[literature row key, ‘info’, ‘count’, 12]<br>[literature row key, ‘info’, ‘avgts’, 1400815843854]<br>count:是每个文献的记录<br>avgts：是所有文献的平均评价时间戳<br>排序要求：将文献按照评价次数逆序排，次数大的靠前，次数相同的平均评价时间相同的靠前。</p>
<p>2. 思想<br>MapReduce在Map端排序时，都只按key排序，而现在我们的排序指标有两个字段count, avgts,组合的key，为了保证排序结果对于第二个字段有序，MapReduce里面叫做Secondary Sort,就是个名称而已，可以扩展为保证第三个字段有序，第四，第五…</p>
<ul>
<li>首先，我们需要重新自定义一个组合Key,即我们将看到的示例中的SortKeyPair类, 包括count和avgts两个字段，并overwrite里面的compare to方法，按我们的需求，count大的靠前，count相同，avgts大的靠前。</li>
<li>其次，利用SortKeyPair，自定义一个SortComparatorClass给Map端排序时用。</li>
<li>再次，自定义PartitionClass.<br>我们单机伪分布式下，默认只有一个Partition，一个Reducer，Partition有无均可，但是分布式环境下，我们要求分布在各个reducer上的结果有序，即评论次数2~5的在reducer0上，6~10的在reducer 1上，11~15在reducer2上，16~20在reducer3上，如果采用取模hash，会造成各个reducer的结果顺序无法控制，因此我们不能用HashPartitioner。而自己的Partitioner则需要只按count进行partition，不管平均评价时间，我们这里把partition的主要的这个key叫做NaturalKey.</li>
<li>最后，我们可以看到reduce端要进行merge，merge过程中，我们需要把相同的count分做一组，需要自定义GroupingComparatorClass。<br>否则如果是按整个组合key进行分组，每个组合key都是不同的，不能分到一个reduce调用中，reduce方法会被调用10万次（单机环境，一个reducer拿到所有10万条文献）， 而如果按count分组，文献评论数目2~20，reduce方法只会被调用最多19次，减少了开销。<br>我们可以看到一个Reducer实例中，reduce方法会被调用多次，按道理是调用一次，但是因为Reducer过程会把数据分成多个组，reduce调用多次是可能的。（具体的需要再看看源码）</li>
</ul>
<h3 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h3><p>1. 自定义组合key类：SortKeyPair.java</p>
<p>[java]<br>public class SortKeyPair implements WritableComparable&lt;SortKeyPair&gt; {<br>    private int count = 0;<br>    private long avgts = 0;</p>
<pre><code>    //要写一个默认构造函数，否则MapReduce的反射机制，无法创建该类报错
public SortKeyPair() {

}

/**
 * 
 * @param count
 * @param timestamp
 *            Average timestamp
 */
public SortKeyPair(int count, long avgts) {
    super();
    this.count = count;
    this.avgts = avgts;
}

public int getCount() {
    return count;
}

public void setCount(int count) {
    this.count = count;
}

public long getAvgts() {
    return avgts;
}

public void setAvgts(long avgts) {
    this.avgts = avgts;
}

@Override
public void write(DataOutput out) throws IOException {
    out.writeInt(this.count);
    out.writeLong(this.avgts);
}

@Override
public void readFields(DataInput in) throws IOException {
    this.count = in.readInt();
    this.avgts = in.readLong();
}

/**
 * We want sort in descending count and descending avgts， Java里面排序默认小的放前面，即返回-1的放前面，这里直接把小值返回1，就会被排序到后面了。
 */
@Override
public int compareTo(SortKeyPair o) {
    int res = this.count &amp;lt; o.getCount() ? 1
            : (this.count == o.getCount() ? 0 : -1);

    if (res == 0) {
        res = this.avgts &amp;lt; o.getAvgts() ? 1
                : (this.avgts == o.getAvgts() ? 0 : -1);
    }

    return res;
}

    //这个方法需要Overrride
@Override
public int hashCode() {
    return Integer.MAX_VALUE - this.count;
}

@Override
public String toString() {
    return this.count + &amp;quot;,&amp;quot; + this.avgts;
}

    // 这个方法，写不写都不会影响的，至少我测的是这样
@Override
public boolean equals(Object obj) {
    if (obj == null) {
        return false;
    }
    if (this == obj) {
        return true;
    }
    if (obj instanceof SortKeyPair) {
        SortKeyPair s = (SortKeyPair) obj;
        return this.count == s.getCount() &amp;amp;&amp;amp; this.avgts == s.getAvgts();
    } else {
        return false;
    }
}
</code></pre><p>}<br>[/java]</p>
<p>2. 自定义SortComparatorClass, 我命名为：CompositeKeyComparator.java</p>
<p>[java]<br>public class CompositeKeyComparator extends WritableComparator{</p>
<pre><code>public CompositeKeyComparator () {
    super(SortKeyPair.class, true);
}

@Override
public int compare(WritableComparable a, WritableComparable b) {
    SortKeyPair s1 = (SortKeyPair)a;
    SortKeyPair s2 = (SortKeyPair)b;

    return s1.compareTo(s2);
}
</code></pre><p>}<br>[/java]</p>
<p>3. 自定义PartitionerClass, 我命名为NaturalKeyPartitioner.java</p>
<p>[java]<br>public class NaturalKeyPartitioner extends Partitioner&lt;SortKeyPair, Text&gt;{</p>
<pre><code>@Override
public int getPartition(SortKeyPair key, Text value, int numPartitions) {
    // % is hash partition, can&apos;t make sure bigger count go to reducer with small id.
    int count = key.getCount();
    if (count &amp;lt;= 5) {
        return 0;
    } else if (count &amp;gt; 5 &amp;amp;&amp;amp; count &amp;lt;= 10) {
        return 1;
    } else if (count &amp;gt; 10 &amp;amp;&amp;amp; count &amp;lt;= 15) {
        return 2;
    } else {
        return 3;
    }
}
</code></pre><p>}<br>[/java]</p>
<p>4. 自定义GroupingComparatorClass,<br>我命名为NaturalKeyGroupComparator.java</p>
<p>[java]<br>public class NaturalKeyGroupComparator extends WritableComparator {</p>
<pre><code>public NaturalKeyGroupComparator() {
    super(SortKeyPair.class, true);
}

@Override
public int compare(WritableComparable a, WritableComparable b) {
    SortKeyPair s1 = (SortKeyPair) a;
    SortKeyPair s2 = (SortKeyPair) b;
    int res = s1.getCount() &amp;lt; s2.getCount() ? 1 : (s1.getCount() == s2
            .getCount() ? 0 : -1);

    return res;
}
</code></pre><p>}<br>[/java]</p>
<p>5. 定义Mapper，Reducer，并定义Job提交</p>
<p>[java]<br>public class SecondarySort {</p>
<pre><code>/**
 * It must be declared static, in case of reflection error
 * 
 * @author lgrcyanny
 * 
 */
public static class SortMapper extends TableMapper&amp;lt;SortKeyPair, Text&amp;gt; {

            // Map input [hbase row key, hbase result], output: [SortKeyPair, Text] (Text中包括了我们最后输出到文件的信息：literature row key, count, avgts)
    @Override
    protected void map(ImmutableBytesWritable key, Result rs,
            Context context) throws IOException, InterruptedException {
        String literature = Bytes.toString(rs.getRow());
        int count = Integer.valueOf(Bytes.toString(rs.getValue(
                Bytes.toBytes(&amp;quot;info&amp;quot;), Bytes.toBytes(&amp;quot;count&amp;quot;))));
        long avgts = Long.valueOf(Bytes.toString(rs.getValue(
                Bytes.toBytes(&amp;quot;info&amp;quot;), Bytes.toBytes(&amp;quot;avgts&amp;quot;))));
        context.write(new SortKeyPair(count, avgts), new Text(literature
                + &amp;quot;,&amp;quot; + count + &amp;quot;,&amp;quot; + avgts));
    }

}

public static class SortReducer extends
        Reducer&amp;lt;SortKeyPair, Text, Text, Text&amp;gt; {

    /**
     * Now the key is max SortKeyPair in the list, we just dump the ordered
     * items
             * Reduce input: [SorKeyPair, list of text], Output: [text, text] 
     */
    @Override
    protected void reduce(SortKeyPair key, Iterable&amp;lt;Text&amp;gt; items,
            Context context) throws IOException, InterruptedException {
        Iterator&amp;lt;Text&amp;gt; iterator = items.iterator();
        while (iterator.hasNext()) {
            context.write(null, iterator.next());// 因为value中已经包括了需要输出的信息，SortKeyPair的信息不需要输出，key设置为null即可。
        }
    }
}

public static void main(String[] args) throws IOException,
        ClassNotFoundException, InterruptedException {
            // 因为数据源是HBase，因此需要使用HBase中的MapReduce启动设置，可以参考HBase官方网站
            // 自己做测试，可以改成文件输入，看具体需求而定
    Configuration conf = HBaseConfiguration.create();
    Job job = new Job(conf, &amp;quot;Secondarysort&amp;quot;);
    job.setJarByClass(SecondarySort.class);
    Scan scan = new Scan();
    scan.addFamily(Bytes.toBytes(&amp;quot;info&amp;quot;));
    scan.setCaching(5000); // Default is 1, set 500 improve performance
    scan.setCacheBlocks(false); // Close block cache for MR job
    TableMapReduceUtil.initTableMapperJob(&amp;quot;pb_stat_comments_count&amp;quot;, scan,
            SortMapper.class, SortKeyPair.class, Text.class, job);
    job.setReducerClass(SortReducer.class);

    // For secondary sort, 这里设置自定义排序的三个类
    job.setSortComparatorClass(CompositeKeyComparator.class);
    job.setPartitionerClass(NaturalKeyPartitioner.class);
    job.setGroupingComparatorClass(NaturalKeyGroupComparator.class);

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);

            // Reducer的输出，设置为文件输出
    FileOutputFormat.setOutputPath(job, new Path(&amp;quot;secondary-sort-res&amp;quot;));

    long start = System.currentTimeMillis();
    boolean res = job.waitForCompletion(true);
    long end = System.currentTimeMillis();
    if (res) {
        System.out.println(&amp;quot;Job done with time &amp;quot; + (end - start));
    } else {
        throw new IOException(&amp;quot;Job exit with error.&amp;quot;);
    }
}
</code></pre><p>}<br>[/java]</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>MapReduce做排序，麻烦的不是写Mapper和Reducer，而是自定义<br>CompositeKeyClass<br>SortComparatorClas<br>PartitionerClass<br>GroupingComparatorClass<br>利用好Shuffle这个很magic课程，可以实现很多奇妙的功能。<br>本次案例的代码在<a href="https://github.com/lgrcyanny/PaperBook-MapReduce/tree/master/src/com/paperbook/mapreduce/stat/secondarysort" target="_blank" rel="external">我的GitHub上</a>。<br>这些都是个人的理解，如有不对之处，还请指正。</p>
<p>最后，本学期的云计算课到最后了才让我们去些MapReduce程序，之前做无关的MySQL的项目，然后迁移HBase，看着很高端，但是做那个网站到底有什么意义，这是云计算，不是前端计算，HBase in Action中也提到，HBase的设计和MySQL的设计是完全不同的，而MySQL我相信到这个阶段的同学们都会用，何必再次重复劳动呢？直接上HBase不就可以了，本末倒置是我对本学期的课程的失望和吐槽。课程内容多半是概念，涉及技术的有多少？看论文，看概念每一个研究生都可以做到，而我想我们缺的是实践上的指导，比如MapReduce，除了能讲讲WordCount，还能再深入点么？Shuffle的过程可以多说说么？Zookeeper里面的PAXOS算法可以跟我们说说么？学院有Hadoop的集群机器，可以让我们去玩玩不？而不是写那个无聊的网站。<br>我个人觉得让我们去写一些Hadoop的架构分析，看看源码，也比写那个无聊的网站强。最后9次作业，做的头大，重复劳动，体力劳动，最后还考试，说写的多分会高，有意思么？那么多作业还考试，数据库课程就两次作业+一个论文，但是我觉得我学了也复习了很多知识，比起那些大而空的概念，高到不知哪里去了。<br>我对云计算，Hadoop,HBase感兴趣，本学期学了很多，但不是课堂上学的，估计我最后对云计算的概念们，要靠下周背PPT了。 我对云计算有兴趣，但对本学期的课程没兴趣。吐槽都是不好的情绪，调整好心态，好好考试吧~BTW，下周还有一次作业。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MapReduce是Hadoop中处理大数据的方法，是一个处理大数据的简单算法、编程泛型。虽然思想简单，但其实真正用起来还是有很多问题，不是所有的问题都可以像WordCount那样典型和直观, 有很多需要trick的地方。MapReduce的中心思想是分而治之，数据要松耦合，可以划分为小数据集并行处理，如果数据本身在计算上存在很强的依赖关系，就不要赶鸭子上架，用MapReduce了。&lt;br&gt;MapReduce编程中，最重要的是要抓住Map和Reduce的input和output，好的input和output可以降低实现的复杂度。最近，写了很多关于MapReduce的job，有倒排索引，统计，排序等。其中，对排序花费了一番功夫，MapReduce做WordCount很好理解,&lt;br&gt;Map input:[offset, text],  output: [word, 1],&lt;/p&gt;
&lt;p&gt;Reduce input: [word, 1], output: [word, totalcount],还可以设置Combiner进行优化。&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.cyanny.com/categories/hadoop/"/>
    
      <category term="HBase" scheme="http://www.cyanny.com/categories/hadoop/hbase/"/>
    
    
      <category term="Hadoop" scheme="http://www.cyanny.com/tags/hadoop/"/>
    
      <category term="HBase" scheme="http://www.cyanny.com/tags/hbase/"/>
    
      <category term="MapReduce" scheme="http://www.cyanny.com/tags/mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>HBase Architecture Analysis Part 3 Pros and Cons</title>
    <link href="http://www.cyanny.com/2014/03/13/hbase-architecture-analysis-part-3-pros-cons/"/>
    <id>http://www.cyanny.com/2014/03/13/hbase-architecture-analysis-part-3-pros-cons/</id>
    <published>2014-03-13T14:13:20.000Z</published>
    <updated>2016-09-27T01:03:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="5-HBase-Physical-Architecture"><a href="#5-HBase-Physical-Architecture" class="headerlink" title="5. HBase Physical Architecture"></a>5. HBase Physical Architecture</h2><p>Figure 5.1 shows the deployment view for HBase cluster:<br>HBase is the master-slave cluster on top of HDFS. The classic deployment is as follows:<br>➢<strong> Master node:</strong> one HMaster and one NameNode running on a machine as the master node.<br>➢ <strong>Slave node:</strong> Each node is running one HRegionServer and one DataNode. And each node report status to the master node and Zookeeper.<br>➢<strong> Zookeeper:</strong> HBase is shipped with ensemble Zookeeper, but for large clusters, using existing Zookeeper is better. Zookeeper is crucial, the HMaster and HRegionServers will register on Zookeeper.<br>➢ <strong>Client</strong>: There can be many clients to access HRegionServer, like Java Client, Shell Client, Thrift Client and Avro Client</p>
<p><a id="more"></a> <a href="http://tinypic.com?ref=6eg29d" target="_blank" rel="external"><img src="http://i58.tinypic.com/6eg29d.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure 5.1 HBase Deployment View</p>
<h2 id="6-HBase-Pros"><a href="#6-HBase-Pros" class="headerlink" title="6.HBase Pros"></a>6.HBase Pros</h2><p><strong>1. Master-Slave Architecture</strong><br>HBase build on top HDFS, HDFS is mater-slave architecture, HBase follows this style. It will improve the interoperability between HBase and HDFS.<br>This style do good for load balance, the HMaster takes over the load balance job, assign regions to region servers and auto failover dead RegionServers.</p>
<p><strong>2. Real-time , random big data access</strong><br>HDFS handles large blocks of data well, but small blocks of data with real-time access is not efficient. HBase uses LSM-trees as data storage architecture. Data is written into WAL first, then to Mem Store. WAL is in case of data lost before Mem Store is flushed to HDFS. Mem Store will be flushed to HDFS when it’s filled up. The mechanism insures that data write is operated in memory, no need of HDFS disk access, which improves write performance.<br>Meanwhile, data read accesses Mem Store first, then to Block Cache, and last the HFiles, so for random data access, if there is cache in memory, the read performance is very well.<br>In short, HBase brings big data online, it is efficient for real-time operations on big data(TB, PB);</p>
<p><strong>3. Column-Oriented data model for big sparse table</strong><br>HBase is NO-SQL, column-family-oriented, each column family is stored together in HStore. The key-value data model stores only non-empty values, no null values. So for large spares table, the disk usage is great in HBase, not like RDBMS storing large quantities of null values.</p>
<p><strong>4. LSM-trees vs. B+ tree</strong><br>HBase data storage architecture is LSM-trees. A big feature of LSM-trees is the merge housekeeping, which merge small files into a larger one to reduce disk seek. The housekeeping is a background process, which will not impact real-time data access.<br>LSM-trees transform random data access into sequential data access, which improves read performance. But B+ tree in RDBMS requires more disk seeks for data modifications, which is very efficient for big data process.</p>
<p><strong>5. Row-level Atomic</strong><br>HBase has no strictly ACID features like RDBMS, how to insure data consistent in HBase and how to avoid dirty read. HBase is row-level automic, in other words, each Put operation on a given row either success or fail. Meanwhile, CheckAnd<em>, Increment</em> operations are also atomic.<br>The scan operation across the table is not on a snapshot of a table, if the data is updated before a Scan operation, the updated version is read by Scan, It ensures data consistent.<br>HBase is good at loose coupled data and not high requirements for consistent. Row-level atomic is a mechanism to improve data consistent, but data consistent is really a shortage of HBase.</p>
<p><strong>6. High Scalability</strong><br>HBase data is stored across the cluster, the cluster has many HRegionServer, which can be scaled. HBase build on top of HDFS, HDFS is high scalability, that is the DataNode can be scaled. HDFS brings great scalability to HBase. Moreover, the master-slave architecture do well in data scalability.</p>
<p><strong>7. High Reliability</strong><br>The HBase cluster is built on commodity hardware, which will be dead at any time. HBase data is replicated across the cluster. Data replication is done by HDFS. HDFS stores 3 replications for each block: on the same node, on another node on the same rack, on another node on another rack. The feature brings high reliability for HBase.</p>
<p><strong>8. Auto Failover</strong><br>HMaster is in charge of auto failover work. When HMaster detects the dead HRegionServer. It will find that the region assignment is invalid, and it will do region assignment again. Auto failover brings great availability for HBase, which is no need of manually HRegionServer failover.</p>
<p><strong>9. Data auto sharding</strong><br>Since data is stored distributed across the cluster. HBase must makes use of all of the machines in the cluster. HRegionServer stores many HRegions. When a HRegion size reaches its threshold, HRegionServer will split it into half.<br>In addition, when the number of regions is over the threshold that the HMaster can handle, the HMaster will merge small regions into a larger one. Auto sharding brings HBase great load balance and keeps programmer away from the details of distributed data sharding.</p>
<p><strong>10. Simple Client Interface</strong><br>HBase provides 5 basic data operations : Put, Get, Delete, Scan and Increment. The Interface is easy to use. HBase provide java interface, and non java interface: Thrift, Avro, REST and Shell. Each interface is transparent for developers to do data operations on HBase.</p>
<h2 id="7-HBase-Cons"><a href="#7-HBase-Cons" class="headerlink" title="7. HBase Cons"></a>7. HBase Cons</h2><p><strong>1. Single Point of Failure (SPOF)</strong><br>The master-slave architecture always has SPOF, HBase is no exception. When there is only on HMaster in a cluster, if the cluster has more than 4000 nodes, the HMaster is the performance bottleneck. Even though client talks directly to HRegionServer, when the HMaster is down, the cluster can be “steady” for a short time, HMaster long time down will bring down the cluster.<br>It is possible to start multi HMaster in a cluster, but only one is active, which is also a performance bottleneck.</p>
<p><strong>2. No transaction</strong><br>As we talked before, HBase data consistent is a shortage. Interrow operations are not atomic, which will bring dirty read. HBase is good at louse coupled data but not good at highly rational data records. That is an important point we must be aware of.</p>
<p><strong>3. No Join, if you want, use MapReduce</strong><br>HBase has no join operation, so data model has to be redesign when migrate from RDBMS, which is a big headache. For highly rational data records, HBase will make you sick. But if you really want implement join, you can use MapReduce, please refer to “HBase in Action”.</p>
<p><strong>4. Index only on key, sorted by key, but RDBMS can be indexed on arbitrary column</strong><br>The column-oriented data model is key-value style, HBase only has index on key: &lt;rowkey, column family, qualifier, version&gt;, rowkey design is the most important thing.<br>The data model paradigm for HBase is quite different for RDBMS. HBase has no index on value. Scan with columnvalue filter for big data is very slow. But RDBMS can be indexed on arbitrary column.<br>If you want to improve index for HBase, you can use MapReduce to build Inverted Index.</p>
<p><strong>5. Security Problem</strong><br>Finally, HBase has security problem. RDBMS like MySQL has authentication and authorization feature, different user has different data access authority. But HBase has no such feature yet. But we can see that, improve security will lower the performance, If the online application can insure the security, HBase will has no need to care about that.</p>
<p><strong>Refereces</strong><br>[1] George, Lars. HBase: the definitive guide. O’Reilly Media, Inc., 2011.<br>[2] Dimiduk, Nick, Amandeep Khurana, and Mark Henry Ryan. HBase in action. Manning, 2013.<br>[3] Chang, Fay, et al. “Bigtable: A distributed storage system for structured data.”ACM Transactions on Computer Systems (TOCS) 26.2 (2008): 4.<br>[4] White, Tom. Hadoop: The Definitive Guide: The Definitive Guide. O’Reilly Media, 2009.<br>[5] Kruchten, Philippe B. “The 4+ 1 view model of architecture.” Software, IEEE12.6 (1995): 42-50.<br>[6] Apache HBase, “The Apache HBase Reference Guide”, apache.org, 2013</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;5-HBase-Physical-Architecture&quot;&gt;&lt;a href=&quot;#5-HBase-Physical-Architecture&quot; class=&quot;headerlink&quot; title=&quot;5. HBase Physical Architecture&quot;&gt;&lt;/a&gt;5. HBase Physical Architecture&lt;/h2&gt;&lt;p&gt;Figure 5.1 shows the deployment view for HBase cluster:&lt;br&gt;HBase is the master-slave cluster on top of HDFS. The classic deployment is as follows:&lt;br&gt;➢&lt;strong&gt; Master node:&lt;/strong&gt; one HMaster and one NameNode running on a machine as the master node.&lt;br&gt;➢ &lt;strong&gt;Slave node:&lt;/strong&gt; Each node is running one HRegionServer and one DataNode. And each node report status to the master node and Zookeeper.&lt;br&gt;➢&lt;strong&gt; Zookeeper:&lt;/strong&gt; HBase is shipped with ensemble Zookeeper, but for large clusters, using existing Zookeeper is better. Zookeeper is crucial, the HMaster and HRegionServers will register on Zookeeper.&lt;br&gt;➢ &lt;strong&gt;Client&lt;/strong&gt;: There can be many clients to access HRegionServer, like Java Client, Shell Client, Thrift Client and Avro Client&lt;/p&gt;
&lt;p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.cyanny.com/categories/hadoop/"/>
    
      <category term="HBase" scheme="http://www.cyanny.com/categories/hadoop/hbase/"/>
    
    
      <category term="Hadoop" scheme="http://www.cyanny.com/tags/hadoop/"/>
    
      <category term="Big Data" scheme="http://www.cyanny.com/tags/big-data/"/>
    
      <category term="HBase" scheme="http://www.cyanny.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>HBase Architecture Analysis Part2(Process Architecture)</title>
    <link href="http://www.cyanny.com/2014/03/13/hbase-architecture-analysis-part2-process-architecture/"/>
    <id>http://www.cyanny.com/2014/03/13/hbase-architecture-analysis-part2-process-architecture/</id>
    <published>2014-03-13T14:00:11.000Z</published>
    <updated>2016-09-27T01:03:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="4-Process-Architecture"><a href="#4-Process-Architecture" class="headerlink" title="4. Process Architecture"></a>4. Process Architecture</h2><h3 id="4-1-HBase-Write-Path"><a href="#4-1-HBase-Write-Path" class="headerlink" title="4.1 HBase Write Path"></a>4.1 HBase Write Path</h3><p>The client doesn’t write data directly into HFile on HDFS. Firstly it writes data to WAL(Write Ahead Log), and Secondly, writes to MemStore shared by a HStore in memory.<br><a id="more"></a><br><a href="http://tinypic.com?ref=20i77l0" target="_blank" rel="external"><img src="http://i58.tinypic.com/20i77l0.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure4.1 HBase Write Path</p>
<p><strong>MemStore</strong> is a write buffer(64MB by default). When the data in MemStore accumulates its threshold, data will be flush to a new HFile on HDFS persistently. Each Column Family can have many HFiles, but each HFile only belongs to one Column Family.<br><strong>WAL</strong> is for data reliability, WAL is persistent on HDFS and each Region Server has only on WAL. When the Region Server is down before MemStore flush, HBase can replay WAL to restore data on a new Region Server.<br>A data write completes successfully only after the data is written to WAL and MemStore.</p>
<h3 id="4-2-HBase-Read-Path"><a href="#4-2-HBase-Read-Path" class="headerlink" title="4.2 HBase Read Path"></a>4.2 HBase Read Path</h3><p>As shown in Figure 4.2, it’s the read path of HBase.<br>1. Client will query the MemStore in memory, if it has the target row.<br>2. When MemStore query failed, client will hit the BlockCache.<br>3. After the MemStore and BlockCache query failed, HBase will load HFiles into memory which may contain the target row info.<br>The MemStore and BlockCache is the mechanism for real time data access for distributed large data.<br><strong>BlockCache is a LRU(Lease Recently Used) priority cache.</strong> Each RegionServer has a single BlockCache. It keeps frequently accessed data from HFile in memory to reduce disk data reads. The “Block”(64KB by default) is the smallest index unit of data or the smallest unit of data that can be read from disk by one pass.<br>For random data access, small block size is preferred, but block index consumes more memory. And for sequential data access, large block size is better, fewer index save more memory.</p>
<p><a href="http://tinypic.com?ref=290x9qt" target="_blank" rel="external"><img src="http://i61.tinypic.com/290x9qt.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure 4.2 HBase Read Path</p>
<h3 id="4-3-HBase-Housekeeping-HFile-Compaction"><a href="#4-3-HBase-Housekeeping-HFile-Compaction" class="headerlink" title="4.3.HBase Housekeeping: HFile Compaction"></a>4.3.HBase Housekeeping: HFile Compaction</h3><p>The data of each column family is flush into multiple HFiles. Too many HFiles means many disk data reads and lower the read performance. Therefore, HBase do HFile compaction periodically.<br><a href="http://tinypic.com?ref=2nk85xj" target="_blank" rel="external"><img src="http://i62.tinypic.com/2nk85xj.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure4.3 HFile Compaction</p>
<p><strong>➢Minor Compaction</strong><br>It happens on multiple HFiles in one HStore. Minor compaction will pick up a couple of adjacent small HFiles and rewrite them into a larger one.<br>The process will keep the deleted or expired cells. The HFile selection standard is configurable. Since minor compaction will affect HBase performace, there is an upper limit on the number of HFiles involved (10 by default).<br><strong>➢Major Compaction</strong><br>Major Compaction compact all HFiles in a HStore(Column Family) into one HFile. It is the only chance to delete records permanently. Major Compaction will usually have to be triggered manually for large clusters.<br>Major Compaction is not region merge, it happens to HStore which will not result in region merge.</p>
<h3 id="4-4-HBase-Delete"><a href="#4-4-HBase-Delete" class="headerlink" title="4.4 HBase Delete"></a>4.4 HBase Delete</h3><p>When HBase client send delete request, the record will be marked “tombstone”, it is a “predicate deletion”, which is supported by LSM-tree. Since HFile is immutable, deletion isn’t available for HFile on HDFS. Therefore, HBase adopts major compaction(Section 4.3) to clean up deleted or expired records.</p>
<h3 id="4-5-Region-Assignment"><a href="#4-5-Region-Assignment" class="headerlink" title="4.5 Region Assignment"></a>4.5 Region Assignment</h3><p>It is a main task of HMaster:<br>1. HMaster invoke the AssignmentManager to do region assignment.<br>2. AssignmentManager checks the existing region assignments in .META.<br>3. If region assignment is valid, then keep the region.<br>4. If region assignment is invalid, then the LoadBalancerFactory will create a DefaultLoadBalancer<br>5. DefaultLoadBalancer will assign the new region randomly to a RegionServer<br>6. Update the assignment to .META.<br>7. The RegionServer open the new region</p>
<h3 id="4-6-Region-Split"><a href="#4-6-Region-Split" class="headerlink" title="4.6 Region Split"></a>4.6 Region Split</h3><p>Region split is the work of RegionServer, not participated by HMaster. When a region in a RegionServer accumulates over size threshold, RegionServer will split the region into half.<br>1. RegionServer offline the region to be split.<br>2. RegionServer split the region into two half<br>3. Update new daughter regions info to .META.<br>4. Open the new daughter regions.<br>5. Report the split info to HMaster.</p>
<h3 id="4-7-Region-Merge"><a href="#4-7-Region-Merge" class="headerlink" title="4.7 Region Merge"></a>4.7 Region Merge</h3><p>A RegionServer can’t have too many regions, because too many regions bring large cost of memory and lower the performance of RegionServer. Meanwhile, HMaster can’t handle load balance with too many regions.<br>Therefore, when the number of regions is over a threshold, region merge is trigged. Region Merge is joined by RegionServer and HMaster.<br>1. Client send RPC region merge request to HMaster<br>2. HMaster moves regions together to the same RegionServer where the more heavily loaded region resided.<br>3. HMaster send request to the RegionServer to run region merge.<br>4. The RegionServer offline the regions to be merged.<br>5. The RegionServer Merge the regions on the local file system.<br>6. Delete the meta info of the merging regions on .META. table, add new meta info to .META. table.<br>7. Open the new merged region<br>8. Report the merge to HMaster</p>
<h3 id="4-8-Auto-Failover"><a href="#4-8-Auto-Failover" class="headerlink" title="4.8 Auto Failover"></a>4.8 Auto Failover</h3><h4 id="4-8-1-RegionServer-Failover"><a href="#4-8-1-RegionServer-Failover" class="headerlink" title="4.8.1 RegionServer Failover"></a>4.8.1 RegionServer Failover</h4><p>When a region server fails, HMaster will do failover automatically.<br>1. RegionServer is down<br>2. HMaster will detect the unavailable RegionServer when there is no heartbeat report.<br>3. HMaster start region reassignment, detect that the region assignment is invalid, then re-assign the regions like the region assignment sequence.</p>
<h4 id="4-8-2-Master-Failover"><a href="#4-8-2-Master-Failover" class="headerlink" title="4.8.2 Master Failover"></a>4.8.2 Master Failover</h4><p>In centralized architecture style, the SPOF is a problem, you may ask How does HBase will do when HMaster failed? There two safeguards for SPOF:<br><strong>1. Multi-Master environment</strong><br>HBase can run in multi-master cluster. There is only one active master, when the master is down, the remaining master will take over the master role.<br>It looks like Hadoop HDFS new feature high availability, a standby NameNode will take over the NameNode role when the active one failed.<br><strong>2. Catalog tables are on RegionServers</strong><br>When client send read/write request to HBase, it talks directly to RegionServer, not HMaster. Hence, the cluster can be steady for a time without HMaster. But it requires that the HMaster failover as soon as possible, HMaster is the vital of the cluster.</p>
<h4 id="4-9-Data-Replication"><a href="#4-9-Data-Replication" class="headerlink" title="4.9 Data Replication"></a>4.9 Data Replication</h4><p>For data reliability, HBase replicates data blocks across the cluster. This is achieved by Hadoop HDFS.<br>By default, there 3 replicas:<br>1. The First replica is written to local node.<br>2. The second replica is written to a random node on a different rack.<br>3. The third replica is on a random node on the same rack<br>HBase stores HFiles and WAL on HDFS. When RegionServer is down, new region assignment can be done by read replicas of HFile and replay the replicas the WAL. In short, HBase has high reliability.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;4-Process-Architecture&quot;&gt;&lt;a href=&quot;#4-Process-Architecture&quot; class=&quot;headerlink&quot; title=&quot;4. Process Architecture&quot;&gt;&lt;/a&gt;4. Process Architecture&lt;/h2&gt;&lt;h3 id=&quot;4-1-HBase-Write-Path&quot;&gt;&lt;a href=&quot;#4-1-HBase-Write-Path&quot; class=&quot;headerlink&quot; title=&quot;4.1 HBase Write Path&quot;&gt;&lt;/a&gt;4.1 HBase Write Path&lt;/h3&gt;&lt;p&gt;The client doesn’t write data directly into HFile on HDFS. Firstly it writes data to WAL(Write Ahead Log), and Secondly, writes to MemStore shared by a HStore in memory.&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.cyanny.com/categories/hadoop/"/>
    
      <category term="HBase" scheme="http://www.cyanny.com/categories/hadoop/hbase/"/>
    
    
      <category term="Hadoop" scheme="http://www.cyanny.com/tags/hadoop/"/>
    
      <category term="Big Data" scheme="http://www.cyanny.com/tags/big-data/"/>
    
      <category term="HBase" scheme="http://www.cyanny.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>HBase Architecture Analysis Part1(Logical Architecture)</title>
    <link href="http://www.cyanny.com/2014/03/13/hbase-architecture-analysis-part1-logical-architecture/"/>
    <id>http://www.cyanny.com/2014/03/13/hbase-architecture-analysis-part1-logical-architecture/</id>
    <published>2014-03-13T13:18:32.000Z</published>
    <updated>2016-09-27T01:03:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><p>Apache HBase is an open source column-oriented database. <strong>It is often described as a sparse, consistent, distributed, multi-dimensional sorted map.</strong> HBase is modeled after Google’s “Bigtable: A distributed Storage System for Structured Data”, which can host very large tables with billions of rows, X millions of columns.<br><a id="more"></a><br>HBase is a No-SQL database, and it has very different paradigms from traditional RDBMS, which speaks SQL and enforce relationships upon data. HBase stores structured and semi-structured data with key-value style. Each row in HBase is located by<strong> &lt;Rowkey, Column Family, Column Qualifier, Version&gt;.</strong></p>
<p>HBase stores data distributed on Hadoop HDFS shipped with random data access. HDFS is Hadoop Distributed File System, which stores big data sets with streaming style. It has high reliability, scalability and availability but the cons is that HDFS split data into blocks (64MB or 128MB), it can handle large chucks of data very well, but not very well for small piece of data and low-latency data access. In other words, HDFS isn’t fit for online real time read and write, and that’s the meaning of HBase. HBase goal is to make use of cloud distributed data storage for online real time data access.<br>The blog analyzes the architecture of HBase with the method of “4+1” views by Kruthten.</p>
<h2 id="2-Use-Case-View"><a href="#2-Use-Case-View" class="headerlink" title="2. Use Case View"></a>2. Use Case View</h2><p><a href="http://tinypic.com?ref=346rmhh" target="_blank" rel="external"><img src="http://i60.tinypic.com/346rmhh.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure 2.1 User Case View of HBase<br>The figure 2.1 shows the use case view of HBase. Since from the architecture view, the figure gives more details about the main features of HBase:<br>As shown on figure 2.1, the main features of HBase are as follows:<br><strong>1. For User:</strong></p>
<ul>
<li><strong>Read/Write big data in real time:</strong> HBase provides transparent and simple interface for user. User has no need to care about details about data transforming.</li>
<li><strong>Manage Data Model:</strong> User Need to create and manage column-oriented data model about business logic, and HBase provide Interface for them to do management.</li>
<li><p><strong>Manage System:</strong> User can manually configure and monitor the status of HBase<br><strong> 2. For the System</strong></p>
</li>
<li><p><strong>Store data distributed on HDFS:</strong> HBase has great scalability and reliability based on HDFS.</p>
</li>
<li><strong> Data random online, real time access:</strong> HBase provides block cache and index on files for real time queries.</li>
<li><strong>Automatic Data Sharding:</strong> HBase partitions big data automatically on distributed node.</li>
<li><strong>Automatic failover:</strong> Since hardware failure on clusters is inevitable, HBase can failover automatically for high reliability and availability.</li>
</ul>
<h2 id="3-Logical-Architecture"><a href="#3-Logical-Architecture" class="headerlink" title="3. Logical Architecture"></a>3. Logical Architecture</h2><h3 id="3-1-High-Level-Architecture-and-Style"><a href="#3-1-High-Level-Architecture-and-Style" class="headerlink" title="3.1 High Level Architecture and Style"></a>3.1 High Level Architecture and Style</h3><p><a href="http://tinypic.com?ref=2eezt6x" target="_blank" rel="external"><img src="http://i60.tinypic.com/2eezt6x.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure 3.1 High Level Architecture for HBase<br>As shown in Figure3.1, the HBase architecture is layered overall, layered architecture style is a typical style for database.<br><strong>1. Layered Architecture Style</strong><br>➢<strong> Client Layer:</strong> The layer is for user, HBase provides easy-to-use java client API, and ships with non-java API as well, such as shell, Thrift, Avro and REST.<br>➢ <strong>HBase Layer:</strong> The layer is the core logic of HBase. Overall, HBase has two important components: HMaster and RegionServer. We will discuss them later.<br>Moreover, another important component is Zookeeper, which is a distributed coordination service modeled after Google’s Chubby. It’s another open source project under Apache.<br>➢ <strong>Hadoop HDFS Layer:</strong> HBase build on top of HDFS, which provides distributed data storage and data replication.</p>
<p><strong>2. Master-Slave Architecture Style</strong><br>For distributed applications, how to handle data blocks is a key point. There are two typical styles: <strong>centralized and non-centralized</strong>. Centralized mode is very available for scalability and load balancer, but there is Single Pont of Failure(SPOF) problem and performance bottleneck on the master node. Non-centralized mode has no SPOF problem and performance bottleneck, but not very easy to do load balance and there is data consistent problem.<br>HBase utilizes Centralized Mode: Master-Slave Style. BTW, Hadoop is also master-slave. The architecture style in Hadoop ecosystem is kind of consistent master-slave style.<br><a href="http://tinypic.com?ref=ieff35" target="_blank" rel="external"><img src="http://i62.tinypic.com/ieff35.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure 3.2 Master-Slave Architecture Style of HBase<br>As shown in figure 3.2:</p>
<ul>
<li><strong>HMaster</strong> is the master in such style, which is responsible for RegionServer monitor, region assignment, metadata operations, RegionServer Failover etc. In a distributed cluster, HMaster runs on HDFS NameNode.</li>
<li><strong>RegionServer</strong> is the slave, which is responsible for serving and managing regions. In a distributed cluster, it runs on HDFS DataNode.</li>
<li><strong>Zookeeper</strong> will track the status of Region Server, where the root table is hosted. Since HBase 0.90.x, it introduces an even more tighter integration with Zookeeper. The heartbeat report from Region Server to HMaster is moved to Zookeeper, that is zookeeper has the responsibility of tracking Region Server status. Moreover, Zookeeper is the entry point of client, which enable query Zookeeper about the location of the region hosting the –ROOT- table.</li>
</ul>
<h3 id="3-2-HBase-Data-Model"><a href="#3-2-HBase-Data-Model" class="headerlink" title="3.2 HBase Data Model"></a>3.2 HBase Data Model</h3><p>HBase data model is Column-Family-Oriented. Some special characteristics of HBase data model is as follows:<br>➢ <strong>Key-Value store:</strong> Each cell in a table is specified by the four dimensional key: . The table will not store null values like RDBMS, it works well for large sparse table.<br>➢ <strong>Sorted by key, index only on key.</strong> Row key design is the only most important thing in HBase Schema Design.<br>➢ <strong>Each cell can store different versions,</strong> specified by Timestamp by default<br>➢ The column qualifier is<strong> schema-less,</strong> can be changed during run-time<br>➢ The column qualifier and value is treated a<strong>s arbitrary bytes</strong><br><a href="http://tinypic.com?ref=8wmlhj" target="_blank" rel="external"><img src="http://i58.tinypic.com/8wmlhj.jpg" alt="Image and video hosting by TinyPic"></a><br>Table 3.1 The logical view of data model<br>As shown in Table 3.1, is a logical view of a big table, the table is parse, each row has a unique row key, and the table has three column family: cf1, cf2, cf3. cf1 has two qualifiers: q1-1, q1-2. q1-1 on timestamp t1 has value v1.<br><a href="http://tinypic.com?ref=14smvbr" target="_blank" rel="external"><img src="http://i61.tinypic.com/14smvbr.jpg" alt="Image and video hosting by TinyPic"></a><br>Table 3.2 The physical view of data model<br>In RDBMS, a big table as shown in Table 3.1 will store a lot of null values, but for HBase is not, key-value styles make HBase store only non-empty values as shown in Table3.2. It’s the physical view, a table is partitioned by Column Family, each Column Family is stored in a HStore in a table region on a Region Server.<br>Since the main purpose of the report is architecture analysis, the table design of HBase will not discuss in detail, you can refer to the “HBase in Action”</p>
<h3 id="3-3-HBase-Distributed-Mode"><a href="#3-3-HBase-Distributed-Mode" class="headerlink" title="3.3 HBase Distributed Mode"></a>3.3 HBase Distributed Mode</h3><h4 id="3-3-1-Big-Table-Splitting"><a href="#3-3-1-Big-Table-Splitting" class="headerlink" title="3.3.1 Big Table Splitting"></a>3.3.1 Big Table Splitting</h4><p>HBase can run on local file system, called “standalone mode” for development and testing. For production, HBase should run on cluster on top of HDFS. Big tables are split and stored across the cluster.<br><a href="http://tinypic.com?ref=16h0rdk" target="_blank" rel="external"><img src="http://i62.tinypic.com/16h0rdk.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure 3.3 Region split on big table<br>A table is split into roughly equal size. Regions are assigned to Region Servers across the cluster. And Region Servers host roughly equal number of regions. As shown in Figure3.3, Table A is split into four regions, each of which is assigned to a Region Server.</p>
<h4 id="3-3-2-Region-Server"><a href="#3-3-2-Region-Server" class="headerlink" title="3.3.2 Region Server"></a>3.3.2 Region Server</h4><p>As shown in figure3.4, the Region Server Architecture. It contains several components as follows:</p>
<ul>
<li><strong>One Block Cache</strong>, which is a LRU priority cache for data reading.</li>
<li><strong>One WAL(Write Ahead Log):</strong> HBase use Log-Structured-Merge-Tree(LSM tree) to process data writing. Each data update or delete will be write to WAL first, and then write to MemStore. WAL is persisted on HDFS.</li>
<li><strong>Multiple HRegions:</strong> each HRegion is a partition of table as we talk about in 3.3.1.</li>
<li><strong>In a HRegion:</strong> Multiple HStore: Each HStore is correspond to a Column Family</li>
<li><strong>In a HStore:</strong>  <strong>One MemStore:</strong> store updates or deletes before flush to disk. <strong>Multiple StoreFile</strong>, each of which is correspond to a HFile</li>
<li><strong>A HFile</strong> is immutable, flushed from MemStore, persisted on HDFS<br><a href="http://tinypic.com?ref=c28l" target="_blank" rel="external"><img src="http://i60.tinypic.com/c28l.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure3.4 Region Server Architecture</li>
</ul>
<h4 id="3-3-3-ROOT-and-META-table"><a href="#3-3-3-ROOT-and-META-table" class="headerlink" title="3.3.3 -ROOT- and .META table"></a>3.3.3 -ROOT- and .META table</h4><p>Since table are partitioned and store across the cluster. How can the client find which region hosting a specific row key range? There are two special catalog tables, -ROOT- and .META. table for this.<br>➢<strong>.META. table</strong>: host the region location info for a specific row key range. The table is stored on Region Servers, which can be split into as many region as required.<br>➢<strong>-ROOT- table:</strong> host the .META. table info. There is only one Region Server store the –ROOT- table. And the Root region never split into more than one region.<br>The –ROOT- and .META. table structure logically looks as a B+ tree as shown in figure 3.5.<br>The RegionServer RS1 host the –ROOT- table, the .META. table is split into 3 regions: M1, M2, M3, hosted on RS2, RS3, RS1. Table T1 contains three regions, T2 contains four regions. For example, T1R1 is hosted on RS3, the meta info is hosted on M1.<br><a href="http://tinypic.com?ref=soa649" target="_blank" rel="external"><img src="http://i60.tinypic.com/soa649.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure 3.5 –ROOT-, .META., and User table viewed as B+ tree</p>
<h4 id="3-3-4-Region-Lookup"><a href="#3-3-4-Region-Lookup" class="headerlink" title="3.3.4 Region Lookup"></a>3.3.4 Region Lookup</h4><p>How can client find where the –ROOT- table is? Zookeeper does this work, and how to find the region where the target row is. Even though it belongs to process architecture, it is related to the section, hence we discuss it in advance. let’s look at an example as shown in figure3.6.<br>1. Client query Zookeeper: where is the –ROOT-? On RS1.<br>2. Client request RS1: Which meta region contains row: T10006? META1 on RS2<br>3. Client request RS2: Which region can find the row T10006? Region on RS3<br>4. Client get the from the region on RS3<br>5. Client cache the region info, and is refreshed until the region location info changed.<br><a href="http://tinypic.com?ref=30ml56q" target="_blank" rel="external"><img src="http://i60.tinypic.com/30ml56q.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure 3.6 Region Lookup Process</p>
<h3 id="3-4-HBase-Communication-Protocol"><a href="#3-4-HBase-Communication-Protocol" class="headerlink" title="3.4 HBase Communication Protocol"></a>3.4 HBase Communication Protocol</h3><p><a href="http://tinypic.com?ref=2zggmrk" target="_blank" rel="external"><img src="http://i61.tinypic.com/2zggmrk.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure 3.7 HBase Communication Protocol<br>In HBase 0.96 release, HBase has moved its communication protocol to<strong> Protocol Buffer.</strong> Protocol buffer is a method of serializing structured data developed by Google. Google uses it for almost all of its internal RPC protocol and file formats.<br>Protocol buffer involves an Interface Description Language for cross language service like Apache Thrift.<br>Basically, the communication between sub-systems of HBase is RPC, which is implemented in Protocol Buffer.<br>The main protocols are as follows:<br>➢ <strong>MasterMonitor Protocol:</strong> client use it to monitor the status of HMaster<br>➢ <strong>MasterAdmin Protocol:</strong> client use it to do management for HMaster, such as region manually management, table meta info management.<br>➢<strong> Admin Protocol:</strong> client use it communicate with HRegionServer, to do admin work, such as region split, store file compact, WAL management.<br>➢<strong> Client Protocol:</strong> Client use it to read/write to HRegionServer<br>➢ <strong>RegionServerStatus Protocol</strong>: HRegionServer use it to communicate with HMaster: including the request and response of server startup, server fatal error, server status report.</p>
<h3 id="3-5-Log-Structured-Merge-Trees-LSM-trees"><a href="#3-5-Log-Structured-Merge-Trees-LSM-trees" class="headerlink" title="3.5 Log-Structured Merge-Trees(LSM-trees)"></a>3.5 Log-Structured Merge-Trees(LSM-trees)</h3><p>No-SQL database usually uses LSM-trees as data storage process architecture. HBase is no exception. As we all known, RDBMS adopts B+ tree to organize its indexes, as shown in Fugure 3.3. These B+ trees are often 3-level n-way balance trees. The nodes of a B+ tree are blocks on disk. So for a update by RDBMS, it likely needs 5 times disk operation. (3 times for B+ tree to find the block of the target row, 1 time for target block read, and 1 time for data update).<br>On RDBMS, data is written randomly as heap file on disk, but random data block decrease read performance. That’s why we need B+ tree index. B+ tree is fit well for data read, but is not efficient for data updates. Given the large distributed data, B+ tree is not the competitor for LSM-trees so far.<br><a href="http://tinypic.com?ref=7286l0" target="_blank" rel="external"><img src="http://i58.tinypic.com/7286l0.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure3.8 B+ tree</p>
<p>LSM-trees can be viewed as n-level merge-trees. It transforms random writes into sequential writes using logfile and in-memory store. Figure3.9 shows data write process of LSM-trees.<br><a href="http://tinypic.com?ref=2mw8nky" target="_blank" rel="external"><img src="http://i59.tinypic.com/2mw8nky.jpg" alt="Image and video hosting by TinyPic"></a><br>Figure 3.9 LSM-trees</p>
<p>➢ <strong>Data Write(Insert, update):</strong> Data is written to logfile sequentially first, then to in-memory store, where data is organized as sorted tree, like B+ tree. When the in-memory store is filled up, the tree in the memory will be flushed to a store file on disk. The store files on disk is arranged like B+ tree, as the C1 Tree shown in Figure 3.9 . But store files are optimized for sequential disk access.<br>➢<strong>Data Read: I</strong>n-memory store is searched first. Then search the store files on disk.<br>➢<strong>Data Delete:</strong> Give a data record a “delete marker”, system background will do housekeeping work by merging some store files into a larger one to reduce disk seeks. A data record will be deleted permanently during the housekeeping.<br>LSM-trees’ data updates are operated in memory, no disk access, it’s faster than B+ tree. When the data read is always on the data set that is written recently, LSM-trees will reduce disk seeks, and improve performance. When disk IO is the cost we must consider, LSM-trees is more suitable than B+ tree.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h2&gt;&lt;p&gt;Apache HBase is an open source column-oriented database. &lt;strong&gt;It is often described as a sparse, consistent, distributed, multi-dimensional sorted map.&lt;/strong&gt; HBase is modeled after Google’s “Bigtable: A distributed Storage System for Structured Data”, which can host very large tables with billions of rows, X millions of columns.&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.cyanny.com/categories/hadoop/"/>
    
      <category term="HBase" scheme="http://www.cyanny.com/categories/hadoop/hbase/"/>
    
    
      <category term="Hadoop" scheme="http://www.cyanny.com/tags/hadoop/"/>
    
      <category term="Big Data" scheme="http://www.cyanny.com/tags/big-data/"/>
    
      <category term="HBase" scheme="http://www.cyanny.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>Nodejs HBase0.96 Hadoop2.2.0 Thrift2配置与使用</title>
    <link href="http://www.cyanny.com/2014/02/27/nodejs-hbase-hadoop2-thrift2-e9-85-8d-e7-bd-ae-e4-b8-8e-e4-bd-bf-e7-94-a8/"/>
    <id>http://www.cyanny.com/2014/02/27/nodejs-hbase-hadoop2-thrift2-e9-85-8d-e7-bd-ae-e4-b8-8e-e4-bd-bf-e7-94-a8/</id>
    <published>2014-02-27T08:00:42.000Z</published>
    <updated>2016-09-27T01:03:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>项目如果没有采用Java开发，难道就不能用HBase了么？程序猿不会善罢甘休的，有什么语言就会有什么API存在，我还觉得用Java配置时各种缺包错误很烦呢，记得《数学之美》中曾说道：“做技术有术和道两个层面”，知道HBase的架构和一些底层细节是”道”，而使用各种配置和API开发应用则是”术”，而我们就来试试非Java连接HBase。<br>HBase的第三方接口有Shell, Java, REST和Thrift，可以参考《HBase in Action》chapter 6, REST接口比较慢，使用起来并没有Thrift好。而你可能疑惑什么是Thrift:<br><a id="more"></a><br><em><br>Thrift is a “software framework, for scalable cross-language services development…with a code generation engine to build services that work efficiently and seamlessly between C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js…” Thrift allows us to create a thin API in NodeJS to communicate with HBase using a thin socket protocol. The advantages over REST are that the connection stays alive and the protocol is thinner than XML.
</em><br>Apache Thrift是一个proxy，一个基于RPC的中间件，通过该接口，我们可以采用任何第三方语言连接HBase, 由此可见软件中间件封装底层detail的魅力。HBase提供C++, Java, Python, Node.js的Thrift文件。就Node.js来说，HBase提供了Thrift1和Thrift2， 官方的说法是Thrift2会逐步取代Thrift1， Thrift1我用了，发现命名不规范，而且CRUD并没有全部跑通，之后换用Thrift2，试验后成功了。<br>HBase Thrift API文档很少，官方只找到一个<a href="http://wiki.apache.org/hadoop/Hbase/ThriftApi" title="Python HBase API" target="_blank" rel="external">Python的老版本API</a>, 参考过的博客中采用Thrift2和Node.js的，Google一圈也没有，因此在这里把自己的一些小试验写下来，分享给朋友们，如有不对之处，还请指正。</p>
<h3 id="一、环境配置"><a href="#一、环境配置" class="headerlink" title="一、环境配置"></a>一、环境配置</h3><p>1. 系统要求<br>*nix系统，我是Max OX 10.9，因此在配置上也是以Mac的配置为准。</p>
<p>2. 安装Hadoop2.2.0和HBase 0.96.1，并下载HBase 0.96.1的源码到本地，并解压到本地<br>可以参考我之前的两个配置文章：<br><a href="http://cyanny/myblog/2014/02/06/set-hadoop-hbase-part1/" title="Set up Hadoop 2.2 and HBase 0.96 part1" target="_blank" rel="external">Set up Hadoop 2.2 and HBase 0.96 part1</a><br><a href="http://cyanny/myblog/2014/02/06/set-hadoop-hbase-part2/" title="Set up Hadoop 2.2 and HBase 0.96 part2" target="_blank" rel="external">Set up Hadoop 2.2 and HBase 0.96 part2</a></p>
<p>3. Mac需要安装brew, Ubuntu用apt-get就可以</p>
<p>4. 安装thrift<br>[shell]<br>$ brew install thrift #Maybe cost a lot of time<br>$ thrift -version<br>Thrift version 0.9.1<br>[/shell]</p>
<p>5. 下载 [Node-HBase-Thrift2]<br>这是我写的使用HBase Thrift2接口的Demo，可以使用该Demo进行测试。<br>[shell]<br>$ git clone <a href="https://github.com/lgrcyanny/Node-HBase-Thrift2" target="_blank" rel="external">https://github.com/lgrcyanny/Node-HBase-Thrift2</a><br>[/shell]</p>
<p>6. 生成HBase Thrift的相关文件<br>[shell]<br>$ cd Node-HBase-Thrift2<br>$ thrift –gen js:node path-to-hbasesrc/hbase-0.96.1-src/hbase-hrift/src/main/resources/org/apache/hadoop/hbase/thrift2/Hbase.thrift</p>
<p>#then you will get gen-nodejs directory<br>[/shell]<br>编译生成连接Thrift的js文件，需要使用HBase 0.96.1源码<br>BTW, 在你下载的Node-HBase-Thrift2包中，已经有生成的gen-nodejs文件夹，如果你在生成Thrift相关文件时出现任何问题，可以尝试使用这gen-nodejs文件夹下编译好的文件。</p>
<p>8. npm安装thrift包<br>[shell]<br>$ npm install thrift<br>[/shell]</p>
<p>9. 打开HDFS，HBase, HBase Thrift2接口<br>[shell]<br>$ start-dfs.sh<br>$ start-hbase.sh<br>$ hbase thrift2 start -f #Use framed transport， This transport is required when using a non-blocking server. It sends data in frames, where each frame is preceded by length information. Node.js is no-blocking server, we must use &quot;-f&quot; option, or connection lost.<br>$ jps<br>23835 HMaster<br>23764 HQuorumPeer<br>24109 ThriftServer<br>23556 SecondaryNameNode<br>24264 Jps<br>23384 NameNode<br>23927 HRegionServer<br>23463 DataNode<br>[/shell]</p>
<p>10.插入测试数据<br>[shell]<br>$ hbase shell<br>&gt; put ‘users’, ‘TheRealMT’, ‘info:email’, ‘mark@gmail.com’<br>&gt; put ‘users’, ‘TheRealMT’, ‘info:passoword’, ‘abc123’<br>&gt; put ‘users’, ‘TheRealMT’, ‘info:name’, ‘Mark Twain’<br>&gt; put ‘users’, ‘wwzyhao’, ‘info:count’, 1<br>&gt; put ‘users’, ‘wwzyhao’, ‘info:passowrd’, 7818271<br>&gt; put ‘users’, ‘wwzyhao’, ‘info:email’, 7818271<br>&gt; put ‘users’, ‘wwzyhao’, ‘info:gender’, ‘male’<br>&gt; scan ‘users’<br>ROW                                      COLUMN+CELL<br> TheRealMT                               column=info:email, timestamp=1392811829718, value=mark@gmail.com<br> TheRealMT                               column=info:name, timestamp=1392811829718, value=Mark Twain<br> TheRealMT                               column=info:password, timestamp=1392814983344, value=abc123<br> wwzyhao                                 column=info:count, timestamp=1393489157537, value=male<br> wwzyhao                                 column=info:email, timestamp=1393407097540, value=sks66782@gmail.com<br> wwzyhao                                 column=info:gender, timestamp=1393489039523, value=male<br> wwzyhao                                 column=info:password, timestamp=1393416922377, value=7612111<br>2 row(s) in 0.0560 seconds<br>[/shell]</p><p></p>
<h3 id="二、使用Thrift2"><a href="#二、使用Thrift2" class="headerlink" title="二、使用Thrift2"></a>二、使用Thrift2</h3><p>为什么不是Thrift1呢？确实，HBase 0.96.1中提供了两个编译文件thrift, thrift2, 前者我们称之为thrift1，我最初使用的是Thrift1，二者最大的痛处是没有文档，官方文档寥寥无几，有一种孤军深入的感觉。不过，API们都是相似的，语义相同，语言不同罢了，这时候代码就是文档。Thrift1的代码规范不如Thrift2，Thrift2更接近Java实现的API调用方式，在使用Thrift1后觉得不好用，就弃用之，投奔Thrift2了。<br>在完成了配置后，我们开始代码了：<br>HBase提供的CRUD操作是经典的：Put, Get, Delete, Scan和Increment</p>
<h4 id="1-HBase-Get"><a href="#1-HBase-Get" class="headerlink" title="1. HBase Get"></a>1. HBase Get</h4><p>[javascript]<br>var thrift = require(‘thrift’);<br>var HBase = require(‘./gen-nodejs/THBaseService’);<br>var HBaseTypes = require(‘./gen-nodejs/hbase_types’);</p>
<p>var connection = thrift.createConnection(‘localhost’, 9090, {<br>  transport: thrift.TFramedTransport,<br>  protocol: thrift.TBinaryProtocol<br>});</p>
<p>connection.on(‘connect’, function () {<br>  console.log(‘connected’);<br>  var client = thrift.createClient(HBase, connection);</p>
<p>  // row is rowid, columns is array of TColumn, please refer to hbase_types.js<br>  var tGet = new HBaseTypes.TGet({row: ‘TheRealMT’,<br>    columns: [new HBaseTypes.TColumn({family: ‘info’})]});<br>  client.get(‘users’, tGet, function (err, data) {<br>    if (err) {<br>      console.log(err);<br>    } else {<br>      console.log(data);<br>    }<br>    connection.end();<br>  });</p>
<p>});</p>
<p>connection.on(‘error’, function(err){<br>  console.log(‘error’, err);<br>});<br>[/javascript]</p>
<p>我在demo中分文件展示了这5种操作，因为每个操作是以event的形式绑定到connection上，如果将所有的操作都写到：<br>[javascript]<br>connection.on(‘connect’, function () {<br>  console.log(‘connected’);<br>  var client = thrift.createClient(HBase, connection);<br>  // …<br>});<br>[/javascript]<br>运行时connection.end()后，会出现”Write after end”的Error，我觉得分文件封装是更好的方式。</p>
<h4 id="2-HBase-Put"><a href="#2-HBase-Put" class="headerlink" title="2. HBase Put"></a>2. HBase Put</h4><p>这个操作是向HBase写入数据, 包括更新和插入。<br>[javascript]<br>connection.on(‘connect’, function () {<br>  console.log(‘connected’);<br>  var client = thrift.createClient(HBase, connection);</p>
<p>  var tPut = new HBaseTypes.TPut({row: ‘wwzyhao’,<br>    columnValues: [<br>    new HBaseTypes.TColumnValue({family: ‘info’, qualifier: ‘hobbies’, value: ‘music’}),<br>    new HBaseTypes.TColumnValue({family: ‘info’, qualifier: ‘name’, value: ‘Thomas Zhang’})<br>    ]});<br>  client.put(‘users’, tPut, function (err) {<br>    if (err) {<br>      console.log(err);<br>      return;<br>    }<br>    console.log(‘success’);<br>    connection.end();<br>  });<br>});<br>[/javascript]</p>
<h4 id="3-HBase-Delete"><a href="#3-HBase-Delete" class="headerlink" title="3. HBase Delete"></a>3. HBase Delete</h4><p>[javascript]<br>connection.on(‘connect’, function () {<br>  console.log(‘connected’);<br>  var client = thrift.createClient(HBase, connection);</p>
<p>  // Please run the command in shell first: put ‘users’, ‘wwzyhao’, ‘info:gender’, ‘male’<br>  var tDelete = new HBaseTypes.TDelete({row: ‘wwzyhao’,<br>   columns: [new HBaseTypes.TColumn({family: ‘info’, qualifier: ‘gender’})]});<br>  client.deleteSingle(‘users’, tDelete, function (err) {<br>    if (err) {<br>      console.log(err);<br>      return;<br>    } else {<br>      console.log(‘delete success’);<br>    }<br>    connection.end();<br>  });<br>});<br>[/javascript]</p>
<h4 id="4-HBase-Scan"><a href="#4-HBase-Scan" class="headerlink" title="4. HBase Scan"></a>4. HBase Scan</h4><p>Get操作本质上是基于Scan的，Get只能获取单行的数据，Scan可以获取多行的数据。<br>[javascript]<br>connection.on(‘connect’, function () {<br>  console.log(‘connected’);<br>  var client = thrift.createClient(HBase, connection);&lt;/p&gt;</p>
<p>  var tScanner = new HBaseTypes.TScan({startRow: ‘TheRealMT’,<br>    columns: [new HBaseTypes.TColumn({family: ‘info’})]});<br>  client.openScanner(‘users’, tScanner, function (err, scannerId) {<br>    if (err) {<br>      console.log(err);<br>      return;<br>    }<br>    console.log(‘scannerid : ‘ + scannerId);<br>    client.getScannerRows(scannerId, 10, function (serr, data) {<br>      if (serr) {<br>        console.log(serr);<br>        return;<br>      }<br>      console.log(data);<br>    });<br>    client.closeScanner(scannerId, function (err) {<br>      if (err) {<br>        console.log(err);<br>      }<br>    });<br>    connection.end();<br>  });<br>});<br>[/javascript]</p>
<h4 id="5-HBase-Increment"><a href="#5-HBase-Increment" class="headerlink" title="5. HBase Increment"></a>5. HBase Increment</h4><p>[javascript]<br>connection.on(‘connect’, function () {<br>  console.log(‘connected’);<br>  var client = thrift.createClient(HBase, connection);</p>
<p>  // put ‘users’, ‘wwzyhao’, ‘info’, ‘count’, 1<br>  var tIncrement = new HBaseTypes.TIncrement({<br>    row:’wwzyhao’,<br>    columns: [new HBaseTypes.TColumn({family: ‘info’, qualifier: ‘count’})]<br>  });<br>  client.increment(‘users’, tIncrement, function (err) {<br>    if (err) {<br>      console.log(err);<br>      return;<br>    }<br>    console.log(‘increment success.’);<br>    connection.end();<br>  });<br>});<br>[/javascript]</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>1.<a href="http://dailyjs.com/2013/07/04/hbase/" target="_blank" rel="external">Getting Started with HBase and Thrift for Node</a><br>2. HBase in Action<br>3. <a href="http://blog.evernote.com/tech/2012/12/20/building-apache-thrift-on-mac-os-x/" target="_blank" rel="external">building-apache-thrift-on-mac-os-x</a><br>4. <a href="https://wiki.apache.org/hadoop/Hbase/ThriftApi" title="Thrift API" target="_blank" rel="external">Thrift API</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;项目如果没有采用Java开发，难道就不能用HBase了么？程序猿不会善罢甘休的，有什么语言就会有什么API存在，我还觉得用Java配置时各种缺包错误很烦呢，记得《数学之美》中曾说道：“做技术有术和道两个层面”，知道HBase的架构和一些底层细节是”道”，而使用各种配置和API开发应用则是”术”，而我们就来试试非Java连接HBase。&lt;br&gt;HBase的第三方接口有Shell, Java, REST和Thrift，可以参考《HBase in Action》chapter 6, REST接口比较慢，使用起来并没有Thrift好。而你可能疑惑什么是Thrift:&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.cyanny.com/categories/hadoop/"/>
    
      <category term="HBase" scheme="http://www.cyanny.com/categories/hadoop/hbase/"/>
    
    
      <category term="Hadoop" scheme="http://www.cyanny.com/tags/hadoop/"/>
    
      <category term="HBase" scheme="http://www.cyanny.com/tags/hbase/"/>
    
      <category term="Learn" scheme="http://www.cyanny.com/tags/learn/"/>
    
  </entry>
  
</feed>
