<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>CyannyLive</title>
  
  <subtitle>AI and Big Data</subtitle>
  <link href="http://www.cyanny.com/atom.xml" rel="self"/>
  
  <link href="http://www.cyanny.com/"/>
  <updated>2020-12-27T14:22:40.700Z</updated>
  <id>http://www.cyanny.com/</id>
  
  <author>
    <name>Cyanny Liang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>An Introduction to Bayesian Networks</title>
    <link href="http://www.cyanny.com/2020/12/27/an-introduction-to-bayesian-networks/"/>
    <id>http://www.cyanny.com/2020/12/27/an-introduction-to-bayesian-networks/</id>
    <published>2020-12-27T10:54:04.000Z</published>
    <updated>2020-12-27T14:22:40.700Z</updated>
    
    <content type="html"><![CDATA[<p>冬日晴好, 下午看完了论文, 对Bayesian Network是什么有了系统的了解.论文是causalnex工具里提到的<br>Stephenson, Todd Andrew. An introduction to Bayesian network theory and usage. No. REP_WORK. IDIAP, 2000.</p><p>该论文主要论述了以下几点:</p><ul><li>What is Bayesian network</li><li>Inference Bayesian network: junction tree algorithm</li><li>Learning Bayesian Network</li><li>Applications<ul><li>Automatic Speech Recognition: Dynamic Bayesian Network</li><li>Computer troubleshooting</li><li>Medical diagnosis</li></ul></li></ul><a id="more"></a><h1 id="1-What-is-Bayesian-Network-BN"><a href="#1-What-is-Bayesian-Network-BN" class="headerlink" title="1.What is Bayesian Network(BN)"></a>1.What is Bayesian Network(BN)</h1><p>A directed acyclic graph(DAG) with probability distribution for each variable<br>这是一个交叉领域, 涉及概率论和图论, 主要可以应用于因果推断, 其优势是:</p><ul><li>可以引入专家经验</li><li>通过图结构化简联合概率分布求解</li></ul><p>下图是Hackerman解释BN时用的信用欺诈网络:<br><img src="https://wx4.sinaimg.cn/mw690/761b7938ly1gm2s0q7nz4j20mk0i4n0b.jpg" alt="bn01"></p><h1 id="2-Inference-Bayesian-Network"><a href="#2-Inference-Bayesian-Network" class="headerlink" title="2.Inference Bayesian Network"></a>2.Inference Bayesian Network</h1><p>一个示例如下, 对欺诈模型进行条件概率求解时, 可借助BN进行化简, 这是一个离散变量的例子.<br><img src="https://wx4.sinaimg.cn/mw690/761b7938ly1gm2s0u4vcdj21ek0njn4d.jpg" alt="bn02"></p><p>其他常见的推断方法包括<br><img src="https://wx3.sinaimg.cn/mw690/761b7938ly1gm2s0ye16fj21ch0kf0vd.jpg" alt="bn03"></p><p>作者在论文中, 重点讲述了Junction Tree Method. 该算法通过将图进行Moralize和Triangulate转化为Join Tree进行推断<br><img src="https://wx2.sinaimg.cn/mw690/761b7938ly1gm2s12i6w8j21dh0s6wm4.jpg" alt="bn04"></p><h1 id="3-Learning-Bayesian-Network"><a href="#3-Learning-Bayesian-Network" class="headerlink" title="3.Learning Bayesian Network"></a>3.Learning Bayesian Network</h1><p>需要关心如下四种场景<br><img src="https://wx4.sinaimg.cn/mw690/761b7938ly1gm2s163l7ej20yf0cujup.jpg" alt="bn05"></p><h1 id="4-Applications"><a href="#4-Applications" class="headerlink" title="4.Applications"></a>4.Applications</h1><p>一个实用案例, 在windows95中, 采用了BN进行printer的异常检测<br><img src="https://wx3.sinaimg.cn/mw690/761b7938ly1gm2s1ii2rmj20x90lc46s.jpg" alt="bn06"></p><p>BN在目前的机器学习中, 应该是计算复杂度高, 应用范围不像深度学习这么广, 而因果推理上, 样本量可以不用很大, 会有不错的应用效果.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;冬日晴好, 下午看完了论文, 对Bayesian Network是什么有了系统的了解.论文是causalnex工具里提到的&lt;br&gt;Stephenson, Todd Andrew. An introduction to Bayesian network theory and usage. No. REP_WORK. IDIAP, 2000.&lt;/p&gt;
&lt;p&gt;该论文主要论述了以下几点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is Bayesian network&lt;/li&gt;
&lt;li&gt;Inference Bayesian network: junction tree algorithm&lt;/li&gt;
&lt;li&gt;Learning Bayesian Network&lt;/li&gt;
&lt;li&gt;Applications&lt;ul&gt;
&lt;li&gt;Automatic Speech Recognition: Dynamic Bayesian Network&lt;/li&gt;
&lt;li&gt;Computer troubleshooting&lt;/li&gt;
&lt;li&gt;Medical diagnosis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="Causal Inference, AI" scheme="http://www.cyanny.com/tags/causal-inference-ai/"/>
    
  </entry>
  
  <entry>
    <title>Strucutre Learning Algorithm NOTEARS</title>
    <link href="http://www.cyanny.com/2020/12/25/strucutre-learning-algorithm-notears/"/>
    <id>http://www.cyanny.com/2020/12/25/strucutre-learning-algorithm-notears/</id>
    <published>2020-12-25T14:01:59.000Z</published>
    <updated>2020-12-27T14:05:59.103Z</updated>
    
    <content type="html"><![CDATA[<p>最近三年扎进了AI领域, 学了很多算法, 最近开始真正拉高维度看AI, AI不仅仅是Machine Learning, 还有State Based, Variable Bases, Logic编程等方法. 最近半年看了<strong>The book of Why</strong>, 深受启发, 看世界的角度也发生很大变化, 同时也觉得因果推理将是一个值得研究的好领域, 就算目前落地场景不多, 相信未来也是大有可为.</p><p>今天静下来, 好好看了在CausalNex库中, 用到的算法NOTEARS, 用于结构学习, 该论文发表在2018的NIPS, 方法神奇, 解决方案简洁, 以下是自己的一些笔记:</p><p>Paper: Zheng, Xun, et al. “DAGs with NO TEARS: Continuous optimization for structure learning.” Advances in Neural Information Processing Systems 31 (2018): 9472-9483.</p><a id="more"></a><h1 id="1-主要问题"><a href="#1-主要问题" class="headerlink" title="1.主要问题"></a>1.主要问题</h1><p>Bayesian Network Graph(DAG) Structure Estimating, 这是一个经典的NP-HARD问题</p><table><thead><tr><th align="left">维度</th><th align="left">传统解法</th><th align="left">NOTEARS</th></tr></thead><tbody><tr><td align="left">思路</td><td align="left">combinatorial optimization problem, local heuristics for enforcing the acyclicity constraint, 例如 Order search, greedy search, …</td><td align="left">Score based continuous learning, standard numerical algorithm</td></tr><tr><td align="left">复杂度</td><td align="left">O(d!), d is nodes</td><td align="left">O(d^3), 当图入度很高时, 计算效率很高</td></tr><tr><td align="left">优势</td><td align="left">–</td><td align="left">支持有向和无向图, 代码简洁不超过60lines, 与Global Optimization算法结果接近</td></tr><tr><td align="left">局限</td><td align="left">困难的组合优化问题</td><td align="left">建模函数是Smooth function, nonconvex</td></tr></tbody></table><h1 id="2-NOTEARS算法"><a href="#2-NOTEARS算法" class="headerlink" title="2.NOTEARS算法"></a>2.NOTEARS算法</h1><p>其英文缩写是 Non-combinatorial Optimization via Trace Exponential and Augmented lagRangian for Structure learning<br>该算法我个人理解主要的贡献是找到一种数学建模方法, 把DAG的学习问题转化为可以用拉格朗日乘数法可求最优解的问题, 其建模的问题定义如下:</p><p><img src="https://wx4.sinaimg.cn/mw690/761b7938ly1gm0h7k87gij21hl0kpdm0.jpg" alt="model def"></p><p>基于拉格朗日乘数法进行数值优化, 优势是:一个有n个变量与k个约束条件的最优化问题转换为一个解有n + k个变量的方程组的解的问题, 同时可复用一些优化算法: L-BFGS, quasi-Newton(PQN)</p><ul><li>例如: 求f(x, y)在g(x, y)=c约数下的最大值, 可以转化为求下面函数的极值的问题:</li><li>\(L(x, y, \gamma) = f(x, y) + \gamma * (g(x, y) - c)\)</li></ul><p>下图是NOTEARS抽象为拉格朗日乘数的公式</p><p><img src="https://wx4.sinaimg.cn/mw690/761b7938ly1gm0h7pxt34j21e6089abm.jpg" alt="lagrangian equation"></p><p>下图是NOTEARS的算法流程</p><p><img src="https://wx2.sinaimg.cn/mw690/761b7938ly1gm0h7tsn6lj20yu0bqgno.jpg" alt="notears algorithm"></p><h1 id="3-NOTEARS算法效果"><a href="#3-NOTEARS算法效果" class="headerlink" title="3.NOTEARS算法效果"></a>3.NOTEARS算法效果</h1><p>作者从ERS, SF4中生成了Node数为20, 样本量为1000和20的数据集, 图表示是邻接矩阵, 可看到学习后的效果和true graph很接近, 效果比Fast Greedy Search(FGS)要好, 同时和Global Optimizer的结果很接近, 准确性好</p><p><img src="https://wx4.sinaimg.cn/mw690/761b7938ly1gm0h7xhnenj21et0mwqcw.jpg" alt="notears effect"></p><p>继续奋战, 下一波是Bayesian Network ^–^</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近三年扎进了AI领域, 学了很多算法, 最近开始真正拉高维度看AI, AI不仅仅是Machine Learning, 还有State Based, Variable Bases, Logic编程等方法. 最近半年看了&lt;strong&gt;The book of Why&lt;/strong&gt;, 深受启发, 看世界的角度也发生很大变化, 同时也觉得因果推理将是一个值得研究的好领域, 就算目前落地场景不多, 相信未来也是大有可为.&lt;/p&gt;
&lt;p&gt;今天静下来, 好好看了在CausalNex库中, 用到的算法NOTEARS, 用于结构学习, 该论文发表在2018的NIPS, 方法神奇, 解决方案简洁, 以下是自己的一些笔记:&lt;/p&gt;
&lt;p&gt;Paper: Zheng, Xun, et al. “DAGs with NO TEARS: Continuous optimization for structure learning.” Advances in Neural Information Processing Systems 31 (2018): 9472-9483.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Causal Inference, AI" scheme="http://www.cyanny.com/tags/causal-inference-ai/"/>
    
  </entry>
  
  <entry>
    <title>Akka http notes</title>
    <link href="http://www.cyanny.com/2020/09/13/akka-http-notes/"/>
    <id>http://www.cyanny.com/2020/09/13/akka-http-notes/</id>
    <published>2020-09-13T13:47:31.000Z</published>
    <updated>2020-09-13T14:25:06.054Z</updated>
    
    <content type="html"><![CDATA[<p>在快3年多的Scala项目编程中, Akka是我见过的比较高质量的scala库, 其核心抽象是一种基于Actor的编程模型, 同时在这个核心抽象上, 提供一组工具库, 用户只需要按Actor形式写业务逻辑, 框架会帮你处理好底层的消息传递, 高并发和IO问题. Akka在工业场景下, 很接底气, 比如有很多微服务, 服务的性能各有差异, 这时候你需要整合这些微服务, 完成比如广告投放, 在线推荐, 事故检测等业务, Akka的业务抽象就会有很大的用处.</p><p>而最近系统看了Akka-HTTP, 我个人比较喜欢这个库在meta-programming方面的应用, akka-http把一个老生常谈的HTTP库实现的很优雅, 设计和抽象值得推敲, 时间有限, 就看了一周, 以下是一些最近对我帮助比较大的总结, 如果以后有空会继续完善</p><h2 id="1-Akka-HTTP-优势"><a href="#1-Akka-HTTP-优势" class="headerlink" title="1.Akka HTTP 优势"></a>1.Akka HTTP 优势</h2><p>定位: 用于处理复杂业务的Library, 不是一个MVC Framework(such as Play)</p><ul><li>DSL with convenient pathMatchers</li><li>Streaming: 流式传输, 速率限制</li><li>Interacting with actor easy</li></ul><a id="more"></a><h2 id="2-核心数据结构和抽象"><a href="#2-核心数据结构和抽象" class="headerlink" title="2.核心数据结构和抽象"></a>2.核心数据结构和抽象</h2><ul><li>HTTP Module<ul><li>HttpRequest<ul><li>a method (GET, POST, etc.)</li><li>a URI (see URI model for more information)</li><li>a seq of headers</li><li>an entity (body data): HttpEntity<ul><li>HttpEntity类型<ul><li>Strict: 消息体小, 内存可放, String or ByteString</li><li>Default: Streaming Data Source, know data size</li><li>Chunked: unknown length</li><li>Multipart.BodyPart: streaming entity, unknown length</li></ul></li><li>大小控制: max-content-length</li></ul></li><li>a protocol</li></ul></li><li>HttpResponse<ul><li>a status code</li><li>a Seq of headers</li><li>an entity (body data)</li><li>a protocol</li></ul></li><li>Headers<ul><li>content-type: 在HttpEntity中定义</li><li>Content-length</li><li>User-agent: 自动添加</li><li>Date: 自动添加</li></ul></li><li>Other supporting types and tools:<ul><li>Uri, HttpMethods, MediaTypes, StatusCodes, Attributes</li><li>Parsing/Rendering</li></ul></li></ul></li><li>URI Module<ul><li>model, handling special characters<ul><li>foo://example.com:8042/over/there?name=ferret#nose</li><li>scheme://authority/path/query?fragment</li></ul></li><li>Extract directives<ul><li>Uri.query()</li><li>PathDirectives</li></ul></li></ul></li><li>Marshalling/Unmarshalling<ul><li>Marshalling(Serialization/Pickling): Convert high level object to low level wire format<ul><li>Function: A =&gt; Future[List[Marshalling[B]]]<ul><li>Future: 支持异步</li><li>List: 序列化后的对象, 提供多种格式</li><li>Marshalling[B]: 可以访问MediaType, HttpCharset这些属性; 延迟Marshalling的调用, 执行content negotiation</li></ul></li><li>常见的Mashallers<ul><li>type ToEntityMarshaller[T] = Marshaller[T, MessageEntity]</li><li>type ToByteStringMarshaller[T] = Marshaller[T, ByteString]</li><li>type ToHeadersAndEntityMarshaller[T] = Marshaller[T, (immutable.Seq[HttpHeader], MessageEntity)]</li><li>type ToResponseMarshaller[T] = Marshaller[T, HttpResponse]</li><li>type ToRequestMarshaller[T] = Marshaller[T, HttpRequest]</li></ul></li><li>Support Implicit Resolution</li></ul></li><li>Unmarshalling(Deserialization)<ul><li>From low level MessageEntity to high level type T</li><li>Unmarshaller[A, Future[B]]</li><li>Support implicit resolution</li></ul></li><li>Sparay-Json support</li></ul></li></ul><h2 id="3-Server-API"><a href="#3-Server-API" class="headerlink" title="3.Server API"></a>3.Server API</h2><ul><li>High Level: Routing DSL, content negotiation, static content serving<ul><li><a href="https://doc.akka.io/docs/akka-http/current/routing-dsl/index.html">https://doc.akka.io/docs/akka-http/current/routing-dsl/index.html</a></li><li>设计思路:<ul><li>更可读, 可维护性更好, 可组合, 可扩展的API构建方式</li><li>以Directives构建Route Tree</li><li>Route Tree可以转换为底层的Flow</li></ul></li><li>Route的接口定义:<ul><li>type Route = RequestContext =&gt; Future[RouteResult]<ul><li>RequestContext: 提供比HttpRequest更丰富的内容</li><li>RouteResult: 是一个ADT(algebraic data type), 一般从RouteDirectives创建(complete, reject, redirect)</li></ul></li><li>Composing Route<ul><li>transformation: 代理给inner route处理</li><li>filtering: condition and reject</li><li>chaining: concat指令, 构建Route Tree, 不建议使用~符号</li></ul></li><li>Sealing a route<ul><li>可以返回response的route</li></ul></li></ul></li><li>Directives是构建route的核心<ul><li>创建route<ul><li>val route: Route = { ctx =&gt; ctx.complete(“yeah”) }</li><li>val route: Route = _.complete(“yeah”)</li><li>val route = complete(“yeah”)</li></ul></li><li>其他功能<ul><li>Transform/Filter/Extract incoming RequestContext</li><li>Chain RouteResult</li><li>Complete a request</li></ul></li></ul></li></ul></li></ul><h2 id="4-代码重要的模块"><a href="#4-代码重要的模块" class="headerlink" title="4.代码重要的模块"></a>4.代码重要的模块</h2><ul><li>Akka-http: High level functionality, dsl, marshalling/unmarshalling</li><li>Akka-http-core: low level http protocols</li><li>Akka-http-tools<ul><li>Akka-http-testkit</li><li>Akka-http2-support</li><li>Akka-http-spray-json</li><li>Akka-http-xml</li></ul></li></ul><h2 id="5-执行层"><a href="#5-执行层" class="headerlink" title="5.执行层"></a>5.执行层</h2><ul><li>基于akka stream</li><li>Timeout<ul><li>Client/Server: IdleTimeout: 链接如果一段时间没有request和response</li><li>Client:<ul><li>akka.http.client.connecting-timeout</li><li>akka.http.host-connection-pool.max-connection-lifetime: This timeout configures a maximum amount of time, while the connection can be kept open</li></ul></li><li>Server:<ul><li>akka.http.server.request-timeout 20s</li><li>akka.http.server.bind-timeout</li><li>Linger timeout: server implementation will keep a connection open after all data has been delivered to the network layer</li></ul></li></ul></li><li>Caching<ul><li>Based on caffeine(<a href="https://github.com/ben-manes/caffeine/">https://github.com/ben-manes/caffeine/</a>)</li><li>Design Idea:<ul><li>避免羊群效应</li><li>Cache future</li></ul></li><li>应用: Frequency-biased LFU cache</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;在快3年多的Scala项目编程中, Akka是我见过的比较高质量的scala库, 其核心抽象是一种基于Actor的编程模型, 同时在这个核心抽象上, 提供一组工具库, 用户只需要按Actor形式写业务逻辑, 框架会帮你处理好底层的消息传递, 高并发和IO问题. Akka在工业场景下, 很接底气, 比如有很多微服务, 服务的性能各有差异, 这时候你需要整合这些微服务, 完成比如广告投放, 在线推荐, 事故检测等业务, Akka的业务抽象就会有很大的用处.&lt;/p&gt;
&lt;p&gt;而最近系统看了Akka-HTTP, 我个人比较喜欢这个库在meta-programming方面的应用, akka-http把一个老生常谈的HTTP库实现的很优雅, 设计和抽象值得推敲, 时间有限, 就看了一周, 以下是一些最近对我帮助比较大的总结, 如果以后有空会继续完善&lt;/p&gt;
&lt;h2 id=&quot;1-Akka-HTTP-优势&quot;&gt;&lt;a href=&quot;#1-Akka-HTTP-优势&quot; class=&quot;headerlink&quot; title=&quot;1.Akka HTTP 优势&quot;&gt;&lt;/a&gt;1.Akka HTTP 优势&lt;/h2&gt;&lt;p&gt;定位: 用于处理复杂业务的Library, 不是一个MVC Framework(such as Play)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DSL with convenient pathMatchers&lt;/li&gt;
&lt;li&gt;Streaming: 流式传输, 速率限制&lt;/li&gt;
&lt;li&gt;Interacting with actor easy&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="Akka, Scala" scheme="http://www.cyanny.com/tags/akka-scala/"/>
    
  </entry>
  
  <entry>
    <title>Java Performance Toolbox</title>
    <link href="http://www.cyanny.com/2018/08/04/java-performance-notes-monitoring-tools/"/>
    <id>http://www.cyanny.com/2018/08/04/java-performance-notes-monitoring-tools/</id>
    <published>2018-08-04T10:24:34.000Z</published>
    <updated>2018-08-09T06:29:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>I learned <strong>The Java Performance Definitive Guide[chapter 3]</strong> on this weekend, here is a brief summary about Java Performance Toolbox.</p><h2 id="System-Monitoring-Tools"><a href="#System-Monitoring-Tools" class="headerlink" title="System Monitoring Tools"></a>System Monitoring Tools</h2><h3 id="1-CPU-Usage"><a href="#1-CPU-Usage" class="headerlink" title="1. CPU Usage"></a>1. CPU Usage</h3><p>vmstat: Report virtual memory statistics, vmstat reports information about processes, memory, paging, block IO, traps, disks and cpu activity<br>vmstat [options] [delay [count]]</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vmstat 1</span><br><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 0  0      0 71322688 314388 127644112    0    0     0     7    0    0  9  1 90  0  0</span><br><span class="line"> 0  0      0 71322816 314388 127644112    0    0     0     0 5590 7874  1  0 99  0  0</span><br><span class="line"> 0  0      0 71322592 314388 127644144    0    0     0     0 5418 7226  0  0 99  0  0</span><br><span class="line"> 0  0      0 71323208 314388 127644144    0    0     0    12 4952 7199  0  0 100  0  0</span><br><span class="line"> 0  0      0 71323600 314388 127644144    0    0     0   104 5253 7262  1  0 99  0  0</span><br></pre></td></tr></table></figure><p>Tips:</p><ul><li>CPU time is the first thing to examine when looking at performance of an application.</li><li>The goal in optimizing code is to drive the CPU usage up (for a shorter period of time), not down.</li><li>Understand why CPU usage is low before diving in and attempting to tune an application.</li></ul><h3 id="2-Disk-Usage"><a href="#2-Disk-Usage" class="headerlink" title="2. Disk Usage"></a>2. Disk Usage</h3><p>iostat: Report Central Processing Unit (CPU) statistics and input/output statistics for devices and partitions.</p><ul><li>%user: Show the percentage of CPU utilization that occurred while executing at the user level (application).</li><li>%system: Show the percentage of CPU utilization that occurred while executing at the system level (kernel).</li><li>rrqm/s: The number of read requests merged per second that were queued to the device</li><li>avgrq-sz: The average size (in sectors) of the requests that were issued to the device</li><li>%util: Percentage of elapsed time during which I/O requests were issued to the device (bandwidth utilization for the device). Device saturation occurs when this value is close to 100%<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iostat -xm 5</span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           9.40    0.00    0.52    0.01    0.00   90.07</span><br><span class="line"></span><br><span class="line">Device:         rrqm&#x2F;s   wrqm&#x2F;s     r&#x2F;s     w&#x2F;s    rMB&#x2F;s    wMB&#x2F;s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sda               0.00     0.58    0.15    4.49     0.00     0.14    61.65     1.31  283.55    0.22  292.90   0.06   0.03</span><br></pre></td></tr></table></figure><h3 id="3-Network-Usage"><a href="#3-Network-Usage" class="headerlink" title="3. Network Usage"></a>3. Network Usage</h3></li></ul><p>netstat: Print network connections, routing tables, interface statistics, masquerade connections, and multicast memberships</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -s</span><br></pre></td></tr></table></figure><p>The book use <code>nicstat</code>, which is not built-in Linux server, it can be installed by yum.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nicstat 5</span><br></pre></td></tr></table></figure><p>Be careful that the bandwidth is measured in bits per second, but tools generally report bytes per second</p><h2 id="Java-Monitoring-Tools"><a href="#Java-Monitoring-Tools" class="headerlink" title="Java Monitoring Tools"></a>Java Monitoring Tools</h2><h3 id="1-Basic-JVM-INFO"><a href="#1-Basic-JVM-INFO" class="headerlink" title="1. Basic JVM INFO"></a>1. Basic JVM INFO</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">jcmd process_id VM.uptime</span><br><span class="line">jcmd process_id VM.system_properties</span><br><span class="line">jcmd process_id VM.version</span><br><span class="line">jcmd process_id VM.command_line</span><br><span class="line">jinfo -sysprops process_id</span><br><span class="line">jinfo -flags process_id</span><br><span class="line">jinfo -flag PrintGCDetails process_id</span><br></pre></td></tr></table></figure><p>Some tuning flags can be set by jcmd and jinfo in command line, such as manageable options and C2 diagnostic (the flag provides diagnostic output for the compiler engineers to understand how the compiler is functioning).</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jinfo -flag -PrintGCDetails process_id  # turns off PrintGCDetails</span><br><span class="line">jinfo -flag PrintGCDetails process_id</span><br></pre></td></tr></table></figure><p>tips:</p><ul><li>jcmd can be used to find the basic VM information—include the value of all the tuning flags—for a running application.</li><li>Default flag values can be found by including -XX:+PrintFlagsFinal on a command line. This is useful for determining the default ergonomic settings of flags on a particular platform.</li><li>jinfo is useful for inspecting (and in some cases changing) individual flags.</li></ul><h3 id="2-Thread-Info"><a href="#2-Thread-Info" class="headerlink" title="2. Thread Info"></a>2. Thread Info</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jstack process_id</span><br><span class="line">jcmd process_id Thread.print</span><br></pre></td></tr></table></figure><h3 id="3-Class-Info"><a href="#3-Class-Info" class="headerlink" title="3. Class Info"></a>3. Class Info</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jconsole</span><br><span class="line">jstat -&lt;option&gt; [-t] [-h&lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;]]</span><br></pre></td></tr></table></figure><h3 id="4-Heap-Info"><a href="#4-Heap-Info" class="headerlink" title="4. Heap Info"></a>4. Heap Info</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jmap -heap process_id</span><br><span class="line">jmap -dump:[live,] format&#x3D;b, file&#x3D;filename process_id</span><br><span class="line">jhat -port 7000 dump_file</span><br></pre></td></tr></table></figure><p>Another way, add hprof option to java process</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">java -agentlib:hprof&#x3D;help</span><br><span class="line">HPROF: Heap and CPU Profiling Agent (JVMTI Demonstration Code)</span><br><span class="line">hprof usage: java -agentlib:hprof&#x3D;[help]|[&lt;option&gt;&#x3D;&lt;value&gt;, ...]</span><br><span class="line">Option Name and Value  Description                    Default</span><br><span class="line">---------------------  -----------                    -------</span><br><span class="line">heap&#x3D;dump|sites|all    heap profiling                 all</span><br><span class="line">cpu&#x3D;samples|times|old  CPU usage                      off</span><br><span class="line">monitor&#x3D;y|n            monitor contention             n</span><br><span class="line">format&#x3D;a|b             text(txt) or binary output     a</span><br><span class="line">file&#x3D;&lt;file&gt;            write data to file             java.hprof[&#123;.txt&#125;]</span><br><span class="line">net&#x3D;&lt;host&gt;:&lt;port&gt;      send data over a socket        off</span><br><span class="line">depth&#x3D;&lt;size&gt;           stack trace depth              4</span><br><span class="line">interval&#x3D;&lt;ms&gt;          sample interval in ms          10</span><br><span class="line">cutoff&#x3D;&lt;value&gt;         output cutoff point            0.0001</span><br><span class="line">lineno&#x3D;y|n             line number in traces?         y</span><br><span class="line">thread&#x3D;y|n             thread in traces?              n</span><br><span class="line">doe&#x3D;y|n                dump on exit?                  y</span><br><span class="line">msa&#x3D;y|n                Solaris micro state accounting n</span><br><span class="line">force&#x3D;y|n              force output to &lt;file&gt;         y</span><br><span class="line">verbose&#x3D;y|n            print messages about dumps     y</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-agentlib:hprof&#x3D;cpu&#x3D;samples,lineno&#x3D;y # for cpu</span><br><span class="line">-agentlib:hprof&#x3D;heap&#x3D;sites,lineno&#x3D;y # for heap</span><br></pre></td></tr></table></figure><p>Some notes:</p><ul><li>heap=sites, sites is a sorted list of allocation sites.  This identifies the most heavily allocated object types, and the TRACE at which those allocations occurred.</li><li>cpu=samples,  is a statistical profile of program execution.  The VM  periodically samples all running threads, and assigns a quantum to active TRACEs in those threads.</li><li>cpu=time, is a profile of program execution obtained by measuring the time spent in individual methods (excluding the time spent in callees), as well as by counting the number of times each method is called</li></ul><p><a href="https://docs.oracle.com/javase/8/docs/technotes/samples/hprof.html">hprof ref</a></p><h3 id="5-Heap-Dump-Processing"><a href="#5-Heap-Dump-Processing" class="headerlink" title="5. Heap Dump Processing"></a>5. Heap Dump Processing</h3><p>Heap dumps can be captured from the jvisualvm GUI, or from the command line using jcmd or jmap.<br>Or you can use Eclipse Memory Analzyer Tool.</p><h2 id="Java-Profiling-Tools"><a href="#Java-Profiling-Tools" class="headerlink" title="Java Profiling Tools"></a>Java Profiling Tools</h2><h3 id="1-Tools-types"><a href="#1-Tools-types" class="headerlink" title="1. Tools types"></a>1. Tools types</h3><ul><li><p>Sampling profilers<br>Sampling-based profilers are the most common profiler. There may be error in sampling profiler’s result. The way to minimize these errors is to profile over a longer period of time, and to reduce the time interval between samples.</p></li><li><p>Instrumented profilers<br>Instrumented profilers yield more information about an application, but can possibly have a greater effect on the application than a sampling profiler.<br>Instrumented profilers should be set up to instrument small sections of the code, a few classes or packages. That limits their impact on the application’s performance.</p></li></ul><h2 id="2-JMC-Java-Mission-Control"><a href="#2-JMC-Java-Mission-Control" class="headerlink" title="2. JMC(Java Mission Control)"></a>2. JMC(Java Mission Control)</h2><p>It’s a great profiling tool built-in JDK(jdk 7 or higher). On local machine, just type <code>jmc</code> command, the jmc UI will show.<br><img src="http://wx3.sinaimg.cn/mw690/761b7938ly1ftz2nfllbmj21kw0xz4j5.jpg" alt="jmc ui"></p><p>Then how to connect jmc to remote Linux server.</p><ul><li>Firstly, add jmx configurations to Linux java process<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:+UnlockCommercialFeatures -XX:+FlightRecorder -Dcom.sun.management.jmxremote&#x3D;true -Dcom.sun.management.jmxremote.port&#x3D;8091 -Dcom.sun.management.jmxremote.rmi.port&#x3D;8091 -Dcom.sun.management.jmxremote.authenticate&#x3D;false -Dcom.sun.management.jmxremote.ssl&#x3D;false</span><br></pre></td></tr></table></figure></li><li>Secondly, config local jmc connection<br>fill the server and port, click Finished. That’s all.<br><img src="http://wx3.sinaimg.cn/mw690/761b7938ly1ftz2nkhtqvj20sw0pcwis.jpg" alt="config local jmc connection"></li></ul><p><a href="https://docs.oracle.com/javacomponents/jmc-5-5/jmc-user-guide/toc.htm">jmc help guides</a></p><p><em>Use JMC the dump files:</em><br>firstly, add <code>-XX:+UnlockCommercialFeatures -XX:+FlightRecorder</code> to application<br>secondly, type these commands:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jcmd process_id JFR.start</span><br><span class="line">jcmd process_id JFR.dump filename&#x3D;path</span><br><span class="line">jcmd process_id JFR.stop</span><br></pre></td></tr></table></figure><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul><li>System monitoring tools: vmstat, iostat, netstat</li><li>Java built-in tools: jinfo, jcmd, jmap, jhat, jstat, jconsole, jvisualvm, jmc, jhprof</li><li>No perfert tools for everything, when do profiling work, use right tools right applications</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.amazon.com/Java-Performance-Definitive-Guide-Getting/dp/1449358454/ref=sr_1_1?ie=UTF8&qid=1533475568&sr=8-1&keywords=java+performance+definitive+guide">Java Performance: The Definitive Guide</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;I learned &lt;strong&gt;The Java Performance Definitive Guide[chapter 3]&lt;/strong&gt; on this weekend, here is a brief summary about Java Performance Toolbox.&lt;/p&gt;
&lt;h2 id=&quot;System-Monitoring-Tools&quot;&gt;&lt;a href=&quot;#System-Monitoring-Tools&quot; class=&quot;headerlink&quot; title=&quot;System Monitoring Tools&quot;&gt;&lt;/a&gt;System Monitoring Tools&lt;/h2&gt;&lt;h3 id=&quot;1-CPU-Usage&quot;&gt;&lt;a href=&quot;#1-CPU-Usage&quot; class=&quot;headerlink&quot; title=&quot;1. CPU Usage&quot;&gt;&lt;/a&gt;1. CPU Usage&lt;/h3&gt;&lt;p&gt;vmstat: Report virtual memory statistics, vmstat reports information about processes, memory, paging, block IO, traps, disks and cpu activity&lt;br&gt;vmstat [options] [delay [count]]&lt;/p&gt;</summary>
    
    
    
    
    <category term="java performance" scheme="http://www.cyanny.com/tags/java-performance/"/>
    
  </entry>
  
  <entry>
    <title>Big Data And ML Learning</title>
    <link href="http://www.cyanny.com/2018/07/16/big-data-expert/"/>
    <id>http://www.cyanny.com/2018/07/16/big-data-expert/</id>
    <published>2018-07-16T00:33:12.000Z</published>
    <updated>2018-08-05T13:23:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>随着工作的时间一天天过去，不禁会思考对未来的打算，工作的事情更多的是业务和效果，少有时间学习，自我的提升比起学校需要更多的self motivation. 一直都工作在大数据领域，现在虽然业务多些，方向也没有变化，还有了很多机器学习方面的实践。以下是我觉得自己很希望学习的书籍和要点:</p><a id="more"></a><h2 id="大数据"><a href="#大数据" class="headerlink" title="大数据"></a>大数据</h2><ul><li>HBase Definitive Guide</li><li>Learning Apache Flink 比起Spark Streaming确实完善很多</li><li>Streaming Systems</li><li>High Performance Spark</li><li>Hadoop Definitive Guide，很早的时候看过一遍</li><li>Distributed Computing: Principles, Algorithms, and Systems</li><li>Spark源码</li><li>Spring Framework，很成熟服务端方案</li></ul><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><ul><li>Hands on ML</li><li>Tensorflow官方Guide和源码</li><li>DL Learning Coursera</li><li>统计推断</li><li>数学之美</li><li>Coursera ML 课程的作业</li><li>Machine Learning: A Probabilistic Perspective</li><li>Deep Learning的书</li><li>西瓜书</li><li>统计学习方法</li><li>The elements of statistical learning</li><li>Rainforcement Learning</li></ul><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><ul><li>JVM虚拟机深入: Java Performance: The definitive guide</li><li>The C++ Programming Language 4th edition</li><li>Java并发编程</li><li>Designing Data Intensive Applications</li><li>Go programming</li><li>R programming</li><li>Mondern Operating System</li><li>算法，图论, 设计模式，代码大全, 计算机程序的构造与解释</li><li>Programming in Scala(review again)</li><li>High performance Python</li><li>Getting Starting with R</li><li>The implementation of functional language</li><li>Distributed Computing</li><li>Gradle, Maven</li><li>代码整洁，代码大全，程序员自我修养，重够</li></ul><h2 id="投资管理等"><a href="#投资管理等" class="headerlink" title="投资管理等"></a>投资管理等</h2><ul><li>经济学原理</li><li>心理学</li><li>产品：梁宁，听了她的课，学了很多概念，同时也推荐李善友的颠覆式创新</li><li>don’t make me think</li><li>Just for Fun: The Story of an Accidental Revolutionary</li><li>The Hacker Ethic: and the Spirit of the Information Age</li></ul><p>希望在未来2<del>3年年，能完成，今天年底，完成3</del>4个要点，今年上半年已经完成了<em>Hands on ML</em>，<em>Learning Apache Flink</em> 最近在学习Java Performance，Tensorflow和Deep Learning的Coursera课程，加油啦~</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;随着工作的时间一天天过去，不禁会思考对未来的打算，工作的事情更多的是业务和效果，少有时间学习，自我的提升比起学校需要更多的self motivation. 一直都工作在大数据领域，现在虽然业务多些，方向也没有变化，还有了很多机器学习方面的实践。以下是我觉得自己很希望学习的书籍和要点:&lt;/p&gt;</summary>
    
    
    
    
    <category term="learning" scheme="http://www.cyanny.com/tags/learning/"/>
    
  </entry>
  
  <entry>
    <title>Awesome Books for 2018</title>
    <link href="http://www.cyanny.com/2018/01/20/great-books-for-2018/"/>
    <id>http://www.cyanny.com/2018/01/20/great-books-for-2018/</id>
    <published>2018-01-20T07:15:09.000Z</published>
    <updated>2018-01-20T07:52:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>One of my 2018 reservations is reading more books. Here I list some great books in my plan.</p><h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><ul><li>Machine Learning: A Probabilistic Perspective</li><li>Deap Learning(Ian，Goodfellow)</li><li>Pattern Recognition and Machine Learning(Christopher M Bishop)</li><li>The elements of statistic learning</li><li>Hands-On Machine Learning with Scikit-Learn and TensorFlow (in progress now)</li><li>Python Machine Learning</li><li>数学之美</li><li>统计学（复习）</li><li>统计学习方法</li><li>机器学习</li></ul><a id="more"></a><h2 id="Big-Data"><a href="#Big-Data" class="headerlink" title="Big Data"></a>Big Data</h2><ul><li>High Performance Spark(review again)</li><li>Spark The Definitive Guide</li><li>Kafka The Definitive Guide</li><li>Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing</li><li>streaming data understanding the real time pipeline</li><li>Learning Spark Streaming</li><li>Learning Apache Flink</li><li>Stream processing with apache flink</li><li>Architecting HBase Applications</li><li>HBase Definitive Guide</li><li>Designing Data-Intensive Applications</li></ul><h2 id="Programming"><a href="#Programming" class="headerlink" title="Programming"></a>Programming</h2><ul><li>Programming in Scala(review again)</li><li>The C++ Programming Language 4th edition</li><li>High performance Python</li><li>Getting Starting with R</li></ul><h2 id="For-my-field"><a href="#For-my-field" class="headerlink" title="For my field"></a>For my field</h2><ul><li>Gis Fundamentals</li><li>Computing with Spatial Trajectories</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;One of my 2018 reservations is reading more books. Here I list some great books in my plan.&lt;/p&gt;
&lt;h2 id=&quot;Machine-Learning&quot;&gt;&lt;a href=&quot;#Machine-Learning&quot; class=&quot;headerlink&quot; title=&quot;Machine Learning&quot;&gt;&lt;/a&gt;Machine Learning&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Machine Learning: A Probabilistic Perspective&lt;/li&gt;
&lt;li&gt;Deap Learning(Ian，Goodfellow)&lt;/li&gt;
&lt;li&gt;Pattern Recognition and Machine Learning(Christopher M Bishop)&lt;/li&gt;
&lt;li&gt;The elements of statistic learning&lt;/li&gt;
&lt;li&gt;Hands-On Machine Learning with Scikit-Learn and TensorFlow (in progress now)&lt;/li&gt;
&lt;li&gt;Python Machine Learning&lt;/li&gt;
&lt;li&gt;数学之美&lt;/li&gt;
&lt;li&gt;统计学（复习）&lt;/li&gt;
&lt;li&gt;统计学习方法&lt;/li&gt;
&lt;li&gt;机器学习&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="learning" scheme="http://www.cyanny.com/tags/learning/"/>
    
  </entry>
  
  <entry>
    <title>ppmml publish today</title>
    <link href="http://www.cyanny.com/2017/12/29/ppmml-publish/"/>
    <id>http://www.cyanny.com/2017/12/29/ppmml-publish/</id>
    <published>2017-12-29T11:42:02.000Z</published>
    <updated>2017-12-29T12:16:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>On the last day before the New Year Holiday, ppmml is published.<br><a href="https://github.com/lgrcyanny/ppmml">ppmml</a> is a python library for converting machine learning models to pmml file. ppmml wraps jpmml libraries and provides clean interface.</p><h1 id="What-is-pmml-file"><a href="#What-is-pmml-file" class="headerlink" title="What is pmml file?"></a>What is pmml file?</h1><p>PMML - “Predictive Model Markup Language”, which is a standard for XML documents which express trained instances of analytic models.<br>Various platforms adopt pmml as machine learning model standard, including IBM, SAS, Microsoft, Spark, KNIME etd.<a href="http://dmg.org/pmml/products.html">pmml-platforms</a></p><p><a href="https://github.com/jpmml">jpmml</a> has developed pmml model library and supported models of spark, xgboost, tensorflow, sklearn, lightgbm and R. All of these libraries are separated and written in java.<br>ppmml wraps jpmml libraries and proved a simple and easy-to-use API for pmml files transformation.<br>0.0.1 version supports sklearn, tensorflow, spark, lightgbm, xgboost and R models. All models supported by jpmml are supported by ppmml. Common machine learning algorithms are supported, such as Decision Tree, Logistic Regression, GBDT, Random Forest, KMeans. However, Deep Learning support is not ready.</p><a id="more"></a><h1 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --default-timeout=10000 -i https://pypi.anaconda.org/lgrcyanny/simple ppmml</span><br></pre></td></tr></table></figure><p><a href="https://anaconda.org/lgrcyanny/ppmml">ppmml conda package</a></p><h1 id="Geting-Started"><a href="#Geting-Started" class="headerlink" title="Geting Started"></a>Geting Started</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">import</span> ppmml</span><br><span class="line"><span class="comment"># load data and train iris datasets</span></span><br><span class="line">(X, y) = load_iris(<span class="literal">True</span>)</span><br><span class="line">lr = LogisticRegression(tol=<span class="number">1e-5</span>)</span><br><span class="line">lr.fit(X, y)</span><br><span class="line">joblib.dump(lr, <span class="string">&quot;lr.pkl.z&quot;</span>, compress = <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># to pmml file</span></span><br><span class="line">ppmml.to_pmml(<span class="string">&quot;lr.pkl.z&quot;</span>, <span class="string">&quot;lr.pmml&quot;</span>, model_type=<span class="string">&#x27;sklearn&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># prepare test data</span></span><br><span class="line">df = pd.DataFrame(X)</span><br><span class="line">df.columns = [<span class="string">&#x27;x1&#x27;</span>, <span class="string">&#x27;x2&#x27;</span>, <span class="string">&#x27;x3&#x27;</span>, <span class="string">&#x27;x4&#x27;</span>]</span><br><span class="line">df.to_csv(<span class="string">&#x27;test.csv&#x27;</span>, header=<span class="literal">True</span>, index=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># predit with pmml file, a simple predict API based on jpmml-evaluator</span></span><br><span class="line">ppmml.predict(<span class="string">&#x27;lr.pmml&#x27;</span>, <span class="string">&#x27;test.csv&#x27;</span>, <span class="string">&#x27;predict.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><p><a href="https://github.com/lgrcyanny/ppmml">ppmml github</a></p><hr><p><strong>Notes:</strong><br>It’s the last work of this year<br>In memory of the last time here, wish for a great step in 2018<br>May the force be with you</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;On the last day before the New Year Holiday, ppmml is published.&lt;br&gt;&lt;a href=&quot;https://github.com/lgrcyanny/ppmml&quot;&gt;ppmml&lt;/a&gt; is a python library for converting machine learning models to pmml file. ppmml wraps jpmml libraries and provides clean interface.&lt;/p&gt;
&lt;h1 id=&quot;What-is-pmml-file&quot;&gt;&lt;a href=&quot;#What-is-pmml-file&quot; class=&quot;headerlink&quot; title=&quot;What is pmml file?&quot;&gt;&lt;/a&gt;What is pmml file?&lt;/h1&gt;&lt;p&gt;PMML - “Predictive Model Markup Language”, which is a standard for XML documents which express trained instances of analytic models.&lt;br&gt;Various platforms adopt pmml as machine learning model standard, including IBM, SAS, Microsoft, Spark, KNIME etd.&lt;a href=&quot;http://dmg.org/pmml/products.html&quot;&gt;pmml-platforms&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/jpmml&quot;&gt;jpmml&lt;/a&gt; has developed pmml model library and supported models of spark, xgboost, tensorflow, sklearn, lightgbm and R. All of these libraries are separated and written in java.&lt;br&gt;ppmml wraps jpmml libraries and proved a simple and easy-to-use API for pmml files transformation.&lt;br&gt;0.0.1 version supports sklearn, tensorflow, spark, lightgbm, xgboost and R models. All models supported by jpmml are supported by ppmml. Common machine learning algorithms are supported, such as Decision Tree, Logistic Regression, GBDT, Random Forest, KMeans. However, Deep Learning support is not ready.&lt;/p&gt;</summary>
    
    
    
    
    <category term="machine learning" scheme="http://www.cyanny.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>How to Use Scala UDF and UDAF in PySpark</title>
    <link href="http://www.cyanny.com/2017/09/15/spark-use-scala-udf-udaf-in-pyspark/"/>
    <id>http://www.cyanny.com/2017/09/15/spark-use-scala-udf-udaf-in-pyspark/</id>
    <published>2017-09-15T03:16:33.000Z</published>
    <updated>2017-09-17T05:23:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>Spark DataFrame API provides efficient and easy-to-use operations to do analysis on distributed collection of data. Many users love the Pyspark API, which is more usable than scala API. Sometimes when we use UDF in pyspark, the performance will be a problem. How about implementing these UDF in scala, and call them in pyspark? BTW, in spark 2.0, UDAF can only be defined in scala, and how to use it in pyspark? Let’s have a try~</p><a id="more"></a><h2 id="Use-Scala-UDF-in-PySpark"><a href="#Use-Scala-UDF-in-PySpark" class="headerlink" title="Use Scala UDF in PySpark"></a>Use Scala UDF in PySpark</h2><p><strong>1. define scala udf</strong></p><p>Suppose we want to calculate string length, lets define it in scala UDF.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StringLength</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getStringLength</span></span>(s: <span class="type">String</span>) = s.length</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getFun</span></span>(): <span class="type">UserDefinedFunction</span> = udf(getStringLength _)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>2. use udf in python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> Column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_java_column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_seq</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col</span><br><span class="line"> </span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;scala_udf_test&quot;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_length</span>(<span class="params">col</span>):</span></span><br><span class="line">    _string_length = sc._jvm.com.learning.StringLength.getFun()</span><br><span class="line">    <span class="keyword">return</span> Column(_string_length.apply(_to_seq(sc, [col], _to_java_column)))</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span>():</span></span><br><span class="line">    rows = [</span><br><span class="line">        (<span class="string">&quot;k1&quot;</span>, <span class="string">&quot;aaa&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k2&quot;</span>, <span class="string">&quot;dd&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k3&quot;</span>, <span class="string">&quot;cc&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k4&quot;</span>, <span class="string">&quot;eee&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">    df = spark.createDataFrame(rows, [<span class="string">&#x27;key&#x27;</span>, <span class="string">&#x27;value&#x27;</span>])</span><br><span class="line">    df.show(<span class="number">50</span>)</span><br><span class="line">    df.select(col(<span class="string">&quot;key&quot;</span>), string_length(col(<span class="string">&quot;value&quot;</span>))).show()</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    process()</span><br></pre></td></tr></table></figure><p><strong>3. submit the app</strong></p><p>compile the scala code and submit python files with –jars</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --jars testing/learning<span class="number">-1.0</span><span class="number">.0</span>-<span class="type">SNAPSHOT</span>.jar udf_test.py</span><br></pre></td></tr></table></figure><p>the output would be:</p><table><thead><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>k1</td><td>3</td></tr><tr><td>k2</td><td>2</td></tr><tr><td>k3</td><td>2</td></tr><tr><td>k4</td><td>3</td></tr></tbody></table><p><strong>4. performance analysis</strong></p><p>let’s explain the scala UDF in Python<br><img src="http://wx1.sinaimg.cn/mw690/761b7938ly1fjmeol1jg8j20s405odim.jpg" alt="scala udf physical plan"><br>the Project Plan is Scala UDF</p><p>and if we implement Python UDF as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">py_slen = udf(<span class="keyword">lambda</span> s: <span class="built_in">len</span>(s), IntegerType())</span><br><span class="line">df_with_python_udf = (df.select(col(<span class="string">&quot;key&quot;</span>), py_slen(<span class="string">&quot;value&quot;</span>).alias(<span class="string">&quot;slen&quot;</span>)).orderBy(col(<span class="string">&quot;slen&quot;</span>).desc()))</span><br></pre></td></tr></table></figure><p>the Python plan is:<br><img src="http://wx4.sinaimg.cn/mw690/761b7938ly1fjmeofgzbmj210k06igot.jpg" alt="python udf physical plan"><br>the UDF plan is different, which is BatchEvalPython.<br>It can prove that when use scala UDF in python, the evaluation is in JVM and data will not exchange with Python worker. And the performance should be improved.</p><p>I evaluated the performance in local environment with 4cores and 2GB memory, and generated 10million rows for each test, the result is as follows:<br><img src="http://wx2.sinaimg.cn/mw690/761b7938ly1fjmer43xfnj20kw0ckt8z.jpg" alt="scala vs python string len udf"><br><strong>Scala UDF is 1.89 times Python UDF</strong></p><p><strong>And then I implemented another UDF in Scala and Python with regex string parsing</strong>, the performance is<br><img src="http://wx3.sinaimg.cn/mw690/761b7938ly1fjmeopknh4j20a0061748.jpg" alt="scala vs python string regex parsing"></p><p><strong>Scala udf is 2.23 times Python REGEX String Parsing UDF</strong></p><p>the Scala UDF is defined as follows:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by lgrcyanny on 17/9/13.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StringParse</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">STRING_PATTERN</span> = <span class="string">&quot;&quot;</span><span class="string">&quot;(a.*b)&quot;</span><span class="string">&quot;&quot;</span>.r</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parseString</span></span>(str: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> matched = <span class="type">STRING_PATTERN</span>.findFirstMatchIn(str)</span><br><span class="line">    <span class="keyword">if</span> (matched.isEmpty) &#123;</span><br><span class="line">      <span class="string">&quot;&quot;</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      matched.get.group(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getFun</span></span>() = udf(parseString _ )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Python string parse UDF  vs Scala UDF:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> Column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_java_column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_seq</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> length</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_word</span>(<span class="params">length</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;get random word for generate rows&quot;&quot;&quot;</span></span><br><span class="line">    letters = string.ascii_lowercase</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join([random.choice(letters) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length)])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_rows</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;generate rows in key value pair&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># generate rows</span></span><br><span class="line">    letters = <span class="string">&quot;abcdefghijklmnopqrstuvwxyz&quot;</span></span><br><span class="line">    rows = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="built_in">id</span> = random.randint(<span class="number">0</span>, <span class="number">100</span>)</span><br><span class="line">        slen = random.randint(<span class="number">0</span>, <span class="number">20</span>)</span><br><span class="line">        word = random_word(slen)</span><br><span class="line">        rows.append((<span class="built_in">id</span>, letters))</span><br><span class="line">    <span class="keyword">return</span> rows</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_parse</span>(<span class="params">col</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;scala udf parse string&quot;&quot;&quot;</span></span><br><span class="line">    _string_parse = sc._jvm.com.learning.StringParse.getFun()</span><br><span class="line">    <span class="keyword">return</span> Column(_string_parse.apply(_to_seq(sc, [col], _to_java_column)))</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_regex_udf</span>(<span class="params">n=<span class="number">1000</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;test udf with regex parse&quot;&quot;&quot;</span></span><br><span class="line">    rows = generate_rows(n)</span><br><span class="line">    df = spark.createDataFrame(rows, [<span class="string">&#x27;key&#x27;</span>, <span class="string">&#x27;value&#x27;</span>])</span><br><span class="line">    df.show(<span class="number">20</span>)</span><br><span class="line">    pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(a.*b)&quot;</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_string</span>(<span class="params"><span class="built_in">str</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;parse string with python regex&quot;&quot;&quot;</span></span><br><span class="line">        matched = re.search(pattern, <span class="built_in">str</span>)</span><br><span class="line">        <span class="keyword">if</span> matched:</span><br><span class="line">            <span class="keyword">return</span> matched.group(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    py_parse_str = udf(parse_string, StringType())</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    df_with_python_udf = (df.select(col(<span class="string">&quot;key&quot;</span>), py_parse_str(col(<span class="string">&quot;value&quot;</span>)).alias(<span class="string">&quot;parsed_value&quot;</span>))</span><br><span class="line">                          .<span class="built_in">filter</span>(length(col(<span class="string">&quot;parsed_value&quot;</span>)) &gt; <span class="number">0</span>))</span><br><span class="line">    df_with_python_udf.explain(<span class="literal">True</span>)</span><br><span class="line">    df_with_python_udf.show()</span><br><span class="line">    print(<span class="string">&quot;matched rows: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(df_with_python_udf.count()))</span><br><span class="line">    print(<span class="string">&quot;duration for python regex parse: &#123;&#125;s&quot;</span>.<span class="built_in">format</span>(time.time() - start_time))</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    df_with_scala_udf = (df.select(col(<span class="string">&quot;key&quot;</span>), string_parse(col(<span class="string">&quot;value&quot;</span>)).alias(<span class="string">&quot;parsed_value&quot;</span>))</span><br><span class="line">                          .<span class="built_in">filter</span>(length(col(<span class="string">&quot;parsed_value&quot;</span>)) &gt; <span class="number">0</span>))</span><br><span class="line">    df_with_python_udf.explain(<span class="literal">True</span>)</span><br><span class="line">    df_with_scala_udf.show()</span><br><span class="line">    print(<span class="string">&quot;matched rows: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(df_with_scala_udf.count()))</span><br><span class="line">    print(<span class="string">&quot;duration for scala regex parse: &#123;&#125;s&quot;</span>.<span class="built_in">format</span>(time.time() - start_time))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>5. Conclusion</strong></p><p>Databricks used to give a performance for Python vs Scala DataFrame and RDD API:<br><img src="http://wx2.sinaimg.cn/mw690/761b7938ly1fjmeo7vk38j210c0fy48j.jpg" alt="databricks performance"></p><p>the blog is <a href="https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html">here</a>.<br>The performance is a running group-aggregation on 10 million integer pairs on a single machince. The Scala DF is almost 5 times Python lambda function in RDD Python.</p><p>Even though, the Scala UDF is not 5 times Python UDF, about 2 times in my test, using scala UDF can improve performance indeed.</p><h2 id="Use-Scala-UDAF-in-PySpark"><a href="#Use-Scala-UDAF-in-PySpark" class="headerlink" title="Use Scala UDAF in PySpark"></a>Use Scala UDAF in PySpark</h2><p>UDAF now only supports defined in Scala and Java(spark 2.0)</p><p><strong>1. define scala UDAF</strong></p><p>when define UDAF, it must extend class <code>UserDefinedAggregateFunction</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">ArrayType</span>, <span class="type">DataType</span>, <span class="type">StringType</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GroupConcat</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;s&quot;</span>, <span class="type">StringType</span>)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;buff&quot;</span>, <span class="type">ArrayType</span>(<span class="type">StringType</span>))</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">StringType</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer.update(<span class="number">0</span>, <span class="type">ArrayBuffer</span>.empty[<span class="type">String</span>])</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer.update(<span class="number">0</span>, buffer.getSeq[<span class="type">String</span>](<span class="number">0</span>) :+ input.getString(<span class="number">0</span>))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1.update(<span class="number">0</span>, buffer1.getSeq[<span class="type">String</span>](<span class="number">0</span>) ++ buffer2.getSeq[<span class="type">String</span>](<span class="number">0</span>))</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getSeq[<span class="type">String</span>](<span class="number">0</span>).mkString(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>2. use UDAF in python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> Column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_java_column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_seq</span><br><span class="line"> </span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;scala_udf_test&quot;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_concat</span>(<span class="params">col</span>):</span></span><br><span class="line">    _groupConcat = sc._jvm.com.learning.GroupConcat.apply</span><br><span class="line">    <span class="keyword">return</span> Column(_groupConcat(_to_seq(sc, [col], _to_java_column)))</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span>():</span></span><br><span class="line">    rows = [</span><br><span class="line">        (<span class="string">&quot;k1&quot;</span>, <span class="string">&quot;a&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k1&quot;</span>, <span class="string">&quot;b&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k1&quot;</span>, <span class="string">&quot;c&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k2&quot;</span>, <span class="string">&quot;d&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k3&quot;</span>, <span class="string">&quot;e&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k3&quot;</span>, <span class="string">&quot;f&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">    df = spark.createDataFrame(rows, [<span class="string">&#x27;key&#x27;</span>, <span class="string">&#x27;value&#x27;</span>])</span><br><span class="line">    df.show(<span class="number">50</span>)</span><br><span class="line">    df.groupBy(<span class="string">&quot;key&quot;</span>).agg(group_concat(<span class="string">&quot;value&quot;</span>).alias(<span class="string">&quot;concat&quot;</span>)).show()</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    process()</span><br></pre></td></tr></table></figure><p><strong>3. submit the app</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;spark-submit --jars testing&#x2F;learning-1.0.0-SNAPSHOT.jar udf_test.py</span><br></pre></td></tr></table></figure><p>the output would be:</p><table><thead><tr><th>key</th><th>cancat</th></tr></thead><tbody><tr><td>k1</td><td>a,b,c</td></tr><tr><td>k2</td><td>d</td></tr><tr><td>k3</td><td>e,f</td></tr></tbody></table><p><strong>4. references</strong></p><ul><li><a href="https://stackoverflow.com/questions/31640729/spark-sql-replacement-for-mysql-group-concat-aggregate-function">spark-sql-replacement-for-mysql-group-concat-aggregate-function</a></li><li><a href="https://stackoverflow.com/questions/33233737/spark-how-to-map-python-with-scala-or-java-user-defined-functions">spark-how-to-map-python-with-scala-or-java-user-defined-functions</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;Spark DataFrame API provides efficient and easy-to-use operations to do analysis on distributed collection of data. Many users love the Pyspark API, which is more usable than scala API. Sometimes when we use UDF in pyspark, the performance will be a problem. How about implementing these UDF in scala, and call them in pyspark? BTW, in spark 2.0, UDAF can only be defined in scala, and how to use it in pyspark? Let’s have a try~&lt;/p&gt;</summary>
    
    
    
    
    <category term="spark" scheme="http://www.cyanny.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>My First Commit to Spark Community</title>
    <link href="http://www.cyanny.com/2017/08/30/my-first-commit-to-spark-community/"/>
    <id>http://www.cyanny.com/2017/08/30/my-first-commit-to-spark-community/</id>
    <published>2017-08-30T01:50:37.000Z</published>
    <updated>2017-08-30T07:24:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>I have been worked on spark related projects for almost 2 years. Today I submit a small patch to spark community. Hope to be a contributor~<br><a href="https://issues.apache.org/jira/browse/SPARK-21859">https://issues.apache.org/jira/browse/SPARK-21859</a></p><a id="more"></a><p>It’s a PR about SparkFiles.get problem on yarn. Event though it’s a small patch, it’s a big progress for me to be an open source contributor.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;I have been worked on spark related projects for almost 2 years. Today I submit a small patch to spark community. Hope to be a contributor~&lt;br&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-21859&quot;&gt;https://issues.apache.org/jira/browse/SPARK-21859&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="Spark" scheme="http://www.cyanny.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark streaming exactly-once analysis</title>
    <link href="http://www.cyanny.com/2017/08/27/spark-streaming-exactly-once-analysis/"/>
    <id>http://www.cyanny.com/2017/08/27/spark-streaming-exactly-once-analysis/</id>
    <published>2017-08-27T03:31:58.000Z</published>
    <updated>2017-10-24T07:27:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近对Spark Streaming接触比较多，主要关注的是streaming的准确性方面的需求, 忙了快半年，不禁想问为什么需要在exactly-once上花费这么多时间呢。streaming和batch的处理逻辑有什么区别呢？我觉得streaming更适合一些简单的过滤，能在100ms以内能算完的逻辑，而这些逻辑用batch也可以算完，为什么要streaming呢？用户们更希望的是更快。如果batch也能满足低延迟的需求，streaming系统就不需要了。而问题是为什么我们需要一个单独的streaming系统？</p><a id="more"></a><p> 生产环境中的版本是1.6，spark streaming的API在1.6上是基于RDD的DStream API，相比Structured Streaming，更稳定和成熟些。而我们的用户们，比较关心的是streaming系统</p><p> 开源里广泛使用的Streaming系统是Storm和Flink。Storm早期用record ack的方式保证at-least once，但没有提供exactly-once的保证，后面又有了storm trident.</p><h2 id="Spark-Streaming-Receiver模式没有exactly-once保证"><a href="#Spark-Streaming-Receiver模式没有exactly-once保证" class="headerlink" title="Spark Streaming Receiver模式没有exactly-once保证"></a>Spark Streaming Receiver模式没有exactly-once保证</h2><h2 id="Flink中的Exactly-Once保证"><a href="#Flink中的Exactly-Once保证" class="headerlink" title="Flink中的Exactly-Once保证"></a>Flink中的Exactly-Once保证</h2><h2 id="Storm的Eactly-Once保证"><a href="#Storm的Eactly-Once保证" class="headerlink" title="Storm的Eactly-Once保证"></a>Storm的Eactly-Once保证</h2><h3 id="Storm-Architecture"><a href="#Storm-Architecture" class="headerlink" title="Storm Architecture"></a>Storm Architecture</h3><p><img src="http://wx3.sinaimg.cn/mw690/761b7938ly1fizgwunfr6j21kw0n7e6n.jpg" alt="storm architecture"><br>以Storm on Yarn来说明Storm的架构：</p><ul><li>client将jar包通过yarn上传</li><li>在一台NodeManager上启动Nimbus，这是master节点，负责管理StormTopology, 分发task，心跳等</li><li>其他的NodeManager上启动Supervisor, 相当于slave节点，管理storm worker<ul><li> 在每个supervisor上，可以启动多个worker进程，每个worker进程可以运行多个task，task是多线程的，由worker管理。这些task运行的就是Spout或Bolt定义的操作</li></ul></li><li> Zookeeper, Storm运行时状态的管理</li></ul><p>Storm方面算是简单调研，理解不是很深入，具体的参考<a href="http://storm.apache.org/releases/1.1.1/Setting-up-a-Storm-cluster.html">官方文档</a></p><h3 id="Storm-exactly-once"><a href="#Storm-exactly-once" class="headerlink" title="Storm exactly-once"></a>Storm exactly-once</h3><p>####1. Storm Transactional Topologies(deprecated)<br>Strom 0.7版本中，实现了<a href="http://storm.apache.org/releases/1.1.1/Transactional-topologies.html">transctional topologies</a>来保证exactly-once,<br><strong>1.transactional phrases 类似两阶段事务机制</strong></p><ul><li>The processing phase: this is the phase that can be done in parallel for many batches 可以并发执行计算partial result</li><li>The commit phase: The commit phases for batches are strongly ordered. So the commit for batch 2 is not done until the commit for batch 1 has been successful. 保证batch的提交是按顺序的</li></ul><p>来个直观的，用户需要构建的Topology如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MemoryTransactionalSpout spout = <span class="keyword">new</span> MemoryTransactionalSpout(DATA, <span class="keyword">new</span> Fields(<span class="string">&quot;word&quot;</span>), PARTITION_TAKE_PER_BATCH);</span><br><span class="line">TransactionalTopologyBuilder builder = <span class="keyword">new</span> TransactionalTopologyBuilder(<span class="string">&quot;global-count&quot;</span>, <span class="string">&quot;spout&quot;</span>, spout, <span class="number">3</span>);</span><br><span class="line">builder.setBolt(<span class="string">&quot;partial-count&quot;</span>, <span class="keyword">new</span> BatchCount(), <span class="number">5</span>)</span><br><span class="line">        .shuffleGrouping(<span class="string">&quot;spout&quot;</span>);</span><br><span class="line">builder.setBolt(<span class="string">&quot;sum&quot;</span>, <span class="keyword">new</span> UpdateGlobalCount())</span><br><span class="line">        .globalGrouping(<span class="string">&quot;partial-count&quot;</span>);</span><br></pre></td></tr></table></figure><p><strong>2.关键点</strong></p><ul><li>一个拓扑里只有一个Transactional Spout，其实现是由一个单线程的Coordinator Spout + 多个Emitter Bolt组成。利用Storm的ACK Framework机制，判断一个batch是否执行完成</li><li>Committer Bolt 可以有多个，需要收到Transactional Spout的commit信息才会执行commit</li></ul><p>####2. Storm Trident Topologies<br>storm 1.1的版本中，引入新的Trident API解决exactly-once，这是transactional topologies的升级版本。API的易用性改善，exactly-once也是采用事务机制</p><p><strong>1.exactly once</strong></p><ul><li>Tuples are processed as small batches 采用micor-batch机制</li><li>Each batch of tuples is given a unique id called the “transaction id” (txid). If the batch is replayed, it is given the exact same txid. 每个batch有唯一的batchid</li><li>State updates are ordered among batches. That is, the state updates for batch 3 won’t be applied until the state updates for batch 2 have succeeded. 按txid顺序提交</li></ul><p><strong>2.trident example</strong><br><a href="https://github.com/lgrcyanny/LearningStorm/blob/master/src/main/scala/com/learning/storm/TridentTest.scala">github example</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> topology = <span class="keyword">new</span> <span class="type">TridentTopology</span>()</span><br><span class="line"><span class="comment">// define spout</span></span><br><span class="line"><span class="keyword">val</span> spout = <span class="keyword">new</span> <span class="type">FixedBatchSpout</span>(<span class="keyword">new</span> <span class="type">Fields</span>(<span class="string">&quot;sentence&quot;</span>), <span class="number">3</span>,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Values</span>(<span class="string">&quot;the cow jumped over the moon&quot;</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Values</span>(<span class="string">&quot;the man went to the store and bought some candy&quot;</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Values</span>(<span class="string">&quot;four score and seven years ago&quot;</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Values</span>(<span class="string">&quot;how many apples can you eat&quot;</span>))</span><br><span class="line">  spout.setCycle(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordsCount: <span class="type">TridentState</span> = topology.newStream(<span class="string">&quot;wordsSpout&quot;</span>, spout)</span><br><span class="line">    .each(<span class="keyword">new</span> <span class="type">Fields</span>(<span class="string">&quot;sentence&quot;</span>), <span class="keyword">new</span> <span class="type">Split</span>(), <span class="keyword">new</span> <span class="type">Fields</span>(<span class="string">&quot;word&quot;</span>))</span><br><span class="line">    .groupBy(<span class="keyword">new</span> <span class="type">Fields</span>(<span class="string">&quot;word&quot;</span>))</span><br><span class="line">    .persistentAggregate(<span class="keyword">new</span> <span class="type">MemoryMapState</span>.<span class="type">Factory</span>(), <span class="keyword">new</span> <span class="type">Count</span>(), <span class="keyword">new</span> <span class="type">Fields</span>(<span class="string">&quot;count&quot;</span>))</span><br><span class="line">    .parallelismHint(<span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>###<strong>总结</strong></p><p>Storm里为了exacly once，需要做到：</p><ul><li>源端可重放</li><li>batch要有唯一的txid</li><li>commit时按顺序提交，类似事务的两阶段提交</li></ul><hr/>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近对Spark Streaming接触比较多，主要关注的是streaming的准确性方面的需求, 忙了快半年，不禁想问为什么需要在exactly-once上花费这么多时间呢。streaming和batch的处理逻辑有什么区别呢？我觉得streaming更适合一些简单的过滤，能在100ms以内能算完的逻辑，而这些逻辑用batch也可以算完，为什么要streaming呢？用户们更希望的是更快。如果batch也能满足低延迟的需求，streaming系统就不需要了。而问题是为什么我们需要一个单独的streaming系统？&lt;/p&gt;</summary>
    
    
    
    
    <category term="spark streaming" scheme="http://www.cyanny.com/tags/spark-streaming/"/>
    
  </entry>
  
  <entry>
    <title>Set Up Apache Storm On Mac In 10min</title>
    <link href="http://www.cyanny.com/2017/04/10/set-up-storm-on-mac-in-10min/"/>
    <id>http://www.cyanny.com/2017/04/10/set-up-storm-on-mac-in-10min/</id>
    <published>2017-04-10T12:19:23.000Z</published>
    <updated>2017-04-10T12:49:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>Storm is a great real time streaming system. Recently, my project is about spark streaming. I want to learn storm either to know more about streaming system. Okay, let’s fire up.<br>Today I tried to install storm cluster on my local mac.<br>It was easy to install. It will cost you about 10min.</p><a id="more"></a><h2 id="1-install-zookeeper"><a href="#1-install-zookeeper" class="headerlink" title="1. install zookeeper"></a>1. install zookeeper</h2><ul><li>download <a href="http://www.apache.org/dyn/closer.cgi/zookeeper/">zookeeper-3.4.9</a></li><li>configure conf/zoo.cfg as follows:</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The number of milliseconds of each tick</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of ticks that the initial</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> synchronization phase can take</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of ticks that can pass between</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sending a request and getting an acknowledgement</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="meta">#</span><span class="bash"> the directory <span class="built_in">where</span> the snapshot is stored.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">do</span> not use /tmp <span class="keyword">for</span> storage, /tmp here is just</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> replace it as your <span class="built_in">local</span> dir</span></span><br><span class="line">dataDir=/Users/lgrcyanny/Codelab/zookeeper/zookeeper-3.4.9/zkdata</span><br><span class="line"><span class="meta">#</span><span class="bash"> the port at <span class="built_in">which</span> the clients will connect</span></span><br><span class="line">clientPort=2181</span><br></pre></td></tr></table></figure><h2 id="2-install-storm"><a href="#2-install-storm" class="headerlink" title="2. install storm"></a>2. install storm</h2><ul><li>download <a href="http://storm.apache.org/downloads.html">latest storm 1.1.0</a></li><li>configure conf/storm.yaml as follows:</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">storm.zookeeper.servers:</span><br><span class="line">    - &quot;localhost&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">storm.zookeeper.port:2181</span></span><br><span class="line"></span><br><span class="line">storm.local.dir: &quot;/Users/lgrcyanny/Codelab/storm/apache-storm-1.1.0/storm-local&quot;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="bash"><span class="comment"># nimbus.seeds: [&quot;host1&quot;, &quot;host2&quot;, &quot;host3&quot;]</span></span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="bash">nimbus.seeds: [<span class="string">&quot;localhost&quot;</span>]</span></span><br><span class="line"></span><br><span class="line">supervisor.slots.ports:</span><br><span class="line">    - 6700</span><br><span class="line">    - 6701</span><br><span class="line">    - 6702</span><br><span class="line">    - 6703</span><br></pre></td></tr></table></figure><p>to understand these config, please refer to: <a href="http://storm.apache.org/releases/1.1.0/Setting-up-a-Storm-cluster.html">Setting-up-a-Storm-cluster.html</a></p><h2 id="3-start-stom"><a href="#3-start-stom" class="headerlink" title="3. start stom"></a>3. start stom</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> start nimbus</span></span><br><span class="line">./bin/storm nimbus</span><br><span class="line"><span class="meta">#</span><span class="bash"> start supervisor <span class="keyword">for</span> workers</span></span><br><span class="line">./bin/storm supervisor</span><br><span class="line"><span class="meta">#</span><span class="bash"> start ui</span></span><br><span class="line">./bin/storm ui</span><br></pre></td></tr></table></figure><p>open <a href="http://localhost:8080/">http://localhost:8080</a>, you will see storm started:<br><img src="http://wx1.sinaimg.cn/large/761b7938ly1fehv2miunpj21kw0ps0we.jpg" alt="start storm"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Storm is a great real time streaming system. Recently, my project is about spark streaming. I want to learn storm either to know more about streaming system. Okay, let’s fire up.&lt;br&gt;Today I tried to install storm cluster on my local mac.&lt;br&gt;It was easy to install. It will cost you about 10min.&lt;/p&gt;</summary>
    
    
    
    
    <category term="apache storm" scheme="http://www.cyanny.com/tags/apache-storm/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Logistic Regression</title>
    <link href="http://www.cyanny.com/2017/03/25/machine-learning-logistic-regression/"/>
    <id>http://www.cyanny.com/2017/03/25/machine-learning-logistic-regression/</id>
    <published>2017-03-25T13:48:51.000Z</published>
    <updated>2017-03-25T07:32:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>Logistic Regression is for classification problem, and the predication value is fixed descrete values, such as 1 for positive or 0 for negative. The essence of logistic regression is:</p><ul><li>hypothesis function is sigmoid function</li><li>cost function: J(theta)</li><li>gradient descent and algorithms</li><li>advantanced optimization with regularization to solve overfitting problem.<a id="more"></a><h2 id="Basics-about-logistic-regression"><a href="#Basics-about-logistic-regression" class="headerlink" title="Basics about logistic regression"></a>Basics about logistic regression</h2>hypothesis function = 1 / (1 + exp(-htheta(x))),<br>where htheta(x) = theta’ * x(theta’ is transpose theta)<br><img src="http://ww2.sinaimg.cn/mw690/761b7938jw1f2rxxio8x0j20v80nit9x.jpg" alt="Sigmoid Function or Logistic Function"><br>htheta(x) mean <strong>Probalitiy that y=1, given x parameterized by theta P(y=1 | x; theta)</strong>,<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> htheta(x) &gt;= <span class="number">0.5</span>, then y = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> htheta(x) &lt; <span class="number">0.5</span>, then y = <span class="number">0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Descision-Boundary"><a href="#Descision-Boundary" class="headerlink" title="Descision Boundary"></a>Descision Boundary</h2><img src="http://ww3.sinaimg.cn/mw690/761b7938jw1f2rxxhyf4ij20v00ngtbs.jpg" alt="descision boundary"><br>Our goal is the calculate theta, can classify our traing data with descision boundary.<br>In the example, the traning data can be classified into 2 categories by a straight line.<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (theta&#x27;x) &gt;= <span class="number">0</span>, then htheta(x) &gt;= <span class="number">0.5</span>, then y = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> (theta&#x27;x) &lt; <span class="number">0</span>, then htheta(x) &lt; <span class="number">0.5</span>, then y = <span class="number">0</span></span><br></pre></td></tr></table></figure><h2 id="Cost-function-implementation"><a href="#Cost-function-implementation" class="headerlink" title="Cost function implementation"></a>Cost function implementation</h2>For the assignment of week3, predicate the adimission by university with 2 exams grade data.<br>I optimize the implementation with vectoriaztion</li></ul><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J, grad]</span> = <span class="title">costFunction</span><span class="params">(theta, X, y)</span></span></span><br><span class="line"><span class="comment">%COSTFUNCTION Compute cost and gradient for logistic regression</span></span><br><span class="line"><span class="comment">%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the</span></span><br><span class="line"><span class="comment">%   parameter for logistic regression and the gradient of the cost</span></span><br><span class="line"><span class="comment">%   w.r.t. to the parameters.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize some useful values</span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% You need to return the following variables correctly</span></span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(theta));</span><br><span class="line"></span><br><span class="line"><span class="comment">% ====================== YOUR CODE HERE ======================</span></span><br><span class="line"><span class="comment">% Instructions: Compute the cost of a particular choice of theta.</span></span><br><span class="line"><span class="comment">%               You should set J to the cost.</span></span><br><span class="line"><span class="comment">%               Compute the partial derivatives and set grad to the partial</span></span><br><span class="line"><span class="comment">%               derivatives of the cost w.r.t. each parameter in theta</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Note: grad should have the same dimensions as theta</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Predications: h_theta(x)</span></span><br><span class="line">predications = sigmoid(X * theta);</span><br><span class="line">cost_items = y .* <span class="built_in">log</span>(predications) + (<span class="number">1</span> - y) .* <span class="built_in">log</span>(<span class="number">1</span> - predications);</span><br><span class="line">J = (<span class="number">-1</span> / m) * sum(cost_items);</span><br><span class="line"></span><br><span class="line">grad = (<span class="number">1</span> / m) * (X&#x27; * (hypothesis - y));</span><br><span class="line"></span><br><span class="line"><span class="comment">% =============================================================</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="Cost-function-with-regularization"><a href="#Cost-function-with-regularization" class="headerlink" title="Cost function with regularization"></a>Cost function with regularization</h2><p>Regularzation is for overfitting problem.</p><ul><li>underfit: not fit the training data, with high bias between predications and actual value</li><li>Just Right: great fit</li><li>Overfitting:  often with too many features, not so much traning data, fit traing data well, but with hight variance, predict new data not very well</li></ul><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J, grad]</span> = <span class="title">costFunctionReg</span><span class="params">(theta, X, y, lambda)</span></span></span><br><span class="line"><span class="comment">%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization</span></span><br><span class="line"><span class="comment">%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using</span></span><br><span class="line"><span class="comment">%   theta as the parameter for regularized logistic regression and the</span></span><br><span class="line"><span class="comment">%   gradient of the cost w.r.t. to the parameters.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize some useful values</span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% You need to return the following variables correctly</span></span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(theta));</span><br><span class="line"></span><br><span class="line"><span class="comment">% ====================== YOUR CODE HERE ======================</span></span><br><span class="line"><span class="comment">% Instructions: Compute the cost of a particular choice of theta.</span></span><br><span class="line"><span class="comment">%               You should set J to the cost.</span></span><br><span class="line"><span class="comment">%               Compute the partial derivatives and set grad to the partial</span></span><br><span class="line"><span class="comment">%               derivatives of the cost w.r.t. each parameter in theta</span></span><br><span class="line">hypothesis = sigmoid(X * theta);</span><br><span class="line">cost_items = (y .* <span class="built_in">log</span>(hypothesis)) + (<span class="number">1</span> - y) .* <span class="built_in">log</span>(<span class="number">1</span> - hypothesis);</span><br><span class="line"><span class="comment">% don&#x27;t penalize theta0</span></span><br><span class="line">reg_theta = [<span class="number">0</span>; theta(<span class="number">2</span>:<span class="built_in">length</span>(theta))];</span><br><span class="line">J = (<span class="number">-1</span> / m) * sum(cost_items) + (lambda / (<span class="number">2</span> * m)) * sum(reg_theta .^ <span class="number">2</span>);</span><br><span class="line"><span class="comment">%grad = (1 / m) * sum((predications - y) .* X)&#x27; + (lambda / m) * penalize_theta;</span></span><br><span class="line">grad = (<span class="number">1</span> / m) * (X&#x27; * (hypothesis - y)) + (lambda / m) * reg_theta;</span><br><span class="line"></span><br><span class="line"><span class="comment">% =============================================================</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>the lambda for regularization can’t be too large:</p><ul><li>large lamba will got very small theta value, and underfit.</li><li>small lambda will got large theta velue, and overfit.</li><li>the lambda for the exerise is 1</li></ul><h2 id="Github-assignments"><a href="#Github-assignments" class="headerlink" title="Github assignments"></a>Github assignments</h2><p><a href="https://github.com/lgrcyanny/MachineLearningCoursera/tree/master/assignments/ex2-logistic-regression">Week 3 Assignments</a></p><h2 id="Write-on-the-last"><a href="#Write-on-the-last" class="headerlink" title="Write on the last"></a>Write on the last</h2><p>After one year, I learn the logistic regression again. Last week, Andrew NG left Baidu. Maybe, these great people thought Baidu is not worth to fight for. Now I still decidated on a Spark project and focus on Spark Streaming. As team leader, I am bearing a great burden and is stressful. It’s a great chance to train my leadership. I am also wondering next opportunity. Learning Machine Learning is right and worth to do. Anyway, even though mist is on the path, just go forward and fight~</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Logistic Regression is for classification problem, and the predication value is fixed descrete values, such as 1 for positive or 0 for negative. The essence of logistic regression is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hypothesis function is sigmoid function&lt;/li&gt;
&lt;li&gt;cost function: J(theta)&lt;/li&gt;
&lt;li&gt;gradient descent and algorithms&lt;/li&gt;
&lt;li&gt;advantanced optimization with regularization to solve overfitting problem.</summary>
    
    
    
    
    <category term="Machine Learning" scheme="http://www.cyanny.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>binary search algorithm in scala</title>
    <link href="http://www.cyanny.com/2017/02/21/binary-search-algorithm/"/>
    <id>http://www.cyanny.com/2017/02/21/binary-search-algorithm/</id>
    <published>2017-02-21T01:29:49.000Z</published>
    <updated>2017-08-28T07:30:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>One day, I wanted to use binary search in one of my feature in my project. My friend said the algorithm was not easy to implement bug free. I did’t believe that. I spent 10min to write it.</p><a id="more"></a><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span></span>(list: <span class="type">Array</span>[<span class="type">Int</span>], start: <span class="type">Int</span>, end: <span class="type">Int</span>, x: <span class="type">Int</span>): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">   <span class="keyword">if</span> (start &lt;= end) &#123;</span><br><span class="line">     <span class="keyword">val</span> middle = (end - start) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> (list(middle) == x) &#123;</span><br><span class="line">       <span class="type">Some</span>(middle)</span><br><span class="line">     &#125; <span class="keyword">else</span> <span class="keyword">if</span> (list(middle) &gt; x) &#123;</span><br><span class="line">       search(list, middle + <span class="number">1</span>, end, x)</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       search(list, start, middle - <span class="number">1</span>, x)</span><br><span class="line">     &#125;</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="type">None</span></span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> list = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line">   println(search(list, <span class="number">0</span>, list.size - <span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>Ooh, definitly my code has bug, yes I admitted that it was not very easy to implement binary search bug free.<br>I revised it.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span></span>(list: <span class="type">Array</span>[<span class="type">Int</span>], start: <span class="type">Int</span>, end: <span class="type">Int</span>, x: <span class="type">Int</span>): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (start &lt;= end) &#123;</span><br><span class="line">      <span class="keyword">val</span> middle = (end - start) / <span class="number">2</span> + start <span class="comment">// bug 1, without plus start</span></span><br><span class="line">      <span class="keyword">if</span> (list(middle) == x) &#123;</span><br><span class="line">        <span class="type">Some</span>(middle)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (list(middle) &gt; x) &#123;   <span class="comment">// bug2, when middle bigger than x, not search middle+1,end</span></span><br><span class="line">        search(list, start, middle - <span class="number">1</span>, x)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        search(list, middle + <span class="number">1</span>, end, x)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> list = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line">    println(search(list, <span class="number">0</span>, list.size - <span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">    println(search(list, <span class="number">0</span>, list.size - <span class="number">1</span>, <span class="number">11</span>))</span><br><span class="line">    println(search(list, <span class="number">0</span>, list.size - <span class="number">1</span>, <span class="number">8</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">output:</span><br><span class="line">Some(4)</span><br><span class="line">None</span><br><span class="line">Some(7)</span><br></pre></td></tr></table></figure><p>It was an interesting problem. I should tain my programming skills more.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;One day, I wanted to use binary search in one of my feature in my project. My friend said the algorithm was not easy to implement bug free. I did’t believe that. I spent 10min to write it.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Algorithm" scheme="http://www.cyanny.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>My Booklist and Reservations for 2017</title>
    <link href="http://www.cyanny.com/2017/01/22/booklist-for-2017/"/>
    <id>http://www.cyanny.com/2017/01/22/booklist-for-2017/</id>
    <published>2017-01-22T02:18:09.000Z</published>
    <updated>2017-03-05T02:29:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>一直没有写关于2016的回顾，有很多方面吧。2016年发生很多事儿的一年，对于技术上的发展也有了新的思考，搞技术不再是死磕某种工具、算法或bug，其实本质上是为了解决问题或者做更好的产品。虽然我做的不是具体的产品而是底层的工具和平台，但这些工具的出口也是依赖”pillar application”, 多想想也是好处的。</p><p>2016工作忙，读的书没有很多，但想想扎克伯克比我们还忙一年能挑战23本书确实很牛，其实自己的时间管理是不太到位的，大部分周末都懒散睡觉或者出去逛街了，回归2016年，读的书们：<a id="more"></a></p><ul><li><strong>“Programming in scala, 2nd”</strong></li></ul><p>第一次啃一本大部头英文书883页，我都没有勇气打印出来，看的电子版，目前2017年初，走到第703页，成功在望。看完这个书，对scala的理解和应用，已经超过我的C++和Java技能了</p><ul><li><strong>Spark源码解析, 许鹏著</strong></li></ul><p>如果你问我学spark该看哪本书，我不会给你推荐这个的。不是说它不好，而是应该好好看官方doc. 不过这个书对我理解spark还是带来了很大帮助，但是spark1.0系列的，只能对着spark1.6和spark2.0的源码看，然后看看当年的大神们怎么设计的。许鹏的书里贴了一些关键代码，虽然贴代码占页数有点不厚道，但书叫源码解析所以也就忍了</p><ul><li><strong>Python Tutorial</strong></li></ul><p>我想学python很久了，这个Tutorial我看了好多遍，其实真正到实际应用中，我才真正学会了python</p><ul><li><strong>深入理解Java虚拟机</strong></li></ul><p>JVM的虚拟机调优部分看了，还有很多没看完，用到再查</p><ul><li><strong>spark的各种论文，PPT等</strong></li></ul><p>论文比较分散，有spark sql, spark rdd, spark streaming等，各种PPT也比较分散</p><ul><li><strong>人间词话</strong></li></ul><p>有时候看看诗，心里很开心。比如年终遇到一些事儿的时候喜欢一句诗：”天长水阔知何处”，心里那个纠结</p><ul><li><strong>Dunes: 沙丘，英文版</strong></li></ul><p>科幻大神的书，挑战了英文文学，虽然磕磕绊绊看完了，但现在我可以自信的看完大部头的英文原版文学书啦。之前看过英文的哈利波特，安德游戏，觉得比较幼稚没看下去。沙丘是比较对我胃口的一本</p><ul><li><strong>倚天屠龙记</strong></li></ul><p>我童年在教科书和教辅中度过，这么好的书这么大了才看，真心停不下来</p><ul><li><strong>百万富翁，Mark Twain, 英文版</strong></li></ul><p>喜欢反转的剧情，喜欢Mark Twain的文笔</p><ul><li><strong>各种博客，news</strong></li></ul><p>我手机里有将近30个news app，我是多爱看news，喜欢的包括Business Insider, Hack News, QZ.com, 36氪，钛媒体，推酷等，一般好的news分析我放到pocket里。当然还喜欢听喜马拉雅的段子。再觉得其实看news没有看书好，多看书好</p><p>2017年啦，上面还有programming in scala, 人间词话还没看完，新的书又来啦，虽然写打算我不一定看，但列一下我的打算吧：</p><ul><li>机器学习Coursera课程</li></ul><p>机器学习，AI这么火，我2017年要把这个课程学完</p><ul><li>Creativity, Inc. Overcoming the Unseen Forces That Stand in the Way of True Inspiration</li><li>从0到1</li><li>make users awesome</li><li>platform revolution</li><li>奇点系列：<a href="https://book.douban.com/series/27014">https://book.douban.com/series/27014</a></li><li>各种科幻和悬疑发现中</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;一直没有写关于2016的回顾，有很多方面吧。2016年发生很多事儿的一年，对于技术上的发展也有了新的思考，搞技术不再是死磕某种工具、算法或bug，其实本质上是为了解决问题或者做更好的产品。虽然我做的不是具体的产品而是底层的工具和平台，但这些工具的出口也是依赖”pillar application”, 多想想也是好处的。&lt;/p&gt;
&lt;p&gt;2016工作忙，读的书没有很多，但想想扎克伯克比我们还忙一年能挑战23本书确实很牛，其实自己的时间管理是不太到位的，大部分周末都懒散睡觉或者出去逛街了，回归2016年，读的书们：</summary>
    
    
    
    
    <category term="Learning" scheme="http://www.cyanny.com/tags/learning/"/>
    
  </entry>
  
  <entry>
    <title>Scala Collections</title>
    <link href="http://www.cyanny.com/2016/11/07/scala-collections/"/>
    <id>http://www.cyanny.com/2016/11/07/scala-collections/</id>
    <published>2016-11-07T02:09:10.000Z</published>
    <updated>2017-03-05T02:28:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>In scala there are many fancy collections with great utilities. Here are some key notes for scala collections which did a great help to me.</p><a id="more"></a><h1 id="Collections-Hierarchy"><a href="#Collections-Hierarchy" class="headerlink" title="Collections Hierarchy"></a>Collections Hierarchy</h1><p><img src="http://ww1.sinaimg.cn/mw1024/761b7938jw1f9jbrdeugkj20ku0q041j.jpg" alt="collection hierarchy"></p><h1 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h1><p>Collections have two kinds:</p><pre><code>- mutable collections- immutable collections</code></pre><h2 id="Immutable-Collections"><a href="#Immutable-Collections" class="headerlink" title="Immutable Collections"></a>Immutable Collections</h2><ul><li><p>Lists are finite immutable sequences. They provide constant-time access to their first element as well as the rest of the list</p></li><li><p>A stream is like a list except that its elements are computed lazily. Because of this, a stream can be infinitely long. for example:</p></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> s = <span class="number">1</span> #:: <span class="number">2</span> #:: <span class="number">3</span> #:: <span class="type">Stream</span>.empty</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span></span>(m: <span class="type">Int</span>, n: <span class="type">Int</span>): <span class="type">Stream</span>[<span class="type">Int</span>] = m #:: fib(n, m + n)</span><br></pre></td></tr></table></figure><ul><li><p>Vectors are a new collection type in Scala 2.8 that give efficient access to elements beyond the head.</p><ul><li>Access to any elements of a vector take only “effectively constant time,” as defined below.</li><li>shallow trees</li><li>when only one level, store 32 elements in an array</li><li>if lager than 32, grow to 2 levels, each node in level 2 has 32 elements, and level 1 store 32 pointers, now level 2 has 2^10 elements</li><li>level 3 has 2^ 15 elements</li><li>to access an element, the complexity is log32(N)</li><li>Vector have very decent random access performance</li><li>The default implementation to immutable IndexedSeq</li></ul></li><li><p>Stack</p><ul><li>first-in-last-out</li><li>you can use ArrayBuffer to implement a Stack<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> emtpyStack = <span class="type">Stack</span>.empty</span><br><span class="line"><span class="keyword">val</span> hasOne = emptyStack.push(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Immutable queues</p></li><li><p>Ranges</p>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> range = <span class="number">1</span> to <span class="number">10</span></span><br></pre></td></tr></table></figure></li><li><p>Hash Tries</p><ul><li>Hash tries4 are a standard way to implement immutable sets and maps efficiently.</li></ul></li><li><p>RedBlackTrees</p><ul><li>Red-black trees are a form of balanced binary trees where some nodes are designated “red” and others “black.”</li><li>TreeSet</li><li>TreeMap</li><li>default implementation for SortedSet</li></ul></li><li><p>Immutable bit sets<br><img src="http://ww2.sinaimg.cn/mw690/761b7938jw1f9jc4ydow0j20r208276p.jpg" alt="bit set example"></p><ul><li>Operations on bit sets are very fast. Testing for inclusion takes constant time. Adding an item to the set takes time proportional to the number of Longs in the bit set’s array, which is typically a small number.</li></ul></li><li><p>ListMap</p><ul><li> The only possible difference is if the map is for some reason constructed in such a way that the first elements in the list are selected much more often than the other elements.</li></ul></li></ul><h2 id="Mutable-Collections"><a href="#Mutable-Collections" class="headerlink" title="Mutable Collections"></a>Mutable Collections</h2><ul><li><p>Array buffer: operations simply access and modify the underlying array.</p></li><li><p>List Buffer: A list buffer is like an array buffer except that it uses a linked list internally instead of an array.</p><ul><li>you plan to convert the buffer to a list once it is built up, use a list buffer instead of an array buffer.</li></ul></li><li><p>StringBuilder:  a string builder is useful for building strings</p></li><li><p>LinkedList: Linked lists are mutable sequences that consist of nodes that are linked with next pointer</p><ul><li>use  LinkedList.empty.isEmpty for empty list</li><li>linked lists are best operated on sequen- tially. In addition, linked lists make it easy to insert an element or linked list into another linked list.</li></ul></li><li><p>Double Linked List: The main benefit of that additional link is that it makes element removal very fast</p></li><li><p>Mutable List:</p><ul><li>A MutableList consists of a single linked list together with a pointer that refers to the terminal empty node of that list.</li><li>The default implementation for LinearSeq</li></ul></li><li><p>Queue</p><ul><li>the dequeue method will just remove the head element from the queue and return it</li></ul></li><li><p>Array Sequences</p><ul><li>A class for polymorphic arrays of elements that’s represented internally by an array of objects</li><li>Array sequences are mutable sequences of fixed size that store their elements internally in an Array[AnyRef]</li></ul></li><li><p>Stack</p><ul><li>It works exactly the same as the immutable version except that modifications happen in place</li></ul></li><li><p>ArrayStack</p><ul><li>ArrayStack is an alternative implementation of a mutable stack, which is backed by an Array that gets resized as needed</li><li>It provides fast indexing and is generally slightly more efficient for most operations than a normal mutable stack.</li></ul></li><li><p>HashTable</p><ul><li>A hash table stores its elements in an underlying array, placing each item at a position in the array determined by the hash code of that item.</li><li>As a result, the default mutable map and set types in Scala are based on hash tables.</li><li>HashMap, HashSet implements with hash tables in array</li><li>Iteration over a hash table is not guaranteed to occur in any particular order.<ul><li>To get a guaranteed iteration order, use a linked hash map or set instead of a regular one.</li><li>Iteration over such a collection is always in the same order that the elements were initially added.</li></ul></li></ul></li><li><p>Weak Hash Maps</p><ul><li>A weak hash map is a special kind of hash map in which the garbage collector does not follow links from the map to the keys stored in it</li><li>This means that a key and its associated value will disappear from the map if there is no other reference to that key</li><li>Weak hash maps are useful for tasks such as caching, where you want to re-use an expensive function’s result if the function is called again on the same key</li><li>Weak hash maps in Scala are implemented as a wrapper of an underlying Java implementation, java.util.WeakHashMap.</li></ul></li><li><p>Concurrent Maps</p><ul><li>A concurrent map can be accessed by several threads at once.</li><li> Currently, its only implementation is Java’s java.util.concurrent.ConcurrentMap</li></ul></li><li><p>BitSet</p><ul><li>Mutable bit sets are slightly more efficient at updating than immutable ones, because they don’t have to copy around Longs that haven’t changed.</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;In scala there are many fancy collections with great utilities. Here are some key notes for scala collections which did a great help to me.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Scala" scheme="http://www.cyanny.com/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>春江花月夜</title>
    <link href="http://www.cyanny.com/2016/09/27/%E6%98%A5%E6%B1%9F%E8%8A%B1%E6%9C%88%E5%A4%9C/"/>
    <id>http://www.cyanny.com/2016/09/27/%E6%98%A5%E6%B1%9F%E8%8A%B1%E6%9C%88%E5%A4%9C/</id>
    <published>2016-09-27T01:35:59.000Z</published>
    <updated>2017-03-05T02:27:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>有人说张若虚的这首诗很值得背诵下来</p><a id="more"></a><h2 id="春江花月夜"><a href="#春江花月夜" class="headerlink" title="春江花月夜"></a>春江花月夜</h2><hr><blockquote><p>春江潮水连海平，海上明月共潮生。<br>滟滟随波千万里，何处春江无月明！<br>江流宛转绕芳甸，月照花林皆似霰;<br>空里流霜不觉飞，汀上白沙看不见。<br>江天一色无纤尘，皎皎空中孤月轮。<br>江畔何人初见月？江月何年初照人？<br>人生代代无穷已，江月年年只相似。<br>不知江月待何人，但见长江送流水。<br>白云一片去悠悠，青枫浦上不胜愁。<br>谁家今夜扁舟子？何处相思明月楼？<br>可怜楼上月徘徊，应照离人妆镜台。<br>玉户帘中卷不去，捣衣砧上拂还来。<br>此时相望不相闻，愿逐月华流照君。<br>鸿雁长飞光不度，鱼龙潜跃水成文。<br>昨夜闲潭梦落花，可怜春半不还家。<br>江水流春去欲尽，江潭落月复西斜。<br>斜月沉沉藏海雾，碣石潇湘无限路。<br>不知乘月几人归，落月摇情满江树。</p></blockquote><hr>]]></content>
    
    
    <summary type="html">&lt;p&gt;有人说张若虚的这首诗很值得背诵下来&lt;/p&gt;</summary>
    
    
    
    
    <category term="诗话" scheme="http://www.cyanny.com/tags/%E8%AF%97%E8%AF%9D/"/>
    
  </entry>
  
  <entry>
    <title>Eight Queens Problem In Scala</title>
    <link href="http://www.cyanny.com/2016/09/27/eight-queens-problem-in-scala/"/>
    <id>http://www.cyanny.com/2016/09/27/eight-queens-problem-in-scala/</id>
    <published>2016-09-27T01:15:28.000Z</published>
    <updated>2017-03-05T02:29:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>I have dedicated in <strong>Programming in Scala</strong> for about 4 months. My work is busy, but I can’t give up reading more books.<br>Scala is a fabulous language, both object oriented and functional.<br>Eight qeens problem can be expressed in scala easily and concise.</p><a id="more"></a><h2 id="Eight-Queens-Problem"><a href="#Eight-Queens-Problem" class="headerlink" title="Eight Queens Problem"></a>Eight Queens Problem</h2><p>Given a standard chess-board, place eight queens such that no queen is in check from any other (a queen can check another piece if they are on the same column, row, or diagonal)</p><h2 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h2><p>The problem in scala is recursively.</p><ol><li><p>First each solution is a List[(Row, Column)]</p><pre><code> - Each element is a coordinated, the queen position in each row - The coordicate for row k comes first, followed by row `k-1`, `k-2`, ... 0</code></pre></li><li><p>Use a Set[List[Row, Column]], represent all solutions</p></li><li><p>To place next <code>k+1</code> qeen, we iterate all solutions, if match the condition, yield another list</p></li></ol><h2 id="Lets-run-the-code"><a href="#Lets-run-the-code" class="headerlink" title="Lets run the code"></a>Lets run the code</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * the coordinates of the queen in row k comes first in each List[(ROW, Column)], followed</span></span><br><span class="line"><span class="comment">    * by k -1, k - 2, ..., 0 and so on</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param n</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">queens</span></span>(n: <span class="type">Int</span>): <span class="type">Set</span>[<span class="type">List</span>[(<span class="type">Row</span>, <span class="type">Column</span>)]] = &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">placeQueen</span></span>(k: <span class="type">Int</span>): <span class="type">Set</span>[<span class="type">List</span>[(<span class="type">Row</span>, <span class="type">Column</span>)]] = &#123;</span><br><span class="line">      <span class="keyword">if</span> (k &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="type">Set</span>(<span class="type">List</span>())</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> &#123;</span><br><span class="line">          queens &lt;- placeQueen(k - <span class="number">1</span>)</span><br><span class="line">          column &lt;- <span class="number">0</span> until n</span><br><span class="line">          queen = (k, column)</span><br><span class="line">          <span class="keyword">if</span> isSafe(queen, queens)</span><br><span class="line">        &#125; <span class="keyword">yield</span> queen :: queens</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSafe</span></span>(queen: (<span class="type">Row</span>, <span class="type">Column</span>), queens: <span class="type">List</span>[(<span class="type">Row</span>, <span class="type">Column</span>)]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">      queens.forall &#123; placedQueen =&gt;</span><br><span class="line">        placedQueen._1 != queen._1 &amp;&amp;</span><br><span class="line">          placedQueen._2 != queen._2 &amp;&amp;</span><br><span class="line">          (<span class="type">Math</span>.abs(placedQueen._1 - queen._1) !=</span><br><span class="line">            <span class="type">Math</span>.abs(placedQueen._2 - queen._2))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    placeQueen(n - <span class="number">1</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="Github-Link"><a href="#Github-Link" class="headerlink" title="Github Link"></a>Github Link</h2><p><a href="https://github.com/lgrcyanny/ScalaPractice/blob/master/ProgrammingInScala/src/main/scala/com/chapter23/EightQueens.scala">Eight Queens</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/lgrcyanny/ScalaPractice.git</span><br><span class="line"></span><br><span class="line">mvn clean package</span><br><span class="line"></span><br><span class="line">cd ProgrammingInScala</span><br><span class="line"></span><br><span class="line">scala -cp target/programming-in-scala-1.0-SNAPSHOT.jar com.chapter23.EightQueens</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;I have dedicated in &lt;strong&gt;Programming in Scala&lt;/strong&gt; for about 4 months. My work is busy, but I can’t give up reading more books.&lt;br&gt;Scala is a fabulous language, both object oriented and functional.&lt;br&gt;Eight qeens problem can be expressed in scala easily and concise.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Algorithm" scheme="http://www.cyanny.com/tags/algorithm/"/>
    
    <category term="Scala" scheme="http://www.cyanny.com/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Neural Networks</title>
    <link href="http://www.cyanny.com/2016/04/17/machine-learning-neural-networks/"/>
    <id>http://www.cyanny.com/2016/04/17/machine-learning-neural-networks/</id>
    <published>2016-04-17T13:29:20.000Z</published>
    <updated>2017-08-28T14:30:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>This week is about the mysterious Neural Networks. The courses in this week just explain the basics about Neural Networks.</p><h2 id="What-is-Neural-Networks"><a href="#What-is-Neural-Networks" class="headerlink" title="What is Neural Networks"></a>What is Neural Networks</h2><p>It’s a technique to train our data based on how human brains works. A simple Neural Network has:</p><ul><li>input layer</li><li>hidden layer</li><li>output layer</li></ul><p>We use Neural NetWorks to make classification and regression.<br>We use sigmoid function the map data from input layer to hidden layer then the output layer, the function is called activation function.</p><a id="more"></a><p><img src="http://ww3.sinaimg.cn/mw690/761b7938jw1f3019b35a2j21380kcn1d.jpg" alt="Neural Network"></p><p>In Neural Network, we add bias unit, x0, a1 to do calculate.<br>With Neural Network, we build more complex hypothesis function.<br><img src="http://ww4.sinaimg.cn/mw690/761b7938jw1f301cv572fj214o0m2teg.jpg" alt="Neural Network"></p><p>To play with neural network, you can try google’s open source <a href="http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.28657&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification">tensorflow</a></p><h2 id="Handwritten-Digital-Classification"><a href="#Handwritten-Digital-Classification" class="headerlink" title="Handwritten Digital Classification"></a>Handwritten Digital Classification</h2><p>This week’s assignment is to do multi classification on handwritten recognize.</p><p><strong>Do multi classification with one-vs-all logistic regression</strong><br>For handwritens in 10 lables: 0~9, we do 10 regression regression to calculate 10 group of theta. And then make 10 predications base on these 10 group of theta, choose the lable with max hypothesis value（probaility value）</p><p><strong>1. Cost function</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J, grad]</span> = <span class="title">lrCostFunction</span><span class="params">(theta, X, y, lambda)</span></span></span><br><span class="line">reg_theta = [<span class="number">0</span>;theta(<span class="number">2</span>:<span class="keyword">end</span>)];</span><br><span class="line">predictions = sigmoid(X * theta);</span><br><span class="line">J = (<span class="number">-1</span> / m) * sum((y .* <span class="built_in">log</span>(predictions) + (<span class="number">1</span> - y) .* <span class="built_in">log</span>(<span class="number">1</span> - predictions))) + (lambda / (<span class="number">2</span> * m)) * sum(reg_theta .^ <span class="number">2</span>);</span><br><span class="line">grad = (<span class="number">1</span> / m) * (X&#x27; * (predictions - y)) + (lambda / m) * reg_theta;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p><strong>2. OneVsAll</strong><br>make 10 classifications</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[all_theta]</span> = <span class="title">oneVsAll</span><span class="params">(X, y, num_labels, lambda)</span></span></span><br><span class="line"><span class="comment">% Some useful variables</span></span><br><span class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>);</span><br><span class="line">n = <span class="built_in">size</span>(X, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% You need to return the following variables correctly</span></span><br><span class="line">all_theta = <span class="built_in">zeros</span>(num_labels, n + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Add ones to the X data matrix</span></span><br><span class="line">X = [<span class="built_in">ones</span>(m, <span class="number">1</span>) X];</span><br><span class="line"></span><br><span class="line"><span class="comment">% Note: For this assignment, we recommend using fmincg to optimize the cost</span></span><br><span class="line"><span class="comment">%       function. It is okay to use a for-loop (for c = 1:num_labels) to</span></span><br><span class="line"><span class="comment">%       loop over the different classes.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%       fmincg works similarly to fminunc, but is more efficient when we</span></span><br><span class="line"><span class="comment">%       are dealing with large number of parameters.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:num_labels</span><br><span class="line">    initial_theta = <span class="built_in">zeros</span>(n + <span class="number">1</span>, <span class="number">1</span>);</span><br><span class="line">    options = optimset(<span class="string">&#x27;GradObj&#x27;</span>, <span class="string">&#x27;on&#x27;</span>, <span class="string">&#x27;MaxIter&#x27;</span>, <span class="number">50</span>);</span><br><span class="line">    <span class="comment">% theta is a column vector</span></span><br><span class="line">    <span class="comment">% Run fmincg to obtain the optimal theta</span></span><br><span class="line">    [theta] = fmincg(@(t)(lrCostFunction(t, X, (y == <span class="built_in">i</span>), lambda)), ...</span><br><span class="line">                    initial_theta, options);</span><br><span class="line">    all_theta(<span class="built_in">i</span>, :) = theta&#x27;;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>3. PredictOneVsAll</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span> = <span class="title">predictOneVsAll</span><span class="params">(all_theta, X)</span></span></span><br><span class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>);</span><br><span class="line">num_labels = <span class="built_in">size</span>(all_theta, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% You need to return the following variables correctly</span></span><br><span class="line">p = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X, <span class="number">1</span>), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Add ones to the X data matrix</span></span><br><span class="line">X = [<span class="built_in">ones</span>(m, <span class="number">1</span>) X];</span><br><span class="line">predictions = X * all_theta&#x27;;</span><br><span class="line"><span class="comment">% calculate max of each row</span></span><br><span class="line">[max_predictions, p] = <span class="built_in">max</span>(predictions, [], <span class="number">2</span>);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>The logistic has great accurracy, about 95% in this case, but neural network will have higher accuracy, about 97%.</p><h2 id="Neural-Forward-Propagation-algorithm"><a href="#Neural-Forward-Propagation-algorithm" class="headerlink" title="Neural Forward Propagation algorithm"></a>Neural Forward Propagation algorithm</h2><p>In the assignment, it build hypothesis function with 3 layers neural network.<br>The predications implementation</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span> = <span class="title">predict</span><span class="params">(Theta1, Theta2, X)</span></span></span><br><span class="line"><span class="comment">%PREDICT Predict the label of an input given a trained neural network</span></span><br><span class="line"><span class="comment">%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the</span></span><br><span class="line"><span class="comment">%   trained weights of a neural network (Theta1, Theta2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Useful values</span></span><br><span class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>);</span><br><span class="line">num_labels = <span class="built_in">size</span>(Theta2, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% You need to return the following variables correctly </span></span><br><span class="line">p = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X, <span class="number">1</span>), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% ====================== YOUR CODE HERE ======================</span></span><br><span class="line"><span class="comment">% Instructions: Complete the following code to make predictions using</span></span><br><span class="line"><span class="comment">%               your learned neural network. You should set p to a </span></span><br><span class="line"><span class="comment">%               vector containing labels between 1 to num_labels.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Hint: The max function might come in useful. In particular, the max</span></span><br><span class="line"><span class="comment">%       function can also return the index of the max element, for more</span></span><br><span class="line"><span class="comment">%       information see &#x27;help max&#x27;. If your examples are in rows, then, you</span></span><br><span class="line"><span class="comment">%       can use max(A, [], 2) to obtain the max for each row.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Theta1 is 25 * 401, X is 5000 * 401</span></span><br><span class="line">X = [<span class="built_in">ones</span>(m, <span class="number">1</span>) X];</span><br><span class="line"><span class="comment">% z2 is 5000 * 25</span></span><br><span class="line">z2 = X * Theta1&#x27;;</span><br><span class="line">a2 = sigmoid(z2);</span><br><span class="line"><span class="comment">% a2 with bias unit is 5000 * 26</span></span><br><span class="line">a2 = [<span class="built_in">ones</span>(m, <span class="number">1</span>) a2];</span><br><span class="line"></span><br><span class="line"><span class="comment">% Theta2 is 10 * 26</span></span><br><span class="line"><span class="comment">% z3 is 5000 * 10</span></span><br><span class="line">z3 = a2 * Theta2&#x27;;</span><br><span class="line">a3 = sigmoid(z3);</span><br><span class="line">[max_valid, p] = <span class="built_in">max</span>(a3, [], <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% =========================================================================</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>My question is:</p><ul><li>how to train the Theta1, Theta2</li><li>how to decide how many units in hidden layer<br>In the later course, I think NG will explain it. Next week, I will learn backpropagation algorithm.</li></ul><h2 id="My-assignment"><a href="#My-assignment" class="headerlink" title="My assignment"></a>My assignment</h2><p><a href="https://github.com/lgrcyanny/MachineLearningCoursera/tree/master/assignments">Week Assignments</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;This week is about the mysterious Neural Networks. The courses in this week just explain the basics about Neural Networks.&lt;/p&gt;
&lt;h2 id=&quot;What-is-Neural-Networks&quot;&gt;&lt;a href=&quot;#What-is-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;What is Neural Networks&quot;&gt;&lt;/a&gt;What is Neural Networks&lt;/h2&gt;&lt;p&gt;It’s a technique to train our data based on how human brains works. A simple Neural Network has:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input layer&lt;/li&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;output layer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use Neural NetWorks to make classification and regression.&lt;br&gt;We use sigmoid function the map data from input layer to hidden layer then the output layer, the function is called activation function.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Machine Learning" scheme="http://www.cyanny.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Linear Regression</title>
    <link href="http://www.cyanny.com/2016/04/04/machine-learning-linear-regression/"/>
    <id>http://www.cyanny.com/2016/04/04/machine-learning-linear-regression/</id>
    <published>2016-04-04T07:55:31.000Z</published>
    <updated>2017-03-05T02:27:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>I have been learning the coursera Machine Learning Course by Andrew Ng for two weeks now. Machine Learning is fun and different. For the coursera assignment1 of linear regression, I want to share something.</p><a id="more"></a><h2 id="Using-matlab"><a href="#Using-matlab" class="headerlink" title="Using matlab"></a>Using matlab</h2><p>I think matlab is better than octave, please use coursera account. <a href="https://www.coursera.org/learn/machine-learning/supplement/rANSM/installing-matlab">Install matlab</a></p><h2 id="Octave-Install"><a href="#Octave-Install" class="headerlink" title="Octave Install"></a>Octave Install</h2><p>The course use Octave/Matlab for programming practice. I learned octave basics in two days. I don’t have too much time, can just doing these homework in weekends. For Octavel installed on mac, I encounter some problems and solved it. Now octave is 4.2.0, I think ocatve is better now.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install octave</span><br></pre></td></tr></table></figure><p>if you encounter some problem, you can solve it as follows:</p><ul><li>brew update &amp;&amp; brew upgrade</li><li>brew tap –repair</li><li>brew install octave</li><li>install xserver(seems no need to install)<ul><li><a href="http://www.xquartz.org/">http://www.xquartz.org/</a></li></ul></li><li>font can’t find when plot<ul><li>export FONTCONFIG_PATH=/opt/X11/lib/X11/fontconfig</li></ul></li><li>can’t plot unknown or ambiguous terminal type; type just ‘set terminal’ for a list<ul><li>brew uninstall gnuplot</li><li>download and install aquaterm: <a href="https://sourceforge.net/projects/aquaterm/?source=typ_redirect">https://sourceforge.net/projects/aquaterm/?source=typ_redirect</a></li><li>brew install gnuplot –with-aquaterm –with-qt4</li></ul></li><li>add start config to /usr/local/share/octave/site/m/startup/octaverc<ul><li>PS1(‘&gt;&gt; ‘)</li></ul></li></ul><h2 id="Gradient-Descent-Algorithm"><a href="#Gradient-Descent-Algorithm" class="headerlink" title="Gradient Descent Algorithm"></a>Gradient Descent Algorithm</h2><p>Implementing gradient desenct algorithm in vectorization style was more efficient than iteration algorithm. Here is my implementation:<br>No for loop looks elegant.</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, num_iters)</span></span></span><br><span class="line"><span class="comment">%GRADIENTDESCENT Performs gradient descent to learn theta</span></span><br><span class="line"><span class="comment">%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by</span></span><br><span class="line"><span class="comment">%   taking num_iters gradient steps with learning rate alpha</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize some useful values</span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line">J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</span><br><span class="line"></span><br><span class="line">    <span class="comment">% ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment">% Instructions: Perform a single gradient step on the parameter vector</span></span><br><span class="line">    <span class="comment">%               theta.</span></span><br><span class="line">    <span class="comment">%</span></span><br><span class="line">    <span class="comment">% Hint: While debugging, it can be useful to print out the values</span></span><br><span class="line">    <span class="comment">%       of the cost function (computeCost) and gradient here.</span></span><br><span class="line">    <span class="comment">%</span></span><br><span class="line">    predications = X * theta;</span><br><span class="line">    errors = predications - y; <span class="comment">% m by 1 vector</span></span><br><span class="line">    <span class="comment">% sum_delta = (alpha / m) * sum(errors .* X, 1); % sum by column, which is 1 by n + 1 matrix</span></span><br><span class="line">    <span class="comment">% transpose X, no need sum(errors .* X, 1) here</span></span><br><span class="line">    sum_delta = (alpha / m) .* (X&#x27; * errors);</span><br><span class="line">    theta = theta - sum_delta;</span><br><span class="line"></span><br><span class="line">    <span class="comment">% ============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">% Save the cost J in every iteration</span></span><br><span class="line">    J_history(iter) = computeCost(X, y, theta);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Another implementation by my wwzyhao</strong><br>[by wwzyhao]</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, num_iters)</span></span></span><br><span class="line"><span class="comment">%GRADIENTDESCENT Performs gradient descent to learn theta</span></span><br><span class="line"><span class="comment">%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by</span></span><br><span class="line"><span class="comment">%   taking num_iters gradient steps with learning rate alpha</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize some useful values</span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line">J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</span><br><span class="line"></span><br><span class="line">    delta = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X, <span class="number">2</span>), <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:m</span><br><span class="line">        x = (X(<span class="built_in">j</span>,:))&#x27;;</span><br><span class="line">        delta = delta + (<span class="number">1</span> / m) * (theta&#x27; * x - y(<span class="built_in">j</span>)) * x;</span><br><span class="line">    <span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line">    theta = theta - alpha * delta;</span><br><span class="line"></span><br><span class="line">    <span class="comment">% ============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">% Save the cost J in every iteration</span></span><br><span class="line">    J_history(iter) = computeCost(X, y, theta);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>Not better than me! haha~</p><h2 id="My-assignments-on-github"><a href="#My-assignments-on-github" class="headerlink" title="My assignments on github"></a>My assignments on github</h2><p><a href="https://github.com/lgrcyanny/MachineLearningCoursera/tree/master/assignments/ex1/ex1">Assignments1</a><br>For submition errors, please refer to<a href="https://learner.coursera.help/hc/en-us/community/posts/204693179-linear-regression-submit-error">Jacob Middag</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;I have been learning the coursera Machine Learning Course by Andrew Ng for two weeks now. Machine Learning is fun and different. For the coursera assignment1 of linear regression, I want to share something.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Machine Learning" scheme="http://www.cyanny.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Programming In Scala Overview Key Note</title>
    <link href="http://www.cyanny.com/2016/03/06/programming-in-scala-overview-key-note/"/>
    <id>http://www.cyanny.com/2016/03/06/programming-in-scala-overview-key-note/</id>
    <published>2016-03-06T10:01:42.000Z</published>
    <updated>2017-03-05T02:27:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>过去半年里，都在忙着spark相关的项目，主要的编程语言是scala，前些年主要用的是c++，刚转到scala上时，有点不适应函数式编程语言的思想，现在已经半年多过去了，觉得scala真的是awesome，简洁有力的表达，整合了functional programming和object programming，并且设计良好。<br>去年上了Martin Odersky的编程课<i>Functional Programming In Scala</i>, 觉得挺感兴趣，学习了很多，想在scala语言上看得更多，更深入些，我选择了Scala的权威著作<a href="http://www.artima.com/shop/programming_in_scala_2ed">Programming In Scala</a>, 虽然这书有快900页，但我想挑战下，与其每天泡在各种新闻资讯里，还不如看看书泡在书里。每天都看看书，觉得挺好，有些笔记和心得就写在博客里，毕竟工作忙，写博客的时间很少，但我想还是应该多总结和留下写想法。之后阅读Spark的源码，我也会坚持写下些东西，看过了和写下来总还是不一样嘛。</p><a id="more"></a><h1 id="Scala-Overview"><a href="#Scala-Overview" class="headerlink" title="Scala Overview"></a>Scala Overview</h1><h2 id="1-Scala-is-a-Scalable-Language"><a href="#1-Scala-is-a-Scalable-Language" class="headerlink" title="1. Scala is a Scalable Language"></a>1. Scala is a Scalable Language</h2><ul><li>Scala is a blend of object-oriented and functional programing language</li><li>Grow new types, such as BigInt</li><li>Grow new control structures, such as actor based api</li></ul><h2 id="2-What-makes-scala-scalable"><a href="#2-What-makes-scala-scalable" class="headerlink" title="2. What makes scala scalable"></a>2. What makes scala scalable</h2><p>fusion object-oriented and functional programming</p><ul><li><strong>Object-Oriented</strong><ul><li>it combines data with operations under a formalized interface. So objects have a lot to do with language scalability: the same techniques apply to the construction of small as well as large program</li><li>Scala is an object-oriented language in pure form: every value is an object and every operation is a method call.</li><li> An example is Scala’s traits. Traits are like interfaces in Java, but they can also have method implementations and even fields</li></ul></li><li><strong>Functional</strong><ul><li>Functional programming fundation was raid in lonzo Church’s lambda calculus, in the 1930. The first functional programming language is Lisp, created in the late 1950s, other functional programming languages are Scheme, SML, Erlang, Haskell, OCaml, and F#.</li><li>Functional programming two main ideas:<ul><li>Firstly, <strong>functional are first class values</strong><ul><li>You can pass functions as arguments to other functions, return them as results from functions, or store them in variables. You can also define a function inside another function, just as you can define an integer value inside a function</li><li>Functions that are first-class values provide a convenient means for abstracting over operations and creating new control structures.</li></ul></li><li>Secondly, <strong>operations of a program should map input values to output values rather than change data in place.</strong><ul><li>methods should not have any side effects<ul><li> <strong>Referentially transparent</strong>, which means that for any given input the method call could be replaced by its result without affecting the program’s semantics</li></ul></li><li>encourage immutable data structures and referentially transparent methods</li></ul></li></ul></li></ul></li></ul><h2 id="3-Why-chooose-scala"><a href="#3-Why-chooose-scala" class="headerlink" title="3. Why chooose scala?"></a>3. Why chooose scala?</h2><ul><li><strong>Compatibility</strong><ul><li>compile to JVM bytecode, run on jvm</li><li>compatible with java types, reuse java types</li><li>implicity conversions : support string.toInt,</li><li>java can call scala code</li></ul></li><li><strong>Brevity, scala is concise</strong><ul><li>reduction on list</li><li>Avoid biolerplate in java, such as avoid class getter and setter, default constructor</li><li>Type inference</li><li>tools in library, can be used as trait</li></ul></li><li><strong>High-level abstractions</strong><ul><li>Scala helps you manage complexity by letting you raise the level of abstraction in the interfaces you design and use.<br>For example, The Scala code treats the same strings as higher-level sequences of characters that can be queried with predicates<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tr = str.exists(_.isUpper)</span><br></pre></td></tr></table></figure></li><li>Functional literals are lightweight</li></ul></li><li><strong>Advanced static type system</strong><ul><li>it allows you to parameterize types with generics, to combine types using intersections, and to hide details of types using abstract types.</li><li>Although some argues that static typed language is verbose and not flexible, In scala  <strong>verbosity</strong> is avoided through type inference and <strong>flexibility</strong> is gained through pattern matching and several new ways to write and compose types.</li><li>Advantages of static typing system:<ul><li>Verifiable properties: Static type systems can prove the absence of certain run-time errors. Reduce the number of unit tests.</li><li>Safe refactoring : make changes to a codebase with a high degree of confidence</li><li>Documentation: Static types are program documentation that is checked by the compiler for correctness.<ul><li>Scala has a very sophisticated type inference system that lets you omit almost all type information that’s usually considered annoying</li></ul></li></ul></li></ul></li></ul><h2 id="4-Scala-roots"><a href="#4-Scala-roots" class="headerlink" title="4.Scala roots"></a>4.Scala roots</h2><p>Although only a few features of Scala are genuinely new; most have been already applied in some form in other languages. Its design models many languages, such as SmallTalk, Ruby, Algol, Simula, OCaml, Haskell etc.<br>Scala’s innovations come primarily from how its constructs are put together<br>Scala is also not the first language to integrate functional and object-oriented programming, although it probably goes furthest in this direction</p><p>Given Scala’s innovations:</p><ul><li>  its abstract types provide a more object-oriented alternative to generic types,</li><li>  its traits allow for flexible component assembly,</li><li>  its extractors provide a representation-independent way to do pattern matching.</li></ul><h2 id="5-Starting-Programming"><a href="#5-Starting-Programming" class="headerlink" title="5. Starting Programming"></a>5. Starting Programming</h2><p><a href="https://github.com/lgrcyanny/ScalaPractice/tree/master/ProgrammingInScala">Programming In Scala</a><br>读书的时候写写代码，都是书里的例子，用maven构建的scala progject。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;过去半年里，都在忙着spark相关的项目，主要的编程语言是scala，前些年主要用的是c++，刚转到scala上时，有点不适应函数式编程语言的思想，现在已经半年多过去了，觉得scala真的是awesome，简洁有力的表达，整合了functional programming和object programming，并且设计良好。&lt;br&gt;去年上了Martin Odersky的编程课&lt;i&gt;Functional Programming In Scala&lt;/i&gt;, 觉得挺感兴趣，学习了很多，想在scala语言上看得更多，更深入些，我选择了Scala的权威著作&lt;a href=&quot;http://www.artima.com/shop/programming_in_scala_2ed&quot;&gt;Programming In Scala&lt;/a&gt;, 虽然这书有快900页，但我想挑战下，与其每天泡在各种新闻资讯里，还不如看看书泡在书里。每天都看看书，觉得挺好，有些笔记和心得就写在博客里，毕竟工作忙，写博客的时间很少，但我想还是应该多总结和留下写想法。之后阅读Spark的源码，我也会坚持写下些东西，看过了和写下来总还是不一样嘛。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Scala" scheme="http://www.cyanny.com/tags/scala/"/>
    
    <category term="Language" scheme="http://www.cyanny.com/tags/language/"/>
    
  </entry>
  
</feed>
