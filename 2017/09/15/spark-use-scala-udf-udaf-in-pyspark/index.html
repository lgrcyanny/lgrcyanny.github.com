<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>How to Use Scala UDF and UDAF in PySpark | CyannyLive | AI and Big Data</title>

  
  <meta name="author" content="Cyanny Liang">
  

  
  <meta name="description" content="Wisdom comes from inside">
  

  
  
  <meta name="keywords" content="spark">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="How to Use Scala UDF and UDAF in PySpark"/>

  <meta property="og:site_name" content="CyannyLive"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="CyannyLive" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
<meta name="generator" content="Hexo 5.3.0"></head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">CyannyLive</a>
    </h1>
    <p class="site-description">AI and Big Data</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
        <li><a href="/about/">About</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>How to Use Scala UDF and UDAF in PySpark</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/09/15/spark-use-scala-udf-udaf-in-pyspark/" rel="bookmark">
        <time class="entry-date published" datetime="2017-09-15T03:16:33.000Z">
          2017-09-15
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Spark DataFrame API provides efficient and easy-to-use operations to do analysis on distributed collection of data. Many users love the Pyspark API, which is more usable than scala API. Sometimes when we use UDF in pyspark, the performance will be a problem. How about implementing these UDF in scala, and call them in pyspark? BTW, in spark 2.0, UDAF can only be defined in scala, and how to use it in pyspark? Let’s have a try~</p>
<a id="more"></a>

<h2 id="Use-Scala-UDF-in-PySpark"><a href="#Use-Scala-UDF-in-PySpark" class="headerlink" title="Use Scala UDF in PySpark"></a>Use Scala UDF in PySpark</h2><p><strong>1. define scala udf</strong></p>
<p>Suppose we want to calculate string length, lets define it in scala UDF.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StringLength</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getStringLength</span></span>(s: <span class="type">String</span>) = s.length</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getFun</span></span>(): <span class="type">UserDefinedFunction</span> = udf(getStringLength _)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>2. use udf in python</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> Column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_java_column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_seq</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col</span><br><span class="line"> </span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;scala_udf_test&quot;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_length</span>(<span class="params">col</span>):</span></span><br><span class="line">    _string_length = sc._jvm.com.learning.StringLength.getFun()</span><br><span class="line">    <span class="keyword">return</span> Column(_string_length.apply(_to_seq(sc, [col], _to_java_column)))</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span>():</span></span><br><span class="line">    rows = [</span><br><span class="line">        (<span class="string">&quot;k1&quot;</span>, <span class="string">&quot;aaa&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k2&quot;</span>, <span class="string">&quot;dd&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k3&quot;</span>, <span class="string">&quot;cc&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k4&quot;</span>, <span class="string">&quot;eee&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">    df = spark.createDataFrame(rows, [<span class="string">&#x27;key&#x27;</span>, <span class="string">&#x27;value&#x27;</span>])</span><br><span class="line">    df.show(<span class="number">50</span>)</span><br><span class="line">    df.select(col(<span class="string">&quot;key&quot;</span>), string_length(col(<span class="string">&quot;value&quot;</span>))).show()</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    process()</span><br></pre></td></tr></table></figure>
<p><strong>3. submit the app</strong></p>
<p>compile the scala code and submit python files with –jars</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --jars testing/learning<span class="number">-1.0</span><span class="number">.0</span>-<span class="type">SNAPSHOT</span>.jar udf_test.py</span><br></pre></td></tr></table></figure>
<p>the output would be:</p>
<table>
<thead>
<tr>
<th>key</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>k1</td>
<td>3</td>
</tr>
<tr>
<td>k2</td>
<td>2</td>
</tr>
<tr>
<td>k3</td>
<td>2</td>
</tr>
<tr>
<td>k4</td>
<td>3</td>
</tr>
</tbody></table>
<p><strong>4. performance analysis</strong></p>
<p>let’s explain the scala UDF in Python<br><img src="http://wx1.sinaimg.cn/mw690/761b7938ly1fjmeol1jg8j20s405odim.jpg" alt="scala udf physical plan"><br>the Project Plan is Scala UDF</p>
<p>and if we implement Python UDF as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">py_slen = udf(<span class="keyword">lambda</span> s: <span class="built_in">len</span>(s), IntegerType())</span><br><span class="line">df_with_python_udf = (df.select(col(<span class="string">&quot;key&quot;</span>), py_slen(<span class="string">&quot;value&quot;</span>).alias(<span class="string">&quot;slen&quot;</span>)).orderBy(col(<span class="string">&quot;slen&quot;</span>).desc()))</span><br></pre></td></tr></table></figure>
<p>the Python plan is:<br><img src="http://wx4.sinaimg.cn/mw690/761b7938ly1fjmeofgzbmj210k06igot.jpg" alt="python udf physical plan"><br>the UDF plan is different, which is BatchEvalPython.<br>It can prove that when use scala UDF in python, the evaluation is in JVM and data will not exchange with Python worker. And the performance should be improved.</p>
<p>I evaluated the performance in local environment with 4cores and 2GB memory, and generated 10million rows for each test, the result is as follows:<br><img src="http://wx2.sinaimg.cn/mw690/761b7938ly1fjmer43xfnj20kw0ckt8z.jpg" alt="scala vs python string len udf"><br><strong>Scala UDF is 1.89 times Python UDF</strong></p>
<p><strong>And then I implemented another UDF in Scala and Python with regex string parsing</strong>, the performance is<br><img src="http://wx3.sinaimg.cn/mw690/761b7938ly1fjmeopknh4j20a0061748.jpg" alt="scala vs python string regex parsing"></p>
<p><strong>Scala udf is 2.23 times Python REGEX String Parsing UDF</strong></p>
<p>the Scala UDF is defined as follows:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by lgrcyanny on 17/9/13.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StringParse</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">STRING_PATTERN</span> = <span class="string">&quot;&quot;</span><span class="string">&quot;(a.*b)&quot;</span><span class="string">&quot;&quot;</span>.r</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parseString</span></span>(str: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> matched = <span class="type">STRING_PATTERN</span>.findFirstMatchIn(str)</span><br><span class="line">    <span class="keyword">if</span> (matched.isEmpty) &#123;</span><br><span class="line">      <span class="string">&quot;&quot;</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      matched.get.group(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getFun</span></span>() = udf(parseString _ )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Python string parse UDF  vs Scala UDF:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> Column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_java_column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_seq</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> length</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_word</span>(<span class="params">length</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;get random word for generate rows&quot;&quot;&quot;</span></span><br><span class="line">    letters = string.ascii_lowercase</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join([random.choice(letters) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length)])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_rows</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;generate rows in key value pair&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># generate rows</span></span><br><span class="line">    letters = <span class="string">&quot;abcdefghijklmnopqrstuvwxyz&quot;</span></span><br><span class="line">    rows = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="built_in">id</span> = random.randint(<span class="number">0</span>, <span class="number">100</span>)</span><br><span class="line">        slen = random.randint(<span class="number">0</span>, <span class="number">20</span>)</span><br><span class="line">        word = random_word(slen)</span><br><span class="line">        rows.append((<span class="built_in">id</span>, letters))</span><br><span class="line">    <span class="keyword">return</span> rows</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_parse</span>(<span class="params">col</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;scala udf parse string&quot;&quot;&quot;</span></span><br><span class="line">    _string_parse = sc._jvm.com.learning.StringParse.getFun()</span><br><span class="line">    <span class="keyword">return</span> Column(_string_parse.apply(_to_seq(sc, [col], _to_java_column)))</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_regex_udf</span>(<span class="params">n=<span class="number">1000</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;test udf with regex parse&quot;&quot;&quot;</span></span><br><span class="line">    rows = generate_rows(n)</span><br><span class="line">    df = spark.createDataFrame(rows, [<span class="string">&#x27;key&#x27;</span>, <span class="string">&#x27;value&#x27;</span>])</span><br><span class="line">    df.show(<span class="number">20</span>)</span><br><span class="line">    pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;(a.*b)&quot;</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_string</span>(<span class="params"><span class="built_in">str</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;parse string with python regex&quot;&quot;&quot;</span></span><br><span class="line">        matched = re.search(pattern, <span class="built_in">str</span>)</span><br><span class="line">        <span class="keyword">if</span> matched:</span><br><span class="line">            <span class="keyword">return</span> matched.group(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    py_parse_str = udf(parse_string, StringType())</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    df_with_python_udf = (df.select(col(<span class="string">&quot;key&quot;</span>), py_parse_str(col(<span class="string">&quot;value&quot;</span>)).alias(<span class="string">&quot;parsed_value&quot;</span>))</span><br><span class="line">                          .<span class="built_in">filter</span>(length(col(<span class="string">&quot;parsed_value&quot;</span>)) &gt; <span class="number">0</span>))</span><br><span class="line">    df_with_python_udf.explain(<span class="literal">True</span>)</span><br><span class="line">    df_with_python_udf.show()</span><br><span class="line">    print(<span class="string">&quot;matched rows: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(df_with_python_udf.count()))</span><br><span class="line">    print(<span class="string">&quot;duration for python regex parse: &#123;&#125;s&quot;</span>.<span class="built_in">format</span>(time.time() - start_time))</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    df_with_scala_udf = (df.select(col(<span class="string">&quot;key&quot;</span>), string_parse(col(<span class="string">&quot;value&quot;</span>)).alias(<span class="string">&quot;parsed_value&quot;</span>))</span><br><span class="line">                          .<span class="built_in">filter</span>(length(col(<span class="string">&quot;parsed_value&quot;</span>)) &gt; <span class="number">0</span>))</span><br><span class="line">    df_with_python_udf.explain(<span class="literal">True</span>)</span><br><span class="line">    df_with_scala_udf.show()</span><br><span class="line">    print(<span class="string">&quot;matched rows: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(df_with_scala_udf.count()))</span><br><span class="line">    print(<span class="string">&quot;duration for scala regex parse: &#123;&#125;s&quot;</span>.<span class="built_in">format</span>(time.time() - start_time))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>5. Conclusion</strong></p>
<p>Databricks used to give a performance for Python vs Scala DataFrame and RDD API:<br><img src="http://wx2.sinaimg.cn/mw690/761b7938ly1fjmeo7vk38j210c0fy48j.jpg" alt="databricks performance"></p>
<p>the blog is <a target="_blank" rel="noopener" href="https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html">here</a>.<br>The performance is a running group-aggregation on 10 million integer pairs on a single machince. The Scala DF is almost 5 times Python lambda function in RDD Python.</p>
<p>Even though, the Scala UDF is not 5 times Python UDF, about 2 times in my test, using scala UDF can improve performance indeed.</p>
<h2 id="Use-Scala-UDAF-in-PySpark"><a href="#Use-Scala-UDAF-in-PySpark" class="headerlink" title="Use Scala UDAF in PySpark"></a>Use Scala UDAF in PySpark</h2><p>UDAF now only supports defined in Scala and Java(spark 2.0)</p>
<p><strong>1. define scala UDAF</strong></p>
<p>when define UDAF, it must extend class <code>UserDefinedAggregateFunction</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">ArrayType</span>, <span class="type">DataType</span>, <span class="type">StringType</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GroupConcat</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;s&quot;</span>, <span class="type">StringType</span>)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;buff&quot;</span>, <span class="type">ArrayType</span>(<span class="type">StringType</span>))</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">StringType</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer.update(<span class="number">0</span>, <span class="type">ArrayBuffer</span>.empty[<span class="type">String</span>])</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer.update(<span class="number">0</span>, buffer.getSeq[<span class="type">String</span>](<span class="number">0</span>) :+ input.getString(<span class="number">0</span>))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1.update(<span class="number">0</span>, buffer1.getSeq[<span class="type">String</span>](<span class="number">0</span>) ++ buffer2.getSeq[<span class="type">String</span>](<span class="number">0</span>))</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getSeq[<span class="type">String</span>](<span class="number">0</span>).mkString(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>2. use UDAF in python</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> Column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_java_column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_seq</span><br><span class="line"> </span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;scala_udf_test&quot;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_concat</span>(<span class="params">col</span>):</span></span><br><span class="line">    _groupConcat = sc._jvm.com.learning.GroupConcat.apply</span><br><span class="line">    <span class="keyword">return</span> Column(_groupConcat(_to_seq(sc, [col], _to_java_column)))</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span>():</span></span><br><span class="line">    rows = [</span><br><span class="line">        (<span class="string">&quot;k1&quot;</span>, <span class="string">&quot;a&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k1&quot;</span>, <span class="string">&quot;b&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k1&quot;</span>, <span class="string">&quot;c&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k2&quot;</span>, <span class="string">&quot;d&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k3&quot;</span>, <span class="string">&quot;e&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;k3&quot;</span>, <span class="string">&quot;f&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">    df = spark.createDataFrame(rows, [<span class="string">&#x27;key&#x27;</span>, <span class="string">&#x27;value&#x27;</span>])</span><br><span class="line">    df.show(<span class="number">50</span>)</span><br><span class="line">    df.groupBy(<span class="string">&quot;key&quot;</span>).agg(group_concat(<span class="string">&quot;value&quot;</span>).alias(<span class="string">&quot;concat&quot;</span>)).show()</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    process()</span><br></pre></td></tr></table></figure>
<p><strong>3. submit the app</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;spark-submit --jars testing&#x2F;learning-1.0.0-SNAPSHOT.jar udf_test.py</span><br></pre></td></tr></table></figure>
<p>the output would be:</p>
<table>
<thead>
<tr>
<th>key</th>
<th>cancat</th>
</tr>
</thead>
<tbody><tr>
<td>k1</td>
<td>a,b,c</td>
</tr>
<tr>
<td>k2</td>
<td>d</td>
</tr>
<tr>
<td>k3</td>
<td>e,f</td>
</tr>
</tbody></table>
<p><strong>4. references</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/31640729/spark-sql-replacement-for-mysql-group-concat-aggregate-function">spark-sql-replacement-for-mysql-group-concat-aggregate-function</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/33233737/spark-how-to-map-python-with-scala-or-java-user-defined-functions">spark-how-to-map-python-with-scala-or-java-user-defined-functions</a></li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/spark/">spark</a>
    </span>
    

    </div>

    
  </div>
</article>

  
	<section id="comments" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'lgrcyanny';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>





    </main>

    <footer class="site-footer">
  <p class="site-info">
    Copyright
    </br>
    
    &copy; 2021 Cyanny Liang
    
  </p>
</footer>
    
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-40624708-1', 'auto');
    ga('send', 'pageview');

</script>

  </div>
</div>
</body>
</html>