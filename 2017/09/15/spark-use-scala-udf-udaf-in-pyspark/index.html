<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>How to Use Scala UDF and UDAF in PySpark | CyannyLive</title>

  
  <meta name="author" content="Cyanny Liang">
  

  
  <meta name="description" content="Do not go gentle into that good night">
  

  
  
  <meta name="keywords" content="spark">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="How to Use Scala UDF and UDAF in PySpark"/>

  <meta property="og:site_name" content="CyannyLive"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="CyannyLive" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">CyannyLive</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
        <li><a href="/about/">About</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>How to Use Scala UDF and UDAF in PySpark</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/09/15/spark-use-scala-udf-udaf-in-pyspark/" rel="bookmark">
        <time class="entry-date published" datetime="2017-09-15T03:16:33.000Z">
          2017-09-15
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Spark DataFrame API provides efficient and easy-to-use operations to do analysis on distributed collection of data. Many users love the Pyspark API, which is more usable than scala API. Sometimes when we use UDF in pyspark, the performance will be a problem. How about implementing these UDF in scala, and call them in pyspark? BTW, in spark 2.0, UDAF can only be defined in scala, and how to use it in pyspark? Let’s have a try~<br><a id="more"></a></p>
<h2 id="Use-Scala-UDF-in-PySpark"><a href="#Use-Scala-UDF-in-PySpark" class="headerlink" title="Use Scala UDF in PySpark"></a>Use Scala UDF in PySpark</h2><p><strong>1. define scala udf</strong></p>
<p>Suppose we want to calculate string length, lets define it in scala UDF.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">StringLength</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getStringLength</span></span>(s: <span class="type">String</span>) = s.length</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getFun</span></span>(): <span class="type">UserDefinedFunction</span> = udf(getStringLength _)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>2. use udf in python</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</div><div class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> Column</div><div class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_java_column</div><div class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_seq</div><div class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col</div><div class="line"> </div><div class="line">spark = SparkSession.builder.appName(<span class="string">"scala_udf_test"</span>).getOrCreate()</div><div class="line">sc = spark.sparkContext</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_length</span><span class="params">(col)</span>:</span></div><div class="line">    _string_length = sc._jvm.com.learning.StringLength.getFun()</div><div class="line">    <span class="keyword">return</span> Column(_string_length.apply(_to_seq(sc, [col], _to_java_column)))</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">()</span>:</span></div><div class="line">    rows = [</div><div class="line">        (<span class="string">"k1"</span>, <span class="string">"aaa"</span>),</div><div class="line">        (<span class="string">"k2"</span>, <span class="string">"dd"</span>),</div><div class="line">        (<span class="string">"k3"</span>, <span class="string">"cc"</span>),</div><div class="line">        (<span class="string">"k4"</span>, <span class="string">"eee"</span>),</div><div class="line">    ]</div><div class="line">    df = spark.createDataFrame(rows, [<span class="string">'key'</span>, <span class="string">'value'</span>])</div><div class="line">    df.show(<span class="number">50</span>)</div><div class="line">    df.select(col(<span class="string">"key"</span>), string_length(col(<span class="string">"value"</span>))).show()</div><div class="line"> </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    process()</div></pre></td></tr></table></figure>
<p><strong>3. submit the app</strong></p>
<p>compile the scala code and submit python files with –jars</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit --jars testing/learning<span class="number">-1.0</span><span class="number">.0</span>-<span class="type">SNAPSHOT</span>.jar udf_test.py</div></pre></td></tr></table></figure>
<p>the output would be:</p>
<table>
<thead>
<tr>
<th>key</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>k1</td>
<td>3</td>
</tr>
<tr>
<td>k2</td>
<td>2</td>
</tr>
<tr>
<td>k3</td>
<td>2</td>
</tr>
<tr>
<td>k4</td>
<td>3</td>
</tr>
</tbody>
</table>
<p><strong>4. performance analysis</strong></p>
<p>let’s explain the scala UDF in Python<br><img src="http://wx1.sinaimg.cn/mw690/761b7938ly1fjmeol1jg8j20s405odim.jpg" alt="scala udf physical plan"><br>the Project Plan is Scala UDF</p>
<p>and if we implement Python UDF as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">py_slen = udf(<span class="keyword">lambda</span> s: len(s), IntegerType())</div><div class="line">df_with_python_udf = (df.select(col(<span class="string">"key"</span>), py_slen(<span class="string">"value"</span>).alias(<span class="string">"slen"</span>)).orderBy(col(<span class="string">"slen"</span>).desc()))</div></pre></td></tr></table></figure>
<p>the Python plan is:<br><img src="http://wx4.sinaimg.cn/mw690/761b7938ly1fjmeofgzbmj210k06igot.jpg" alt="python udf physical plan"><br>the UDF plan is different, which is BatchEvalPython.<br>It can prove that when use scala UDF in python, the evaluation is in JVM and data will not exchange with Python worker. And the performance should be improved.</p>
<p>I evaluated the performance in local environment with 4cores and 2GB memory, and generated 10million rows for each test, the result is as follows:<br><img src="http://wx2.sinaimg.cn/mw690/761b7938ly1fjmer43xfnj20kw0ckt8z.jpg" alt="scala vs python string len udf"><br><strong>Scala UDF is 1.89 times Python UDF</strong></p>
<p><strong>And then I implemented another UDF in Scala and Python with regex string parsing</strong>, the performance is<br><img src="http://wx3.sinaimg.cn/mw690/761b7938ly1fjmeopknh4j20a0061748.jpg" alt="scala vs python string regex parsing"></p>
<p><strong>Scala udf is 2.23 times Python REGEX String Parsing UDF</strong></p>
<p>the Scala UDF is defined as follows:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span>  org.apache.spark.sql.functions._</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Created by lgrcyanny on 17/9/13.</div><div class="line">  */</div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">StringParse</span> </span>&#123;</div><div class="line">  <span class="keyword">val</span> <span class="type">STRING_PATTERN</span> = <span class="string">""</span><span class="string">"(a.*b)"</span><span class="string">""</span>.r</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parseString</span></span>(str: <span class="type">String</span>): <span class="type">String</span> = &#123;</div><div class="line">    <span class="keyword">val</span> matched = <span class="type">STRING_PATTERN</span>.findFirstMatchIn(str)</div><div class="line">    <span class="keyword">if</span> (matched.isEmpty) &#123;</div><div class="line">      <span class="string">""</span></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      matched.get.group(<span class="number">1</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getFun</span></span>() = udf(parseString _ )</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Python string parse UDF  vs Scala UDF:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</div><div class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> Column</div><div class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_java_column</div><div class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_seq</div><div class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col</div><div class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</div><div class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> length</div><div class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</div><div class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_word</span><span class="params">(length)</span>:</span></div><div class="line">    <span class="string">"""get random word for generate rows"""</span></div><div class="line">    letters = string.ascii_lowercase</div><div class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([random.choice(letters) <span class="keyword">for</span> i <span class="keyword">in</span> range(length)])</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_rows</span><span class="params">(n)</span>:</span></div><div class="line">    <span class="string">"""generate rows in key value pair"""</span></div><div class="line">    <span class="comment"># generate rows</span></div><div class="line">    letters = <span class="string">"abcdefghijklmnopqrstuvwxyz"</span></div><div class="line">    rows = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">        id = random.randint(<span class="number">0</span>, <span class="number">100</span>)</div><div class="line">        slen = random.randint(<span class="number">0</span>, <span class="number">20</span>)</div><div class="line">        word = random_word(slen)</div><div class="line">        rows.append((id, letters))</div><div class="line">    <span class="keyword">return</span> rows</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_parse</span><span class="params">(col)</span>:</span></div><div class="line">    <span class="string">"""scala udf parse string"""</span></div><div class="line">    _string_parse = sc._jvm.com.learning.StringParse.getFun()</div><div class="line">    <span class="keyword">return</span> Column(_string_parse.apply(_to_seq(sc, [col], _to_java_column)))</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_regex_udf</span><span class="params">(n=<span class="number">1000</span>)</span>:</span></div><div class="line">    <span class="string">"""test udf with regex parse"""</span></div><div class="line">    rows = generate_rows(n)</div><div class="line">    df = spark.createDataFrame(rows, [<span class="string">'key'</span>, <span class="string">'value'</span>])</div><div class="line">    df.show(<span class="number">20</span>)</div><div class="line">    pattern = re.compile(<span class="string">r"(a.*b)"</span>)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_string</span><span class="params">(str)</span>:</span></div><div class="line">        <span class="string">"""parse string with python regex"""</span></div><div class="line">        matched = re.search(pattern, str)</div><div class="line">        <span class="keyword">if</span> matched:</div><div class="line">            <span class="keyword">return</span> matched.group(<span class="number">1</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> <span class="string">""</span></div><div class="line">    py_parse_str = udf(parse_string, StringType())</div><div class="line">    start_time = time.time()</div><div class="line">    df_with_python_udf = (df.select(col(<span class="string">"key"</span>), py_parse_str(col(<span class="string">"value"</span>)).alias(<span class="string">"parsed_value"</span>))</div><div class="line">                          .filter(length(col(<span class="string">"parsed_value"</span>)) &gt; <span class="number">0</span>))</div><div class="line">    df_with_python_udf.explain(<span class="keyword">True</span>)</div><div class="line">    df_with_python_udf.show()</div><div class="line">    print(<span class="string">"matched rows: &#123;&#125;"</span>.format(df_with_python_udf.count()))</div><div class="line">    print(<span class="string">"duration for python regex parse: &#123;&#125;s"</span>.format(time.time() - start_time))</div><div class="line"></div><div class="line">    start_time = time.time()</div><div class="line">    df_with_scala_udf = (df.select(col(<span class="string">"key"</span>), string_parse(col(<span class="string">"value"</span>)).alias(<span class="string">"parsed_value"</span>))</div><div class="line">                          .filter(length(col(<span class="string">"parsed_value"</span>)) &gt; <span class="number">0</span>))</div><div class="line">    df_with_python_udf.explain(<span class="keyword">True</span>)</div><div class="line">    df_with_scala_udf.show()</div><div class="line">    print(<span class="string">"matched rows: &#123;&#125;"</span>.format(df_with_scala_udf.count()))</div><div class="line">    print(<span class="string">"duration for scala regex parse: &#123;&#125;s"</span>.format(time.time() - start_time))</div></pre></td></tr></table></figure>
<p><strong>5. Conclusion</strong></p>
<p>Databricks used to give a performance for Python vs Scala DataFrame and RDD API:<br><img src="http://wx2.sinaimg.cn/mw690/761b7938ly1fjmeo7vk38j210c0fy48j.jpg" alt="databricks performance"></p>
<p>the blog is <a href="https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html" target="_blank" rel="external">here</a>.<br>The performance is a running group-aggregation on 10 million integer pairs on a single machince. The Scala DF is almost 5 times Python lambda function in RDD Python.</p>
<p>Even though, the Scala UDF is not 5 times Python UDF, about 2 times in my test, using scala UDF can improve performance indeed.</p>
<h2 id="Use-Scala-UDAF-in-PySpark"><a href="#Use-Scala-UDAF-in-PySpark" class="headerlink" title="Use Scala UDAF in PySpark"></a>Use Scala UDAF in PySpark</h2><p>UDAF now only supports defined in Scala and Java(spark 2.0)</p>
<p><strong>1. define scala UDAF</strong></p>
<p>when define UDAF, it must extend class <code>UserDefinedAggregateFunction</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">ArrayType</span>, <span class="type">DataType</span>, <span class="type">StringType</span>, <span class="type">StructType</span>&#125;</div><div class="line"> </div><div class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">GroupConcat</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">"s"</span>, <span class="type">StringType</span>)</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">"buff"</span>, <span class="type">ArrayType</span>(<span class="type">StringType</span>))</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">StringType</span></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    buffer.update(<span class="number">0</span>, <span class="type">ArrayBuffer</span>.empty[<span class="type">String</span>])</div><div class="line">  &#125;</div><div class="line"> </div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</div><div class="line">      buffer.update(<span class="number">0</span>, buffer.getSeq[<span class="type">String</span>](<span class="number">0</span>) :+ input.getString(<span class="number">0</span>))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"> </div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    buffer1.update(<span class="number">0</span>, buffer1.getSeq[<span class="type">String</span>](<span class="number">0</span>) ++ buffer2.getSeq[<span class="type">String</span>](<span class="number">0</span>))</div><div class="line">  &#125;</div><div class="line"> </div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</div><div class="line">    buffer.getSeq[<span class="type">String</span>](<span class="number">0</span>).mkString(<span class="string">","</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>2. use UDAF in python</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</div><div class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> Column</div><div class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_java_column</div><div class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_seq</div><div class="line"> </div><div class="line">spark = SparkSession.builder.appName(<span class="string">"scala_udf_test"</span>).getOrCreate()</div><div class="line">sc = spark.sparkContext</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_concat</span><span class="params">(col)</span>:</span></div><div class="line">    _groupConcat = sc._jvm.com.learning.GroupConcat.apply</div><div class="line">    <span class="keyword">return</span> Column(_groupConcat(_to_seq(sc, [col], _to_java_column)))</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">()</span>:</span></div><div class="line">    rows = [</div><div class="line">        (<span class="string">"k1"</span>, <span class="string">"a"</span>),</div><div class="line">        (<span class="string">"k1"</span>, <span class="string">"b"</span>),</div><div class="line">        (<span class="string">"k1"</span>, <span class="string">"c"</span>),</div><div class="line">        (<span class="string">"k2"</span>, <span class="string">"d"</span>),</div><div class="line">        (<span class="string">"k3"</span>, <span class="string">"e"</span>),</div><div class="line">        (<span class="string">"k3"</span>, <span class="string">"f"</span>),</div><div class="line">    ]</div><div class="line">    df = spark.createDataFrame(rows, [<span class="string">'key'</span>, <span class="string">'value'</span>])</div><div class="line">    df.show(<span class="number">50</span>)</div><div class="line">    df.groupBy(<span class="string">"key"</span>).agg(group_concat(<span class="string">"value"</span>).alias(<span class="string">"concat"</span>)).show()</div><div class="line"> </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    process()</div></pre></td></tr></table></figure>
<p><strong>3. submit the app</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit --jars testing/learning-1.0.0-SNAPSHOT.jar udf_test.py</div></pre></td></tr></table></figure>
<p>the output would be:</p>
<table>
<thead>
<tr>
<th>key</th>
<th>cancat</th>
</tr>
</thead>
<tbody>
<tr>
<td>k1</td>
<td>a,b,c</td>
</tr>
<tr>
<td>k2</td>
<td>d</td>
</tr>
<tr>
<td>k3</td>
<td>e,f</td>
</tr>
</tbody>
</table>
<p><strong>4. references</strong></p>
<ul>
<li><a href="https://stackoverflow.com/questions/31640729/spark-sql-replacement-for-mysql-group-concat-aggregate-function" target="_blank" rel="external">spark-sql-replacement-for-mysql-group-concat-aggregate-function</a></li>
<li><a href="https://stackoverflow.com/questions/33233737/spark-how-to-map-python-with-scala-or-java-user-defined-functions" target="_blank" rel="external">spark-how-to-map-python-with-scala-or-java-user-defined-functions</a></li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/spark/">spark</a>
    </span>
    

    </div>

    
  </div>
</article>

  
	<section id="comments" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'lgrcyanny';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>





    </main>

    <footer class="site-footer">
  <p class="site-info">
    Copyright
    </br>
    
    &copy; 2018 Cyanny Liang
    
  </p>
</footer>
    
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-40624708-1', 'auto');
    ga('send', 'pageview');

</script>

  </div>
</div>
</body>
</html>