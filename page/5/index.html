<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>CyannyLive</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Keep Learning and Writing">
<meta property="og:type" content="website">
<meta property="og:title" content="CyannyLive">
<meta property="og:url" content="http://www.cyanny.com/page/5/index.html">
<meta property="og:site_name" content="CyannyLive">
<meta property="og:description" content="Keep Learning and Writing">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CyannyLive">
<meta name="twitter:description" content="Keep Learning and Writing">
<meta name="twitter:creator" content="@lgrcyanny">
<link rel="publisher" href="lgrcyanny">
<meta property="fb:admins" content="lgrcyanny">
<meta property="fb:app_id" content="lgrcyanny">
  
  
    <link rel="icon" href="favicon.ico">
  
  <link href='//fonts.useso.com/css?family=Open+Sans:400italic,400,600' rel='stylesheet' type='text/css'>
  <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css" type="text/css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-40624708-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
</head>
<body>
  <div id="container">
    <header id="header">
  <div id="header-main" class="header-inner">
    <div class="outer">
      <a href="/." id="logo"><i class="logo"></i><span class="site-title">CyannyLive</span></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/.">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/jianyinjietu">剪影截图</a>
        
      </nav>
      
      <nav id="sub-nav">
        <div class="profile" id="profile-nav">
          <a id="profile-anchor" href="javascript:;"><img class="avatar" src="/css/images/avatar.png"><i class="fa fa-caret-down"></i></a>
        </div>
      </nav>
      
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"> </button><input type="hidden" name="sitesearch" value="http://www.cyanny.com"></form>
      </div>
    </div>
  </div>
  <div id="main-nav-mobile" class="header-sub header-inner">
    <table class="menu outer">
      <tr>
      
        <td><a class="main-nav-link" href="/.">Home</a></td>
      
        <td><a class="main-nav-link" href="/archives">Archives</a></td>
      
        <td><a class="main-nav-link" href="/categories">Categories</a></td>
      
        <td><a class="main-nav-link" href="/tags">Tags</a></td>
      
        <td><a class="main-nav-link" href="/jianyinjietu">剪影截图</a></td>
      
      <td>
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><input type="hidden" name="sitesearch" value="http://www.cyanny.com"></form>
      </td>
      </tr>
    </table>
  </div>
</header>

    <div class="outer">
      
        <aside id="profile">
  <div class="inner profile-inner">
  	<div class="base-info profile-block">
		  <img id="avatar" src="/css/images/avatar.png">
      <h2 id="name">Cyanny Liang</h2>
      <h3 id="title">Big Data &amp; Distributed Computing</h3>
      <span id="location"><i class="fa fa-map-marker"></i>Beijing, China</span>
      <a id="follow" href="https://github.com/lgrcyanny/">FOLLOW</a>
  	</div>
    <div class="article-info profile-block">
      <div class="article-info-block">
        50
        <span>posts</span>
      </div>
      <div class="article-info-block">
        30
        <span>tags</span>
      </div>
    </div>
    
    <div class="contact-info profile-block">
      <table class="contact-list">
        <tr>
        
          <td><a href="http://github.com/lgrcyanny" target="_blank" title="github"><i class="fa fa-github"></i></a></td>
        
          <td><a href="https://twitter.com/lgrcyanny" target="_blank" title="twitter"><i class="fa fa-twitter"></i></a></td>
        
          <td><a href="https://www.facebook.com/CyannyLIANG" target="_blank" title="facebook"><i class="fa fa-facebook"></i></a></td>
        
          <td><a href="/atom.xml" target="_blank" title="rss"><i class="fa fa-rss"></i></a></td>
        
        </tr>
      </table>
    </div>
    
  </div>
</aside>
      
      <section id="main">
      <article id="post-hadoop-hdfs-high-availability" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/12/05/hadoop-hdfs-high-availability/">Hadoop HDFS High Availability(HA)</a>
    </h1>
  

        <div class="article-meta">
          <div class="article-date">
  <i class="fa fa-calendar"></i>
  <a href="/2013/12/05/hadoop-hdfs-high-availability/">
    <time datetime="2013-12-05T06:54:59.000Z" itemprop="datePublished">2013-12-05</time>
  </a>
</div>
          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/hadoop/">Hadoop</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Reliability, Scalability and Availability are the most important three features for a distributed file system. In HDFS, persistent metadata, using the Secondary NameNode to create checkpoint against data loss, using block replication across the cluster against DataNode failure, all of these brings HDFS high reliability. And the master slave architecture wins HDFS great scalability. [more…]<br>However, HDFS has always had a well-known Single Point of Failure (SPOF) which impacts HDFS’s availability. HDFS fits well for ETL or batch-processing workflows, but in the past few years HDFS begin to be used for real time job, such as HBase. To recover a single NameNode, an administrator starts a new primary NameNode with FsImage and replays its EditLog, waits for Blockreport from DataNodes to leave safe mode. On large clusters with many files and blocks, it will take 30 minutes or more time to recovery. Long time recovery will impact the productivity of internal users and perhaps results in downtime visible to external users.<br>For these reasons, Hadoop community began a new feature for HDFS called High Availability Name Node (HA Name Node) in 2011. The HA makes use of an active and a standby NameNode. When the active NameNode fails, the hot standby NameNode will take over serving the role of an active NameNode without a significant interruption. </p>
<h3 id="HA_Architecture">HA Architecture</h3><p><a href="http://cyanny/myblog/wp-content/uploads/2013/11/hd5.1.png" target="_blank" rel="external"><img src="http://cyanny/myblog/wp-content/uploads/2013/11/hd5.1.png" alt="hd5.1">Figure1 HDFS High Availability Architecture</a><br>As shown in Figure 1, HA architecture has several changes</p>
<ul>
<li>The NameNodes must use highly available shared storage to share EditLog, such as Network File System (NFS), or BookKeeper. The active NameNode will write its EditLog on the Shared dir, and the Standby NameNode polls the shared log frequently and then applies to its in-memory so as to has the most complete and up-to-date file system state in memory.</li>
<li>DataNodes must send Blockreports with block locations to both active and standy NameNodes. Because the block mappings are stored in a NameNode’s memory and not on a disk to increase data access performance.</li>
<li>Clients must be configured to handle NameNode failover, using a mechanism that is transparent to users.<br>The HA guarantees if the active NameNode fails, the standby one will take over quickly in tens of seconds, since it has the latest state available in memory: the latest EditLog entries and an up-to-date block mapping.</li>
</ul>
<h3 id="Failover">Failover</h3><p>The transition from the active NameNode to the standby NameNode is controlled by the failover controller. Each NameNode runs a failover controller to monitor its namenode failure, which is a heartbeating mechanism. Failover is triggered when a NameNode fails.  </p>
<p>After NameNode failover, the Client must be informed which is active NameNode now. Client Failover is handled transparently by the client library. The HDFS client supports the configuration for multiple network addresses, one for each NameNode. The NameNode is identified by a single logical URI which is mapped the two network addresses of the HA Name Nodes via client-side configuration. The client will retry the two addresses until the active NameNode is found. </p>
<h3 id="Fencing">Fencing</h3><p>After Failover, HDFS makes use of the fencing technique to make sure that the previous died active NameNode is prevented from doing any damage and causing corruption. These mechanisms includes killing the NameNode’s process, revoking its access to the shared storage directory and disabling its network port via a remote management command. If these techniques fail, HDFS will use STONITH, that is “shoot the other node in the head”, which uses a specialized power distribution unit to forcibly power down the host machine. </p>
<p>Resources:<br><a href="http://cyanny/myblog/2013/12/05/hadoop-overview/" title="Hadoop Overview" target="_blank" rel="external">Part 0 Hadoop Overview</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-hdfs-review/" title="Hadoop HDFS Review" target="_blank" rel="external">Part 1 Hadoop HDFS Review</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-hdfs-federation/" title="Hadoop HDFS Federation" target="_blank" rel="external">Part 2 Hadoop HDFS Federation</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-hdfs-high-availability/" title="Hadoop HDFS High Availability(HA)" target="_blank" rel="external">Part 3 Hadoop HDFS High Availability(HA)</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-mapreduce-overview/" title="Hadoop MapReduce Overview" target="_blank" rel="external">Part 4 Hadoop MapReduce Overview</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-mapreduce-1-framework/" title="Hadoop MapReduce 1 Framework" target="_blank" rel="external">Part 5 Hadoop MapReduce 1 Framework</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-mapreduce-2-yarn/" title="Hadoop MapReduce 2 (YARN)" target="_blank" rel="external">Part 6 Hadoop MapReduce 2 (YARN)</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-isnt-silver-bullet/" title="Hadoop isn’t Silver Bullet" target="_blank" rel="external">Part 7 Hadoop isn’t Silver Bullet</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.cyanny.com/2013/12/05/hadoop-hdfs-high-availability/" data-id="citktnrqe004pee6rkscnyvzl" class="article-share-link">Share</a>
      
        <a href="http://www.cyanny.com/2013/12/05/hadoop-hdfs-high-availability/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/learning/">Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/research/">Research</a></li></ul>

    </footer>
  </div>
  
</article>


    
      <article id="post-hadoop-hdfs-federation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/12/05/hadoop-hdfs-federation/">Hadoop HDFS Federation </a>
    </h1>
  

        <div class="article-meta">
          <div class="article-date">
  <i class="fa fa-calendar"></i>
  <a href="/2013/12/05/hadoop-hdfs-federation/">
    <time datetime="2013-12-05T06:48:10.000Z" itemprop="datePublished">2013-12-05</time>
  </a>
</div>
          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/hadoop/">Hadoop</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>In traditional HDFS architecture, there is only one NameNode in a cluster, which maintains all of the namespace and block map. Regarding that Hadoop cluster is becoming larger and larger one enterprise platform and every file and block information is in NameNode RAM, when there are more than 4000 nodes with many files, the NameNode memory will reach its limit and it becomes the limiting factor for cluster scaling. In Hadoop 2.x release series, Hadoop introduces HDFS Federation, which allows a cluster to scale by adding NameNodes, each of which manages a portion of the filesystem namespace.[more…]<br><a href="http://cyanny/myblog/wp-content/uploads/2013/11/hd4.1.png" target="_blank" rel="external"><img src="http://cyanny/myblog/wp-content/uploads/2013/11/hd4.1.png" alt="hd4.1">Figure1 HDFS Federation</a><br>As shown in Figure1, there are many federated NameNodes which manages its own namespace. The NameNodes are independent and don’t require coordination with each other. The DataNodes are used as common storage for blocks. Each DataNode registers with all the NameNodes in the cluster and send periodic heartbeats and block report and handles commands from the NameNodes.<br>Each NameNode is responsible for two tasks: Namespace management and Block Management. For the two tasks, mange NameNode manages its own Namespace Volume, which consists of two parts:</p>
<ul>
<li>Namespace: the metadata for each file and block</li>
<li>Block Pool: it is a set of blocks that belong to a single namespace. The Block Pool is in charge of block management task, including processing block reports and maintaining location of blocks.<br>Block pool storage is not partitioned , so DataNodes register with each NameNode in the cluster and store blocks from multiple block pools. The NameNode namespace must to generate block ID for new blocks.<br>A NameSpace Volume is a self-contained uinit . Each NameNode has no need to contact other NameNode and it’s failure will not influence other NameNode.<br>To access a federated HDFS, clients use client-side mount tables to map file paths to NameNodes. A new identifier ClusterID is added to all the nodes in the cluser. Federated NameNodes is configured by ViewFileSystem and the viewfs://URIs.<br>The HDFS Federation brings several benefits:</li>
</ul>
<ul>
<li>NameSpace Scalability: It will support large clusters with many files, just add more NameNode to scale namespace.</li>
<li>Performance: Break the limitation of single node, more NameNodes means more read/write operations and the throughput is improved.</li>
<li>Isolation: A single NameNode has no isolation in multi user environment. While multiple NameNodes can provide isolated NameSpace for both production and experiment application. </li>
</ul>
<p>Resources:<br><a href="http://cyanny/myblog/2013/12/05/hadoop-overview/" title="Hadoop Overview" target="_blank" rel="external">Part 0 Hadoop Overview</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-hdfs-review/" title="Hadoop HDFS Review" target="_blank" rel="external">Part 1 Hadoop HDFS Review</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-hdfs-federation/" title="Hadoop HDFS Federation" target="_blank" rel="external">Part 2 Hadoop HDFS Federation</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-hdfs-high-availability/" title="Hadoop HDFS High Availability(HA)" target="_blank" rel="external">Part 3 Hadoop HDFS High Availability(HA)</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-mapreduce-overview/" title="Hadoop MapReduce Overview" target="_blank" rel="external">Part 4 Hadoop MapReduce Overview</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-mapreduce-1-framework/" title="Hadoop MapReduce 1 Framework" target="_blank" rel="external">Part 5 Hadoop MapReduce 1 Framework</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-mapreduce-2-yarn/" title="Hadoop MapReduce 2 (YARN)" target="_blank" rel="external">Part 6 Hadoop MapReduce 2 (YARN)</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-isnt-silver-bullet/" title="Hadoop isn’t Silver Bullet" target="_blank" rel="external">Part 7 Hadoop isn’t Silver Bullet</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.cyanny.com/2013/12/05/hadoop-hdfs-federation/" data-id="citktnrqi004uee6rm0d0dcdk" class="article-share-link">Share</a>
      
        <a href="http://www.cyanny.com/2013/12/05/hadoop-hdfs-federation/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/learning/">Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/research/">Research</a></li></ul>

    </footer>
  </div>
  
</article>


    
      <article id="post-hadoop-hdfs-review" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/12/05/hadoop-hdfs-review/">Hadoop HDFS Review</a>
    </h1>
  

        <div class="article-meta">
          <div class="article-date">
  <i class="fa fa-calendar"></i>
  <a href="/2013/12/05/hadoop-hdfs-review/">
    <time datetime="2013-12-05T04:00:32.000Z" itemprop="datePublished">2013-12-05</time>
  </a>
</div>
          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/hadoop/">Hadoop</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="HDFS_Basic_Concepts">HDFS Basic Concepts</h3><p>Hadoop Distributed File System, know as HDFS, is a distributed file system designed to store large data sets and streaming data sets on commodity hardware with high scalability, reliability and availability. </p>
<p>HDFS is written in Java based on the Google File System (GFS). HDFS has many advantages compared with other distributed file systems:</p>
<p><strong>1. Highly fault-tolerant</strong></p>
<p>Fault-tolerant is the core architecture for HDFS. Since HDFS can run on low-cost and unreliable hardware, the hardware has a non-trivial probability of failures. HDFS is designed to carry on working without a noticeable interruption to the user in the face of such failure. HDFS provides redundant storage for massive amounts of data and Heartbeat for failure detection.<br>When one node fails, the master will detect it and re-assign the work to a different node. Restarting a node doesn’t need communicating with other data node. When failed node restart it will be added the system automatically. If a node appears to run slowly, the master can redundantly execute another instance of the same task, know as ‘speculative execution’.[more…]</p>
<p><strong>2. Streaming Data Access</strong></p>
<p>HDFS is designed with the most efficient data processing pattern: a write once, read-many times pattern. HDFS split large files into blocks, usually 64MB or 128MB. HDFS performs best with a modest number of large files. It prefers millions, rather than billions of files, and each of which is 100MB or more. No random writes to files is allowed. Also HDFS is optimized for large, steaming reads of files. No Random reads is allowed. </p>
<p>A data set is generated and copied from source and replicated spread the HDFS system. It is efficient to load data from HDFS for big data analysis.</p>
<p><strong>3. Large data sets</strong></p>
<p>Large data means files that are MB, GB or TB in size. There are Hadoop clusters today that store PB data. HDFS support high aggregate data bandwidth and scale to hundreds of nodes in a single cluster. It also supports tens of millions of files processing.</p>
<h3 id="HDFS_NameNode_and_DataNodes_Architecture">HDFS NameNode and DataNodes Architecture</h3><p>HDFS has a master/slave architecture. As shown in Figure1. </p>
<p>The <strong>master node</strong> is the NameNode, which managers the file system namespace, file metadata and regulates the access interface for files by clients. A cluster has only one NameNode, which maintain the map of file metadata and  the location of blocks. </p>
<p>The <strong>slave nodes</strong> are the DataNodes. A cluster has many DataNodes, which holds the actual data blocks.</p>
<p><a href="http://cyanny/myblog/wp-content/uploads/2013/11/hd3.1.png" target="_blank" rel="external"><img src="http://cyanny/myblog/wp-content/uploads/2013/11/hd3.1.png" alt="hd3.1">Figure1 HDFS Architecture</a></p>
<h3 id="NameNode">NameNode</h3><p>NameNode maintains the namespace tree, which is logical location and the mapping of file blocks to DataNodes, which is the physical location.</p>
<p>The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It provides POSIX interface for client, so that user can access HDFS data with Unix like commands and no need to know about the function of NameNode and DataNodes. It is kind of abstraction which decrease the complexity for data access. </p>
<p>The NameNode is the pivot in a cluster. A single NameNode greatly simplifies the architecture of HDFS. NameNode holds all of its metadata in RAM for fast access. It keeps a record of change on disk for crash recovery.</p>
<p>However, once the NameNode fails, the cluster fails. The Single Point of Failure, known as SPOF is really a bottleneck for NameNode. Hadoop 2.0 introduces NameNode Federation and High Availability to solve the problem. We will discuss these in later section.</p>
<h3 id="NameNode_Data_Persistent">NameNode Data Persistent</h3><p>In case of NameNode crash, the namespace information and metadata updates are stored persistently on the local disk in the form of two files: the namespace image called <strong>FsImage</strong> and the <strong>EditLog</strong>.</p>
<p>FsImage is an image file persistently stores the file system namespace, including the file system tree and the metadata for all the files and the directories in the tree, the mapping of blocks to files and file system properties.</p>
<p>EditLog is a transaction log to persistently record every change that occurs to file system metadata.</p>
<p>However, we have to know there is one thing that is not persistent. The block locations we can call it Blockmap, which is stored in NameNode in-memory, not persistent on the local disk. Blockmap is reconstructed from DataNodes when the system starts by the Blockreport of DataNodes.</p>
<p>When the system starts up, NameNode will load FsImage and EditLog from the local disk, applies the transactions from the EditLog to the in-memory representation of the FsImage, and flushes out the new version of FsImage on disk. Then it truncates the old EditLog since its transactions has been applied to the FsImage. This process is called a <strong>checkpoint</strong>.</p>
<h3 id="Secondary_NameNode">Secondary NameNode</h3><p>To increase the reliability and availability of the NameNode, a separate daemon known as the Secondary NameNode takes care of some <strong>housekeeping</strong> tasks for the NameNode. Be careful that the Secondary NameNode is not a backup or a hot standby NameNode.</p>
<p>The housekeeping wok is to periodically merge the namespace image FsImage with the EditLog to prevent the EditLog becoming to large. The Secondary NameNode runs on a single Node, which is a separate physical machine with as much memory and CPU requirements as the NameNode.</p>
<p>The Secondary NameNode keeps a copy of the merged namespace image in case of the NameNode fails. But the time lags will result in data loss certainly. And during the NameNode recovery, the reconstruction of the Blockmap will cost too much time. So Hadoop works out the problem with Hadoop High Availability solution.</p>
<h3 id="DataNodes">DataNodes</h3><p>The DataNodes holds all the actual data blocks. In sum up, it has three functions: </p>
<ul>
<li>Serves read and requests from the file system clients.</li>
<li>Provides block operations, like creation, deletion and replication upon instruction from the NameNode.</li>
<li>Make data Blockreport to NameNode periodically with lists of blocks that they are storing.</li>
</ul>
<p>In enterprise Hadoop deployment, each DataNode is a java program run on a separate JVM, and one instance of DataNode on one machine. </p>
<p>In addition, NameNode is a java program run on a single machine. Written in java provides Hadoop good portability.</p>
<h3 id="The_Communication_Protocols">The Communication Protocols</h3><p>HDFS client, NameNode and DataNodes communication protocols are TCP/IP. Client Protocol is TCP connection between a client and NameNode. The DataNode Protocol is the connection between the NameNode and the DataNodes. HDFS makes an abstraction wraps for the Client Protocol and the DataNode Protocol, which called Remote Procedure Call (RPC). NameNode never initiates any RPCs, only responds to RPC requests from clients or DataNodes.</p>
<h3 id="HDFS_Files_Organization">HDFS Files Organization</h3><p>In traditional concepts, a disk has a block size, which is the minimum amount of data that it can read or write. Disk blocks are normally 512 bytes. While HDFS has the concept of block as well, which is a much larger unit – 64MB by default.</p>
<p>HDFS large files are chopped up into 64MB or 128MB blocks. This brings several benefits:</p>
<ul>
<li>It can take advantage of any of the disks in the cluster, when the file is larger than any single disk.</li>
<li>Making the unit of abstraction of a block simplifies the storage subsystem. HDFS will deal with blocks, rather than a file. Since blocks are a fixed size, it’s easy to calculate how many can be stored on a give disk and eliminate metadata concerns.</li>
<li>Normally, a map task will operate on one local block at a time. Bocks spare the complexity of dealing with files.</li>
<li>It’s easy to do data replication with blocks for providing fault tolerance and availability. Each block is replicated multiple times. Default is to replicate each block 3 times. Replicas are stored on different nodes.</li>
<li>Block data fits well for streaming data. Files are written once and read many times. Blocks minimize the cost of seeking files.</li>
</ul>
<p>Although files are split into 64MB or 128MB blocks ， if a file is smaller than this full 64MB/128MB will not be split.</p>
<h3 id="HDFS_Data_Replication">HDFS Data Replication</h3><p>HDFS each data block has a replica factor, which indicates how many times it should be replicated. Normally, the replica factor is three. Each block is replicated three times and spread on three different machines across the cluster. This provides efficient MapReduce processing because of good data locality.<br><a href="http://cyanny/myblog/wp-content/uploads/2013/11/hd3.2.png" target="_blank" rel="external"><img src="http://cyanny/myblog/wp-content/uploads/2013/11/hd3.2.png" alt="hd3.2">Figure2 Block Replication</a><br>As shown in Figure2, the NameNode holds the metadata for each map of file and blocks, including filename, replica factor and block-id etc. Block data are spread on different DataNodes.</p>
<h3 id="Data_Replica_Placement">Data Replica Placement</h3><p>Block replica placement is not random. Regarding of the reliability and the performance, HDFS policy is to put one replica on one node in the local rack, the second one on a node in a different remote rack and the third one on a different node in the same remote rack.<br><a href="http://cyanny/myblog/wp-content/uploads/2013/11/hd3.3.png" target="_blank" rel="external"><img src="http://cyanny/myblog/wp-content/uploads/2013/11/hd3.3.png" alt="hd3.3">Figure 3 HDFS Racks</a><br>As shown in Figure 3, different DataNodes on different racks, Rack 0 and Rack1. Large HDFS instance run on a cluster of machines that commonly spread across many racks. Network bandwidth for intra-racks is greater than inter-racks. So the HDFS replica policy cuts the inter-rack write traffic and improves write performance. One third of replicas are on one node, two third of replicas are on one rack, the other third are distributed across the remaining racks. This policy guarantees the reliability.</p>
<h3 id="Data_Replication_Pipeline">Data Replication Pipeline</h3><p><a href="http://cyanny/myblog/wp-content/uploads/2013/11/hd3.4.png" target="_blank" rel="external"><img src="http://cyanny/myblog/wp-content/uploads/2013/11/hd3.4.png" alt="hd3.4">Figure 4 Data Replication Pipeline</a><br>As shown in Figure 4, it is the data flow of writing data to HDFS. A client request to request a file does not reach the NameNode immediately. It will follow the steps:<br>Step 1, the HDFS client caches the file data into a temporary local file until the local file accumulates data worth over one HDFS block size.<br>Step 2, the client contacts the NameNode, requests add a File.<br>Step 3, the NameNode inserts the file name into the file system tree and allocates a data block for it. Then responds to the client request with the identity of the DataNodes and the destination data block.<br>Step 4, suppose the HDFS file has a replication factor of three. The client retrieves a list of DataNodes, which contains that will host a replica of that block. The clients flushes the block of data from the local temporary file to the first DataNode.<br>Step 5,.the first DataNode starts receiving the data in small portions like 4KB, writes each portion to its local repository and transfers that portion to the second DataNode. Then the second DataNode retrieves the data portion, stores it and transfers to the third DataNode. Data is pipelined from the first DataNode to the third one.<br>Step 6, when a file is closed, the remaining un-flushed data in temporary local file is transferred to the DataNode. Then the client tells the NameNode that the file is closed.<br>Step 7, the NameNode commits the file creation operation into a persistent one. Be careful if the NameNode dies before the file is closed, the file is lost.<br>So far, we can see that file caching policy improves the writing performance. HDFS is write-once-read-many-times. When a client wants to read a file: It should contacts the NameNode to retrieves the file block map, including block id, block physical location. Then it communicates directly with the DataNodes to read data. The NameNode will not be a bottleneck for data transfer.</p>
<p>Resources:<br><a href="http://cyanny/myblog/2013/12/05/hadoop-overview/" title="Hadoop Overview" target="_blank" rel="external">Part 0 Hadoop Overview</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-hdfs-review/" title="Hadoop HDFS Review" target="_blank" rel="external">Part 1 Hadoop HDFS Review</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-hdfs-federation/" title="Hadoop HDFS Federation" target="_blank" rel="external">Part 2 Hadoop HDFS Federation</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-hdfs-high-availability/" title="Hadoop HDFS High Availability(HA)" target="_blank" rel="external">Part 3 Hadoop HDFS High Availability(HA)</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-mapreduce-overview/" title="Hadoop MapReduce Overview" target="_blank" rel="external">Part 4 Hadoop MapReduce Overview</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-mapreduce-1-framework/" title="Hadoop MapReduce 1 Framework" target="_blank" rel="external">Part 5 Hadoop MapReduce 1 Framework</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-mapreduce-2-yarn/" title="Hadoop MapReduce 2 (YARN)" target="_blank" rel="external">Part 6 Hadoop MapReduce 2 (YARN)</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-isnt-silver-bullet/" title="Hadoop isn’t Silver Bullet" target="_blank" rel="external">Part 7 Hadoop isn’t Silver Bullet</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.cyanny.com/2013/12/05/hadoop-hdfs-review/" data-id="citktnrq8004kee6r4a4y2wxd" class="article-share-link">Share</a>
      
        <a href="http://www.cyanny.com/2013/12/05/hadoop-hdfs-review/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/learning/">Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/research/">Research</a></li></ul>

    </footer>
  </div>
  
</article>


    
      <article id="post-hadoop-overview" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/12/05/hadoop-overview/">Hadoop Overview</a>
    </h1>
  

        <div class="article-meta">
          <div class="article-date">
  <i class="fa fa-calendar"></i>
  <a href="/2013/12/05/hadoop-overview/">
    <time datetime="2013-12-05T03:36:08.000Z" itemprop="datePublished">2013-12-05</time>
  </a>
</div>
          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/hadoop/">Hadoop</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="The_motivation_for_Hadoop">The motivation for Hadoop</h3><p>Apache Hadoop is an open source distributed computing framework for large-scale data sets processing. Doug Cutting, who is the creator of Apache Lucene Project, creates it. </p>
<p>The name of Hadoop is a made-up name, which is the nickname of a stuffed yellow elephant of the kid of Doug. Hadoop has its origins in Apache Nutch, an open source web search engine, and it is built based on work done by Google in early 2000s, specifically on Google papers describing the Google File System (GFS) published in 2003, and MapReduce published in 2004. </p>
<p>Hadoop moved out of Nutch to form an independent in Feb. 2006. Up to now, Hadoop has been powered by many companies, such as Yahoo who claims it has the biggest Hadoop cluster in the world with more than 42000 nodes, LinkedIn who has 4100 nodes, Facebook who has 1400 nodes, Taobao who has the biggest cluster in China with more than 2000 nodes. [more…]</p>
<h3 id="The_problems_for_traditional_big_data_processing">The problems for traditional big data processing</h3><p>Why these companies adopt Hadoop? The reason is simple, that we live in a big data age. We are flooded with big data every day on the Internet. Consider that: Facebook hosts approximately 10 billion photos, taken up on PB storage. Every second on eBay, a total merchandise value of 1400 dollars is traded and 10 million new items are listed on eBay every day. Ancestry.com, the genealogy site, store around 2.5 PB of Data.</p>
<p>How to make use of these big data to make analysis? For traditional methods, use only one machine to process computation, which needs faster processor and RAM. Even though the CPU power doubles every 18 months according to Moores’s Law, it hasn’t meet the big data analysis needs. Yet distributed system evolved to allow developers to use multiple machines for a single job, like MPI, PVM and Condor. However, programing for these traditional distributed systems is complex, you have to deal with these problems:</p>
<ul>
<li>It’s difficult to deal with partial failures of the system. Developers spend more time designing for failure than they do actually working on the problem itself.</li>
<li>Finite and precious bandwidth must be available to combine data from different disks and transfer time is very slow for big data volume.</li>
<li>Data exchange requires synchronization.</li>
<li>Temporal dependencies are complicated.</li>
</ul>
<h3 id="How_can_Hadoop_save_big_data_analysis">How can Hadoop save big data analysis</h3><p>What really counts is big data. Traditional distributed computing can’t handle big data in a decent way, but Hadoop can. Lets see what Hadoop brings to us: </p>
<ul>
<li>Hadoop provide partial failure support. Hadoop Distributed File System (HDFS) can store large data sets with high reliability and scalability.</li>
<li>HDFS provide great fault tolerance. Partial Failure will not result in the failure of the entire system. And HDFS provide data recoverability for partial failure.</li>
<li>Hadoop introduce MapReduce, which spares programmers from low-level details, like partial failure. The MapReduce framework will detect failed tasks and reschedule them automatically.</li>
<li>Hadoop provide data locality. The MapReduce framework tries to collocate data with the compute nodes. Data is local, and tasks are separated with no dependence on each other. So the shared-nothing and data locality architecture can save more bandwidth and solve the complicated dependence problem</li>
</ul>
<p>In summary, Hadoop is a great big data processing tool.</p>
<h3 id="Hadoop_Basic_Concepts_and_Core_Concepts">Hadoop Basic Concepts and Core Concepts</h3><p>The core concepts for Hadoop are to distribute the data as it is initially stored in the system. That is data locality, individual nodes can work on data local to these nodes, and no data transfer over the network is required for initial processing.<br>Here are the basic concepts for Hadoop:</p>
<ul>
<li>Applications are written in high-level code. Developers don’t worry about network programming, temporal dependencies etc.</li>
<li>Nodes talk to each other as little as possible. Developers should not write code which communicates between nodes, that is ‘Shared-Nothing’ architecture.</li>
<li>Data is spread among machines in advance. Computation happens where the data is stored, wherever possible, just as near the node as possible. Data is replicated multiple times on the system for increased availability and reliability.</li>
</ul>
<h3 id="Hadoop_High-Level_Overview">Hadoop High-Level Overview</h3><p>Hadoop consists of two important components: HDFS and MapReduce.<br>HDFS is Hadoop Distributed File System, which is a distributed file system designed to store large data sets and streaming data sets on commodity hardware with high scalability, reliability and availability.</p>
<p>MapReduce is a distributed processing framework designed to operate on large data stored on HDFS, which provides a clean interface for developers.</p>
<p>A set of machines running HDFS and MapReduce is known as a Hadoop Cluster. An individual machine in a cluster is known as nodes. The nodes play different roles. There are two kind important nodes: Master Nodes and Slave Nodes.</p>
<p>There are 5 important daemons on these nodes. </p>
<p><a href="http://cyanny/myblog/wp-content/uploads/2013/11/hd2.1.png" target="_blank" rel="external"><img src="http://cyanny/myblog/wp-content/uploads/2013/11/hd2.1.png" alt="hd2.1">Figure1 Hadoop High-Level Architecture</a></p>
<p>As shown in Figure, Hadoop is comprised of 5 separate daemons:</p>
<ul>
<li>NameNode, which holds the metadata for HDFS.</li>
<li>Secondary NameNode, which performs housekeeping functions for NameNode, and isn’t a backup or hot standby for the NameNode.</li>
<li>DataNode, which stores actual HDFS data blocks. In Hadoop, a large file is split into 64M or 128M blocks.</li>
<li>JobTracker, which manages MapReduce jobs, distributes individual tasks to machines running.</li>
<li>TaskTracker, which initiates and monitors each individual Map and Reduce tasks.</li>
</ul>
<p>Each daemon runs on its own Java Virtual Machine (JVM), no Nodes can run 5 daemons at the same time.  </p>
<p>Master Nodes runs the NameNode, Secondary NameNode, JobTracker daemons. And only one of each of these daemons runs on the cluster.<br>Slave Nodes run the DataNode and TaskTracker daemons. A slave node will run both of these daemons. All of these Slave Nodes run in parallel, each on their own part of the overall dataset locally.</p>
<p>Just for very small clusters, the NameNode, JobTracker and the Secondary NameNode run on a single machine. However, when there are beyond 20-30 nodes. It’s better to run each of them on individual nodes. </p>
<h3 id="Hadoop_Ecosystem">Hadoop Ecosystem</h3><p>Hadoop has a family of related projects based on the infrastructure for distributed computing and large-scale data processing. The core projects are apache open source projects. Except for HDFS and MapReduce, some hadoop related projects are:</p>
<ul>
<li>Ambari, a Hadoop management and cluster monitoring system.</li>
<li>Avro, a serialization system for efficient, cross-language RPC and persistent data storage.</li>
<li>Pig, a data flow language and execution environment for exploring very large datasets, which runs on HDFS and MapReduce clusters.</li>
<li>HBase, provide random, realtime read/write access to BigData. The project built a column-oriented database, host very large tables —— billions of rows X millions of columns. The project is based on Google’s paper BigTable: A Distributed Storage System for Structured Data.</li>
<li>Hive, which is distributed data warehouse. Hive manages data stored in HDFS and provides a query language based on SQL, which is translated by runtime engine to MapReduce Job.</li>
<li>Zookeeper, a distributed, highly available coordination service. It provides centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services, which make build distributed applications more efficient.</li>
<li>Oozie, a service for running and scheduling workflows of Hadoop jobs, which includes MapReduce, Pig, Hive and sqoop jobs.</li>
<li>Mahout, a scalable machine learning libraries based on Hadoop HDFS and MapReduce.</li>
</ul>
<p>Resources:<br><a href="http://cyanny/myblog/2013/12/05/hadoop-overview/" title="Hadoop Overview" target="_blank" rel="external">Part 0 Hadoop Overview</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-hdfs-review/" title="Hadoop HDFS Review" target="_blank" rel="external">Part 1 Hadoop HDFS Review</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-hdfs-federation/" title="Hadoop HDFS Federation" target="_blank" rel="external">Part 2 Hadoop HDFS Federation</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-hdfs-high-availability/" title="Hadoop HDFS High Availability(HA)" target="_blank" rel="external">Part 3 Hadoop HDFS High Availability(HA)</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-mapreduce-overview/" title="Hadoop MapReduce Overview" target="_blank" rel="external">Part 4 Hadoop MapReduce Overview</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-mapreduce-1-framework/" title="Hadoop MapReduce 1 Framework" target="_blank" rel="external">Part 5 Hadoop MapReduce 1 Framework</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-mapreduce-2-yarn/" title="Hadoop MapReduce 2 (YARN)" target="_blank" rel="external">Part 6 Hadoop MapReduce 2 (YARN)</a><br><a href="http://cyanny/myblog/2013/12/05/hadoop-isnt-silver-bullet/" title="Hadoop isn’t Silver Bullet" target="_blank" rel="external">Part 7 Hadoop isn’t Silver Bullet</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.cyanny.com/2013/12/05/hadoop-overview/" data-id="citktnrp1003xee6r273qe3vb" class="article-share-link">Share</a>
      
        <a href="http://www.cyanny.com/2013/12/05/hadoop-overview/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/learning/">Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/research/">Research</a></li></ul>

    </footer>
  </div>
  
</article>


    
      <article id="post-graph-dfs-bfs" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/12/02/graph-dfs-bfs/">Graph DFS and BFS</a>
    </h1>
  

        <div class="article-meta">
          <div class="article-date">
  <i class="fa fa-calendar"></i>
  <a href="/2013/12/02/graph-dfs-bfs/">
    <time datetime="2013-12-02T14:26:47.000Z" itemprop="datePublished">2013-12-02</time>
  </a>
</div>
          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/algorithm/">Algorithm</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>图论是一个重要的部分，以前数据结构课上过，没有作为考试要求，所以很不太熟悉，尤其是没有实现。考完试，无聊中重新看了看，想起以前在eBay，某帅哥突然开玩笑说DFS和BFS嘛，当然要会呀，Cyanny你会的吧，我说忘了，其实细细想起来，还真没有写过代码。In my view，没有实现过，就等于不会，今天实现了BFS和DFS，深深赞一个算导，说得明白和透彻，现在的算法书太多，这算是一本很不错的经典，厚了点，但要是能厚着脸皮看完确实受益匪浅。</p>
<h3 id="广度优先搜索BFS">广度优先搜索BFS</h3><p>其算法思想简介：<br>1. 利用队列<br>2. 每一个Vertex引入三个变量<br>color：WHITE 表示没有访问过，GREY 表示发现，BLACK 表示完成访问<br>parent: 父亲<br>d: 表示从root到该顶点的长度<br>3. 为了实现方便，我采用int来标识每一个顶点，采用邻接表来表示graph<br>深度优先搜索可以生成深度优先树，生成单源最短路径，即从root到每一个顶点的长度是最短路径。[more…]<br>BFS可以采用邻接矩阵表示，其复杂度较高为O(n^2)<br>如果采用邻接表表示，其复杂度为O(E+V),即顶点个数+边的个数<br>[java]<br>public void BFS(int root) {<br>    vertexes = new Vertex[n];<br>    // Initialize each vertex,<br>    // Each vertex has three segment: parent, d, color<br>    for (int i = 0; i &lt; n; i++) {<br>        vertexes[i] = new Vertex(i);<br>    }</p>
<pre><code>Vertex rootv = vertexes[root];
rootv.<span class="keyword">d</span> = 0;
rootv.color = Color.GREY;
queue.add(rootv);

<span class="keyword">while</span> (!queue.isEmpty()) {
    Vertex <span class="keyword">u</span> = queue.remove(0);
    ArrayList&amp;lt;Integer&amp;gt; <span class="keyword">list</span> = <span class="keyword">graph</span>.<span class="literal">get</span>(<span class="keyword">u</span>.vertex);
    <span class="keyword">for</span> (int i = 0; i &amp;lt; <span class="keyword">list</span>.size(); i++) {
        Vertex temp = vertexes[<span class="keyword">list</span>.<span class="literal">get</span>(i)];
        <span class="comment">// Make sure each vertex enqueue only once</span>
        <span class="keyword">if</span> (temp.color == Color.WHITE) {
            temp.parent = <span class="keyword">u</span>;
            temp.<span class="keyword">d</span> = <span class="keyword">u</span>.<span class="keyword">d</span> + 1;
            temp.color = Color.GREY;
            queue.add(temp);
        }                
    }
        <span class="keyword">u</span>.color = Color.BLACK;
    res.add(<span class="keyword">u</span>);
}
</code></pre><p>}<br>[/java]<br><a href="https://github.com/lgrcyanny/Algorithm/blob/master/src/com/algorithm/graph/GraphBFS.java" title="BFS Source" target="_blank" rel="external">BFS Source Code</a></p>
<h3 id="深度优先搜索DFS">深度优先搜索DFS</h3><p>广度优先搜索DFS，基本算法思想<br>1. 采用递归，或stack来实现<br>2. 每一个Vertex引入四个变量<br>color：WHITE 表示没有访问过，GREY 表示发现，BLACK 表示完成访问<br>parent: 父亲<br>timestamp1: 表示第一次访问该顶点的时间戳<br>timestamp2:表示访问结束时的时间戳<br>3. 为了实现方便，我采用int来标识每一个顶点，采用邻接表来表示graph</p>
<p>DFS采用邻接表表示Graph来实现，复杂度O(V+E), 矩阵复杂度依然不好，为O（n^2）<br>DFS是从多个源开始，最终会生成由多个源作为root的森林，即深度优先树（深度优先森林）。<br>同时引入时间戳，是为了获得DFS的进展情况，如果不需要时间戳，只是想简单地打印每一个顶点，可以不用。</p>
<p>以下是递归的实现方法：<br>[java]<br>    public void DFS() {<br>        int i = 0;<br>        for (i = 0; i &lt; n; i++) {<br>            vertexes[i] = new Vertex(i);<br>        }<br>        for (i = 0; i &lt; n; i++) {<br>            Vertex u = vertexes[i];<br>            if (u.color == Color.WHITE) {<br>                visitDFS(u);<br>            }<br>        }<br>    }</p>
<pre><code>private void visitDFS(Vertex <span class="keyword">u</span>) {
    res.add(<span class="keyword">u</span>);
    <span class="keyword">u</span>.color = Color.GREY;
    <span class="keyword">timer</span>++;
    <span class="keyword">u</span>.timestamp1 = <span class="keyword">timer</span>;
    ArrayList&amp;lt;Integer&amp;gt; adjList = <span class="keyword">graph</span>.<span class="literal">get</span>(<span class="keyword">u</span>.vertex);
    <span class="keyword">for</span> (int i = 0; i &amp;lt; adjList.size(); i++) {
        Vertex v = vertexes[adjList.<span class="literal">get</span>(i)];
        <span class="keyword">if</span> (v.color == Color.WHITE) {
            v.parent = <span class="keyword">u</span>;
            visitDFS(v);
        }
    }
    <span class="keyword">u</span>.color = Color.BLACK;
    <span class="keyword">timer</span>++;
    <span class="keyword">u</span>.timestamp2 = <span class="keyword">timer</span>;
}
</code></pre><p>[/java]</p>
<p>以下是采用栈的实现方法改写 “visitDFS”：<br>[java]<br>    /**</p>
<pre><code><span class="comment"> * DFS with iterative method </span>
<span class="comment"> * Make use of stack</span>
<span class="comment"> * @param u</span>
<span class="comment"> */</span>
private void visitDFSIterative(Vertex <span class="keyword">u</span>) {
    res.add(<span class="keyword">u</span>);
    <span class="keyword">u</span>.color = Color.GREY;
    <span class="keyword">timer</span>++;
    <span class="keyword">u</span>.timestamp1 = <span class="keyword">timer</span>;
    <span class="keyword">stack</span>.add(<span class="keyword">u</span>);
    <span class="keyword">while</span>(!<span class="keyword">stack</span>.isEmpty()) {
        Vertex v = <span class="keyword">stack</span>.remove(<span class="keyword">stack</span>.size() - 1);
        ArrayList&amp;lt;Integer&amp;gt; adjList = <span class="keyword">graph</span>.<span class="literal">get</span>(v.vertex);
        <span class="keyword">for</span> (int i = 0; i &amp;lt; adjList.size(); i++) {
            Vertex temp = vertexes[adjList.<span class="literal">get</span>(i)];
            <span class="keyword">if</span> (temp.color == Color.WHITE) {
                temp.parent = v;
                temp.color = Color.GREY;
                <span class="keyword">timer</span>++;
                temp.timestamp1 = <span class="keyword">timer</span>;
                res.add(temp);
                <span class="keyword">stack</span>.add(temp);
            }
        }
        v.color = Color.BLACK;
        <span class="keyword">timer</span>++;
        v.timestamp2 = <span class="keyword">timer</span>;
    }
    <span class="keyword">u</span>.color = Color.BLACK;
    <span class="keyword">timer</span>++;
    <span class="keyword">u</span>.timestamp2 = <span class="keyword">timer</span>;
}
</code></pre><p>[/java]</p>
<p><a href="https://github.com/lgrcyanny/Algorithm/blob/master/src/com/algorithm/graph/GraphDFS.java" title="DFS Source" target="_blank" rel="external">DFS Source Code</a></p>
<p>最终发现，起始实现起来代码不多，重在想法。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.cyanny.com/2013/12/02/graph-dfs-bfs/" data-id="citktnrqn004zee6rk5u2pnp9" class="article-share-link">Share</a>
      
        <a href="http://www.cyanny.com/2013/12/02/graph-dfs-bfs/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/algorithm/">algorithm</a></li></ul>

    </footer>
  </div>
  
</article>


    
      <article id="post-system-analysis-design-kenneth-kendall-review-part4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/11/27/system-analysis-design-kenneth-kendall-review-part4/">System Analysis and Design (Kenneth Kendall) Review Part4</a>
    </h1>
  

        <div class="article-meta">
          <div class="article-date">
  <i class="fa fa-calendar"></i>
  <a href="/2013/11/27/system-analysis-design-kenneth-kendall-review-part4/">
    <time datetime="2013-11-27T09:45:25.000Z" itemprop="datePublished">2013-11-27</time>
  </a>
</div>
          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/system-analysis-and-design/">System Analysis and Design</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="9-_数据库设计">9. 数据库设计</h2><h3 id="9-1-基本概念">9.1.基本概念</h3><p>1.数据库与文件系统的区别<br>传统的文件系统：是扁平文件，无结构的文件，存储简单地数据，读取速度快，存储形式单一；存在数据冗余，更新时间长，不一致性，安全性的问题，冗余会带来插入、更新、删除异常。<br>数据库系统：是堆文件，采用数据表存储，数据存放随机且没有特定的顺序，每一个表需要提供主键；数据库提供索引功能；可以通过范式来保证数据的一致性；有事务处理功能,ACID；可以避免插入、更新、和删除异常；提供安全功能。[more…]</p>
<p>2.数据库设计的目标：<br>a)能向用户提供数据<br>b)准确性，一致性，完整性<br>c)有效的存储数据以及有效的更新和检索<br>d)有目的的检索数据，获取的数据要有利于管理和决策。</p>
<p>3.有效性目标<br>a)保证数据能被各种应用程序的用户共享。<br>b)维护数据的准确性，一致性，完整性<br>c)确保当前的和未来的应用程序所需的所有数据能立即可用<br>d)允许数据库随用户需求的增加而不断演进。<br>e)允许用户建立自己的数据视图，而不用关心数据的实际存储方式。</p>
<p>4.ERD图,要会画，可以看书中的例子<br>实体：任何由某人为手机数据而选择的对象或事件叫做实体。包括：一般实体，关联实体，属性实体。<br>关系：1对1，1对多，多对多<br>属性：实体的特征<br><img src="http://i40.tinypic.com/bf2o8n.jpg" alt="" title="ERD图"></p>
<p>5.键的类型：基于模式而不基于实例<br>a)主键：唯一标识一条记录<br>b)候选键：一个或一组可当作主键使用的键<br>c)辅助键：不能唯一标识一条记录，可以唯一，也可以标识多重记录<br>d)链接键(组合键)：可以选择两个或多个数据组成一个键<br>e)外键：一个属性，它是另外一个表的键</p>
<h3 id="9-2-规范化">9.2.规范化</h3><p>合理规范化的模型可应对需求变更，规范化数据重复降至最少.<br>1.第一范式1NF：数据原子性，每一列只有单值属性</p>
<p>2.第二范式2NF：没有部分依赖，每一个属性不能依赖主键的部分<br>如  (学号, 课程名称) → (姓名, 年龄, 成绩, 学分) ，不符合2NF<br>因为 (课程名称) –&gt; (学分)，(学号)-&gt;(姓名，年龄)，存在部分依赖<br>此时存在数据冗余，插入异常，删除异常，更新异常</p>
<p>分解为：学生：Student(学号, 姓名, 年龄)；<br>Course(课程名称, 学分)<br>SelectCourse(学号, 课程名称, 成绩)</p>
<p>3.第三范式3NF：没有传递依赖，不存在非主属性依赖于非主属性<br>例如： (学号) → (姓名, 年龄, 所在学院, 学院地点, 学院电话),符合2NF， 但不符合3NF，因为 (所在学院) -&gt; (学院地点, 学院电话)是传递依赖，分解为：<br>(学号) → (姓名, 年龄, 所在学院)<br>(所在学院) -&gt; (学院地点, 学院电话)</p>
<p>4.BCNF(Boyce-Codd Normaml Form): nothing but the key, 首先要满足3NF，在此基础上在候选键之间不能有传递依赖。<br>如：(仓库ID, 存储物品ID) →(管理员ID, 数量)<br>　　 (管理员ID, 存储物品ID) → (仓库ID, 数量)<br>(仓库ID, 存储物品ID)和(管理员ID, 存储物品ID)都是候选关键字，表中的唯一非关键字段为数量，符合3NF。但是，由于存在如下决定关系：<br>　　 (仓库ID) → (管理员ID) 　 (管理员ID) → (仓库ID)<br>候选键之间有依赖，不符合BCNF。</p>
<p>分解为：<br>(仓库ID) –&gt; (存储物品ID，数量)<br>(仓库ID) -&gt; (管理员ID)<br>BCNF分解过细，导致查询连接性能低，所以一般3NF就可以了。</p>
<p><a href="http://www.cnblogs.com/zxsoft/archive/2007/08/03/840826.html" title="数据库范式解析" target="_blank" rel="external">数据库范式详解</a></p>
<h3 id="9-3-3NF无损连接分解方法">9.3.3NF无损连接分解方法</h3><p>分解要做到无损连接，依赖保持。<br>考虑 关系  R= {ABCDEFGHI}<br>H -&gt; GD  E-&gt;D  HD-&gt;CE  BD -&gt; A<br>1. Right reduced<br>H -&gt; G<br>H-&gt;D<br>E-&gt;D<br>HD-&gt;C<br>HD-&gt;E<br>BD-&gt;A</p>
<p>2. Left reduced<br>H-&gt;G<br>H-&gt;D<br>E-&gt;D<br>H-&gt;C<br>H-&gt;E<br>BD-&gt;A</p>
<p>3. Find minimal cover<br>尝试删除一个依赖，然后验证是否无损和依赖保持，直到不能再删除<br>可以删除H-&gt;D,就不能再删了<br>H-&gt;G<br>E-&gt;D<br>H-&gt;C<br>H-&gt;E<br>BD-&gt;A<br>Minimal cover R’={H-&gt;GCE, E-D, BD-&gt;A}</p>
<p>4. 补充没有的依赖<br>我们发现I还没有在R’中，因此，提取出R’中的键HB构造关系HB-&gt;I</p>
<p>5. 最终结果<br>R = { H-&gt;GCE, E-D, BD-&gt;A , HB-&gt;I}</p>
<h3 id="9-4-主文件/数据库关系设计指导原则">9.4.主文件/数据库关系设计指导原则</h3><p>1. 指导原则<br>每个单独的数据实体应创建一个主数据表。<br>一个特定的数据字段只应出现在一个主文件中。<br>每个数据表支持CRUD操作</p>
<p>2. 完整性约束<br>实体完整性：主键不能有空值，唯一键可以有空值<br>引用完整性：一对多关系中，“一端”是父表，“多端”是子表，子表的外键必须在父表中有匹配记录；父表记录只在没有子表记录关联时才能删除；父表主键更新，要进行级联更新，即对应的子表外键要更新。<br>域完整性：对数据进行有效性检验，如数据必须大于0</p>
<p>3. 异常<br>数据冗余：可以通过3NF解决<br>插入异常：如果主键重复，或未知，导致插入异常，因为违反实体完整性。<br>删除异常：删除导致相关数据丢失导致<br>更新异常：更新导致不一致性。</p>
<p>4. 检索和显示数据的步骤<br>1)从数据库中选择一个关系</p>
<p>2)连接两个关系: join(inner join, outer join, left join, right join)<br>a)inner join： A join B 只有交集会出现在结果中<br>b)left (outer) join:  A 中与B没有匹配的记录会保留<br>c)right(outer) join: B中与A没有匹配的会保留<br>d)outer join, 又叫full outer join = left join 与right join的并集，所有匹配和未匹配的记录都有 </p>
<p>3)从关系中投影出列<br>4)从关系中选择所需的行<br>5)导出新的属性<br>6)行索引或排序<br>7)计算总计值和进行性能测量<br>8)显示数据</p>
<h3 id="9-5-反规范化">9.5.反规范化</h3><p>1. 原因<br>规范化的方式可以减少数据冗余，但查询速度会下降，一定的冗余，可以提升查询响应速度，避免重复引用检查表。<br>2. 反规范化的6种方法<br>1)合并1：1关系<br>2)部分合并1：<em>关系，不复制外关键字而复制非key的常用的字段<br>3)在1：</em>关系中复制FK（外关键字），减少join的表数量，将另一个表的主键复制变成外键。<br>4)在<em>：</em>关系中复制属性，避免3表join<br>5)引入重复组，将多值属性写在主表里：例如user表中多个电话号码的列，常用地址和常用电话<br>6)为了避免查询和更新这两个不可调和的矛盾，可以将更新和查询放在两张表中，从工作表提取出查询表，专门用于查询。这个方法演化成了数据仓库。这个方法只适用于查询的实时性要求不高的情况</p>
<h2 id="10-_设计准确的数据输入规程">10.    设计准确的数据输入规程</h2><p>数据输入准确性的目标：<br>•为数据创建有意义的代码<br>•设计有效的数据获取方法<br>•保证数据的完整性和有效性<br>•通过有效性检查确保数据的质量</p>
<h3 id="10-1_有效的编码">10.1 有效的编码</h3><h4 id="10-1-1-基本概念">10.1.1.基本概念</h4><p>1.优点：<br>a)提高数据处理效率<br>b)有助于数据的排序<br>c)节约内存和存储空间<br>d)提高数据输入的准确性和效率<br>e)特定类型的编码允许我们按特定的方式处理数据</p>
<p>2.目的<br>a)记录某些事物<br>b)分类信息<br>c)隐蔽信息<br>d)展示信息<br>e)请求相应地处理</p>
<p>3.编码的一般指导原则<br>a)保持代码简洁，短代码比长代码更容易输入和记忆<br>b)保持代码稳定，不应虽每次接收新数据而变化<br>c)代码要独一无二<br>d)允许排序代码<br>e)避免使人迷惑的代码<br>f)保持代码统一<br>g)允许修改代码<br>h)代码要有意义</p>
<h3 id="10-1-2-编码的类型">10.1.2.编码的类型</h3><p>1.记录某些事物<br>a)简单地顺序码<br>•优点：减少指派重复数字的可能性；让使用者估计出订单合适收到<br>•适用于：作业按顺序处理，需要知道数据项输入系统的顺序，需要知道事件的发生顺序。<br>•缺点：容易泄露商业信息，泄露当前已经指派了多少编码</p>
<p>b)字母衍生码<br>•例如：名字的编码： 取前2个辅音字母+名字长度+一个随机数<br>•例如编码女装：考虑品牌，类别，产地，生产日期，款号，尺码，颜色，价格<br>•常用于标识一个账号，邮寄地址标签<br>•注意：避免重复，分布要均匀<br>•缺点：如果采用名字的前三个辅音字母，当辅音字母少于3个，就会生成“RXX”这样的类型； 如果一些数据发生变化，如名字或地址变化，就会改变字母衍生码，而改变文件中的主键</p>
<p>2.分类信息<br>a)分类码<br>•目的：将一组具有特殊信息的数据从其他数据中区分出来，可以由单个字母或数字组成。是描述人物、位置、事物或事件的一种简写形式<br>•例如：计算所得税，分类有支付利息，医药费，税，捐款，应付款，生活用品支出，给每一个分类指派一个字母；<br>•使用单个字母标识分类可能会存在扩展瓶颈，可使用两个以上的字母，如计算机中的快捷键。</p>
<p>b)块顺序码<br>•是顺序码的扩展，对数据分类，对每一个分类分配一个编码范围，在该类别的项目按顺序编码<br>•例如浏览器分配100~999， 数据库 200~299<br>•优点：根据普通的特征对数据分类，还能简单地为下一个需要标识的项目（在同一块）指派一个可用的数字代码</p>
<p>3.隐藏信息<br>密码，隐藏信息，如医药处方，凯撒密码对称加密，Hash密码单向加密</p>
<p>4.展示信息：为用户展示信息，使数据输入更有意义<br>a)有效数字集编码<br>•例如衣服采用有效数字集描述产品信息，“414-219-19-12:表示浅褐色冬季外套，款式219，尺码12”<br>•优点：让员工方便的定位产品类别；查询效率高；有助于销售</p>
<p>b)助记码<br>•帮助记忆，结合字母和符号，醒目而又清晰的编码产品，例如国家简称<br>c)Unicode：显示我们不能输入和看到的字符， 国际标准组织ISO定义Unicode字符集，包括所有标准语言字符，还有65535个空位</p>
<p>5.请求相应的处理<br>a)功能码：用简短的数字或字母来标识一个计算机对数据执行的功能，通常采用顺序码或助记码的形式<br>优点：执行功能只需输入功能码，提高输入效率</p>
<h3 id="10-2-快速而高效的数据获取">10.2.快速而高效的数据获取</h3><p>1.决定要获取什么样的数据<br>如果输入无用，输出也会无用<br>输入的数据分类：随每个事务而改变的数据；能简明的将正在处理的项目与所有其他项目区分出来的数据</p>
<p>2.让计算机完成数据处理：处理重复的任务，如记录事务时间，根据输入计算新值，随时保存和检索数据</p>
<p>3.减少瓶颈和减少额外输入步骤：步骤越少，引入错误的机会就越少</p>
<p>4.选择有效的数据输入方法<br>a)键盘<br>b)扫描仪<br>c)视频，音频<br>d)磁性墨水<br>e)标记识别表单：如答题纸，但缺点是用户可能会一时大意填图出错<br>f)条形码：准确度高<br>g)RFID：射频识别技术</p>
<div style="color: #EF4808"><br>两个好用的Chrome插件，小伙伴们看完复习资料支持一下吧：<br><a href="https://chrome.google.com/webstore/detail/剪影截图/gkloklemhahnoipikedmafefilidffko" title="剪影截图" target="_blank" rel="external">剪影截图：好用的网页截图，一键人人分享工具，快捷键ctrl+shift+z, 双击确定!</a><br><a href="https://chrome.google.com/webstore/detail/tss下载助手/odhkpoplnhfnhhhkgphckabboemiifle" title="TSS下载助手" target="_blank" rel="external">TSS下载助手：让你方便一键批量下载</a><br></div>

<p>资源：<br><a href="http://cyanny/myblog/2013/11/27/system-analysis-design-kenneth-kendall-review-part1/" title="System Analysis and Design (Kenneth Kendall) Review Part1" target="_blank" rel="external">系统分析与设计part1</a><br><a href="http://cyanny/myblog/2013/11/27/system-analysis-design-kenneth-kendall-review-part2/" title="System Analysis and Design (Kenneth Kendall) Review Part2" target="_blank" rel="external">系统分析与设计part2</a><br><a href="http://cyanny/myblog/2013/11/27/system-analysis-design-kenneth-kendall-review-part3/" title="System Analysis and Design (Kenneth Kendall) Review Part3" target="_blank" rel="external">系统分析与设计part3</a><br><a href="http://cyanny/myblog/2013/11/27/system-analysis-design-kenneth-kendall-review-part4/" title="System Analysis and Design (Kenneth Kendall) Review Part4" target="_blank" rel="external">系统分析与设计part4</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.cyanny.com/2013/11/27/system-analysis-design-kenneth-kendall-review-part4/" data-id="citktnrjg000lee6ro6m0xxu5" class="article-share-link">Share</a>
      
        <a href="http://www.cyanny.com/2013/11/27/system-analysis-design-kenneth-kendall-review-part4/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/design/">Design</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/learning/">Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/systemanalysis/">SystemAnalysis</a></li></ul>

    </footer>
  </div>
  
</article>


    
      <nav id="page-nav">
        <a class="extend prev" rel="prev" href="/page/4/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/6/">Next &raquo;</a>
      </nav>
    </section>
      
        <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">recents</h3>
    <div class="widget">
      <ul id="recent-post" class="no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category"></p>
              <p class="item-title"><a href="/2016/09/27/春江花月夜/" class="title">春江花月夜</a></p>
              <p class="item-date"><time datetime="2016-09-27T01:35:59.000Z" itemprop="datePublished">2016-09-27</time></p>
            </div>
          </li>
        
          <li>
            
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/learning/">Learning</a></p>
              <p class="item-title"><a href="/2016/09/27/eight-queens-problem-in-scala/" class="title">eight-queens-problem-in-scala</a></p>
              <p class="item-date"><time datetime="2016-09-27T01:15:28.000Z" itemprop="datePublished">2016-09-27</time></p>
            </div>
          </li>
        
          <li>
            
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">Machine Learning</a></p>
              <p class="item-title"><a href="/2016/04/17/machine-learning-neural-networks/" class="title">Machine Learning Neural Networks</a></p>
              <p class="item-date"><time datetime="2016-04-17T13:29:20.000Z" itemprop="datePublished">2016-04-17</time></p>
            </div>
          </li>
        
          <li>
            
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">Machine Learning</a></p>
              <p class="item-title"><a href="/2016/04/10/machine-learning-logistic-regression/" class="title">Machine Learning Logistic Regression</a></p>
              <p class="item-date"><time datetime="2016-04-10T13:48:51.000Z" itemprop="datePublished">2016-04-10</time></p>
            </div>
          </li>
        
          <li>
            
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">Machine Learning</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/machine-learning/linear-regression/">Linear Regression</a></p>
              <p class="item-title"><a href="/2016/04/04/machine-learning-linear-regression/" class="title">Machine Learning Linear Regression</a></p>
              <p class="item-date"><time datetime="2016-04-04T07:55:31.000Z" itemprop="datePublished">2016-04-04</time></p>
            </div>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/agile/">Agile</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/agile/life/">Life</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/algorithm/">Algorithm</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/distributed-system/">Distributed System</a><span class="category-list-count">4</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/distributed-system/life/">Life</a><span class="category-list-count">4</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/">Hadoop</a><span class="category-list-count">16</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/hbase/">HBase</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/hive/">Hive</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/javascript/">JavaScript</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/learning/">Learning</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/learning/akka/">Akka</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/learning/akka/scala/">Scala</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/learning/scala/">Scala</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/life/">Life</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">Linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">Machine Learning</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/linear-regression/">Linear Regression</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/network/">Network</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/nodejs/">Nodejs</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-analysis-and-design/">System Analysis and Design</a><span class="category-list-count">4</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">tag cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/agile/" style="font-size: 10px;">Agile</a> <a href="/tags/akka/" style="font-size: 10px;">Akka</a> <a href="/tags/big-data/" style="font-size: 12.86px;">Big Data</a> <a href="/tags/coursera/" style="font-size: 12.86px;">Coursera</a> <a href="/tags/design/" style="font-size: 14.29px;">Design</a> <a href="/tags/hbase/" style="font-size: 15.71px;">HBase</a> <a href="/tags/hadoop/" style="font-size: 18.57px;">Hadoop</a> <a href="/tags/hive/" style="font-size: 10px;">Hive</a> <a href="/tags/javascript/" style="font-size: 10px;">JavaScript</a> <a href="/tags/language/" style="font-size: 10px;">Language</a> <a href="/tags/learn/" style="font-size: 10px;">Learn</a> <a href="/tags/learning/" style="font-size: 20px;">Learning</a> <a href="/tags/linux/" style="font-size: 11.43px;">Linux</a> <a href="/tags/machine-learning/" style="font-size: 12.86px;">Machine Learning</a> <a href="/tags/mapreduce/" style="font-size: 10px;">MapReduce</a> <a href="/tags/methodology/" style="font-size: 10px;">Methodology</a> <a href="/tags/network/" style="font-size: 10px;">Network</a> <a href="/tags/node-js/" style="font-size: 10px;">Node.js</a> <a href="/tags/research/" style="font-size: 17.14px;">Research</a> <a href="/tags/resource/" style="font-size: 10px;">Resource</a> <a href="/tags/scala/" style="font-size: 12.86px;">Scala</a> <a href="/tags/scrum/" style="font-size: 10px;">Scrum</a> <a href="/tags/systemanalysis/" style="font-size: 14.29px;">SystemAnalysis</a> <a href="/tags/vps/" style="font-size: 11.43px;">VPS</a> <a href="/tags/algorithm/" style="font-size: 17.14px;">algorithm</a> <a href="/tags/life/" style="font-size: 10px;">life</a> <a href="/tags/math/" style="font-size: 10px;">math</a> <a href="/tags/nodejs/" style="font-size: 10px;">nodejs</a> <a href="/tags/sort/" style="font-size: 11.43px;">sort</a> <a href="/tags/诗话/" style="font-size: 10px;">诗话</a>
    </div>
  </div>

  
  <div id="toTop" class="fa fa-chevron-up"></div>
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Cyanny Liang
    </div>
  </div>
</footer>
    

<script>
  var disqus_shortname = 'lgrcyanny';
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.useso.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>